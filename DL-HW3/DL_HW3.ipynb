{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Deep Learning: Assignment #3\n",
        "## Submission date: 07/01/2026, 23:59.\n",
        "### Topics:\n",
        "- RNNs\n",
        "- GRUs\n",
        "- LSTMs\n",
        "- Transformers\n"
      ],
      "metadata": {
        "id": "qMNKwshvcblK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Submitted by:**\n",
        "\n",
        "- **Student 1 — Name, ID**\n",
        "- **Student 2 — Name, ID**\n"
      ],
      "metadata": {
        "id": "iMUwOkZSc54s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Assignment Instructions:**\n",
        "\n",
        "· Submissions are in **pairs only**. Write both names + IDs at the top of the notebook.\n",
        "\n",
        "· Keep your code **clean, concise, and readable**.\n",
        "\n",
        "· You may work in your IDE, but you **must** paste the final code back into the **matching notebook cells** and run it there.  \n",
        "\n",
        "\n",
        "· <font color='red'>Write your textual answers in red.</font>  \n",
        "(e.g., `<span style=\"color:red\">your answer here</span>`)\n",
        "\n",
        "· All figures, printed results, and outputs should remain visible in the notebook.  \n",
        "Run **all cells** before submitting and **do not clear outputs**.\n",
        "\n",
        "· Use relative paths — **no absolute file paths** pointing to local machines.\n",
        "\n",
        "· **Important:** Your submission must be entirely your own.  \n",
        "Any form of plagiarism (including uncredited use of ChatGPT or AI tools) will result in **grade 0** and disciplinary action.\n"
      ],
      "metadata": {
        "id": "qkusen4bc8sm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 1 — Chatbot Tutorial (35 Points)\n",
        "\n",
        "In this tutorial, we explore a classical and instructive application of\n",
        "**recurrent sequence-to-sequence (seq2seq) models**: building a neural\n",
        "chatbot. Chatbots provide a natural and intuitive testbed for studying\n",
        "sequence models, as they require processing variable-length input\n",
        "sequences and generating coherent variable-length outputs, one token at\n",
        "a time.\n",
        "\n",
        "Conversational agents are a long-standing and active research topic in\n",
        "artificial intelligence. Chatbots appear in a wide range of practical\n",
        "settings, including customer service systems, online helpdesks, and\n",
        "virtual assistants. Many deployed systems rely on **retrieval-based\n",
        "models**, which select responses from a predefined set based on the input\n",
        "query. While such approaches can be effective in narrowly defined\n",
        "domains, they lack the flexibility required for open-domain\n",
        "conversation.\n",
        "\n",
        "Teaching a machine to generate meaningful, context-aware responses\n",
        "across multiple domains remains a challenging and largely unsolved\n",
        "problem. The rise of deep learning has enabled a class of **generative\n",
        "conversational models**, most notably the *Neural Conversational Model*\n",
        "introduced by Vinyals and Le (2015), which demonstrated that an\n",
        "encoder–decoder architecture can be trained end-to-end to map input\n",
        "sentences directly to output sentences.\n",
        "\n",
        "Inspired by this line of work, we study a simplified generative chatbot\n",
        "implemented using modern deep learning tools.\n",
        "\n",
        "In this question, we will work with conversational data extracted from\n",
        "movie scripts in the\n",
        "[Cornell Movie-Dialogs Corpus](https://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html).\n",
        "Each training example consists of an input sentence and a corresponding\n",
        "response sentence, forming a paired sequence-to-sequence learning task.\n"
      ],
      "metadata": {
        "id": "nChedj8RE02i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load & Preprocess Data"
      ],
      "metadata": {
        "id": "NUpTPkZPUrlJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To start, we load the data. Download the data ZIP file\n",
        "[here](https://zissou.infosci.cornell.edu/convokit/datasets/movie-corpus/movie-corpus.zip) and unzip it in the current directory, or alternalively run the following cell:\n"
      ],
      "metadata": {
        "id": "uxNiywKRLcqv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Mqe24ejcUJq"
      },
      "outputs": [],
      "source": [
        "!mkdir data\n",
        "!cd data\n",
        "!wget https://zissou.infosci.cornell.edu/convokit/datasets/movie-corpus/movie-corpus.zip\n",
        "!unzip -q movie-corpus.zip\n",
        "!cd .."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After loading the data, let’s import some necessities."
      ],
      "metadata": {
        "id": "JPK6wWGhOlWK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "from __future__ import unicode_literals\n",
        "\n",
        "import torch\n",
        "from torch.jit import script, trace\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "import csv\n",
        "import random\n",
        "import re\n",
        "import os\n",
        "import unicodedata\n",
        "import codecs\n",
        "from io import open\n",
        "import itertools\n",
        "import math\n",
        "import json\n",
        "\n",
        "\n",
        "USE_CUDA = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if USE_CUDA else \"cpu\")"
      ],
      "metadata": {
        "id": "hPbCdr4KMMLu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next step is to reformat our data file and load the data into\n",
        "structures that we can work with.\n",
        "\n",
        "The [Cornell Movie-Dialogs\n",
        "Corpus](https://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html)_\n",
        "is a rich dataset of movie character dialog:\n",
        "\n",
        "-  220,579 conversational exchanges between 10,292 pairs of movie\n",
        "   characters\n",
        "-  9,035 characters from 617 movies\n",
        "-  304,713 total utterances\n",
        "\n",
        "This dataset is large and diverse, and there is a great variation of\n",
        "language formality, time periods, sentiment, etc. Our hope is that this\n",
        "diversity makes our model robust to many forms of inputs and queries.\n",
        "\n",
        "First, we’ll take a look at some lines of our datafile to see the\n",
        "original format.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5VNg1NgUMSvp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls data"
      ],
      "metadata": {
        "id": "TEghaBHkMU5u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = \"movie-corpus\"\n",
        "corpus_name = \"movie-corpus\"\n",
        "\n",
        "def printLines(file, n=10):\n",
        "    with open(file, 'rb') as datafile:\n",
        "        lines = datafile.readlines()\n",
        "    for line in lines[:n]:\n",
        "        print(line)\n",
        "\n",
        "printLines(os.path.join(corpus, \"utterances.jsonl\"))"
      ],
      "metadata": {
        "id": "UBDxLSZQMaDk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Create formatted data file"
      ],
      "metadata": {
        "id": "2NEREmyNUodJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For convenience, we'll create a nicely formatted data file in which each line contains a tab-separated *query sentence* and a *response sentence* pair.\n",
        "\n",
        "The following functions facilitate the parsing of the raw\n",
        "*utterances.jsonl* data file.\n",
        "\n",
        "\n",
        "In the next cell, you'll find the functions:\n",
        "-  ``loadLinesAndConversations`` splits each line of the file into a dictionary of lines with fields: lineID, characterID, and text and then groups them into conversations with fields: conversationID, movieID, and lines.\n",
        "\n",
        "-  ``extractSentencePairs`` extracts pairs of sentences from\n",
        "   conversations.\n",
        "\n",
        "\n",
        "> Run the code in the next cell."
      ],
      "metadata": {
        "id": "ZPf46yAiN5U1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Splits each line of the file to create lines and conversations\n",
        "def loadLinesAndConversations(fileName):\n",
        "    lines = {}\n",
        "    conversations = {}\n",
        "    with open(fileName, 'r', encoding='iso-8859-1') as f:\n",
        "        for line in f:\n",
        "            lineJson = json.loads(line)\n",
        "            # Extract fields for line object\n",
        "            lineObj = {}\n",
        "            lineObj[\"lineID\"] = lineJson[\"id\"]\n",
        "            lineObj[\"characterID\"] = lineJson[\"speaker\"]\n",
        "            lineObj[\"text\"] = lineJson[\"text\"]\n",
        "            lines[lineObj['lineID']] = lineObj\n",
        "\n",
        "            # Extract fields for conversation object\n",
        "            if lineJson[\"conversation_id\"] not in conversations:\n",
        "                convObj = {}\n",
        "                convObj[\"conversationID\"] = lineJson[\"conversation_id\"]\n",
        "                convObj[\"movieID\"] = lineJson[\"meta\"][\"movie_id\"]\n",
        "                convObj[\"lines\"] = [lineObj]\n",
        "            else:\n",
        "                convObj = conversations[lineJson[\"conversation_id\"]]\n",
        "                convObj[\"lines\"].insert(0, lineObj)\n",
        "            conversations[convObj[\"conversationID\"]] = convObj\n",
        "\n",
        "    return lines, conversations\n",
        "\n",
        "\n",
        "# Extracts pairs of sentences from conversations\n",
        "def extractSentencePairs(conversations):\n",
        "    qa_pairs = []\n",
        "    for conversation in conversations.values():\n",
        "        # Iterate over all the lines of the conversation\n",
        "        for i in range(len(conversation[\"lines\"]) - 1):  # We ignore the last line (no answer for it)\n",
        "            inputLine = conversation[\"lines\"][i][\"text\"].strip()\n",
        "            targetLine = conversation[\"lines\"][i+1][\"text\"].strip()\n",
        "            # Filter wrong samples (if one of the lists is empty)\n",
        "            if inputLine and targetLine:\n",
        "                qa_pairs.append([inputLine, targetLine])\n",
        "    return qa_pairs"
      ],
      "metadata": {
        "id": "Fjk7-RjfNsik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we’ll call these functions and create the file. We’ll call it\n",
        "*formatted_movie_lines.txt*.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0pj4e4Q0N2pc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define path to new file\n",
        "datafile = os.path.join(corpus, \"formatted_movie_lines.txt\")\n",
        "\n",
        "delimiter = '\\t'\n",
        "# Unescape the delimiter\n",
        "delimiter = str(codecs.decode(delimiter, \"unicode_escape\"))\n",
        "\n",
        "# Initialize lines dict and conversations dict\n",
        "lines = {}\n",
        "conversations = {}\n",
        "# Load lines and conversations\n",
        "print(\"\\nProcessing corpus into lines and conversations...\")\n",
        "lines, conversations = loadLinesAndConversations(os.path.join(corpus, \"utterances.jsonl\"))\n",
        "\n",
        "# Write new csv file\n",
        "print(\"\\nWriting newly formatted file...\")\n",
        "with open(datafile, 'w', encoding='utf-8') as outputfile:\n",
        "    writer = csv.writer(outputfile, delimiter=delimiter, lineterminator='\\n')\n",
        "    for pair in extractSentencePairs(conversations):\n",
        "        writer.writerow(pair)\n",
        "\n",
        "# Print a sample of lines\n",
        "print(\"\\nSample lines from file:\")\n",
        "printLines(datafile)"
      ],
      "metadata": {
        "id": "1RODL5HeNvO9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Trim Data"
      ],
      "metadata": {
        "id": "DUprvMAtUjE4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now move from raw text to a representation that a neural network can process.\n",
        "Our next task is to build a **vocabulary** and load the **query/response pairs**\n",
        "into memory.\n",
        "\n",
        "Unlike images, text does not come with an inherent mapping to a numerical space.\n",
        "A sequence model expects **integer token indices**, so we must define a mapping\n",
        "from each unique word in the dataset to a discrete index.\n",
        "\n",
        "To do this, we define a `Voc` (vocabulary) class that maintains:\n",
        "\n",
        "- `word2index`: a mapping from each word to an integer index  \n",
        "- `index2word`: the inverse mapping from indices back to words  \n",
        "- `word2count`: a frequency table used for trimming rare words  \n",
        "- `num_words`: the current vocabulary size  \n",
        "\n",
        "In addition, we reserve a small set of **special tokens**:\n",
        "\n",
        "- `PAD` for padding shorter sequences in a batch  \n",
        "- `SOS` to mark the start of a sequence for the decoder  \n",
        "- `EOS` to mark the end of a sequence  \n",
        "- `UNK` to represent words that are not in the vocabulary  \n",
        "\n",
        "Later, we will remove infrequent words from the vocabulary using a minimum\n",
        "frequency threshold (`MIN_COUNT`). This reduces noise and decreases the effective\n",
        "problem size, which often improves training stability and convergence.\n"
      ],
      "metadata": {
        "id": "n8ZVo4fvUg4n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Default word tokens\n",
        "PAD_token = 0  # Used for padding short sentences\n",
        "SOS_token = 1  # Start-of-sentence token\n",
        "EOS_token = 2  # End-of-sentence token\n",
        "UNK_token = 3  # Unknown word token\n",
        "\n",
        "class Voc:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.trimmed = False\n",
        "        self.word2index = {}\n",
        "        self.word2count = {}\n",
        "        self.index2word = {\n",
        "            PAD_token: \"PAD\",\n",
        "            SOS_token: \"SOS\",\n",
        "            EOS_token: \"EOS\",\n",
        "            UNK_token: \"UNK\",\n",
        "        }\n",
        "        self.num_words = 4  # Count default tokens\n",
        "\n",
        "    def addSentence(self, sentence):\n",
        "        for word in sentence.split(' '):\n",
        "            self.addWord(word)\n",
        "\n",
        "    def addWord(self, word):\n",
        "        if word not in self.word2index:\n",
        "            self.word2index[word] = self.num_words\n",
        "            self.word2count[word] = 1\n",
        "            self.index2word[self.num_words] = word\n",
        "            self.num_words += 1\n",
        "        else:\n",
        "            self.word2count[word] += 1\n",
        "\n",
        "    # Remove words below a certain count threshold\n",
        "    def trim(self, min_count):\n",
        "        if self.trimmed:\n",
        "            return\n",
        "        self.trimmed = True\n",
        "\n",
        "        keep_words = []\n",
        "        for k, v in self.word2count.items():\n",
        "            if v >= min_count:\n",
        "                keep_words.append(k)\n",
        "\n",
        "        print('keep_words {} / {} = {:.4f}'.format(\n",
        "            len(keep_words), len(self.word2index), len(keep_words) / len(self.word2index)\n",
        "        ))\n",
        "\n",
        "        # Reinitialize dictionaries\n",
        "        self.word2index = {}\n",
        "        self.word2count = {}\n",
        "        self.index2word = {\n",
        "            PAD_token: \"PAD\",\n",
        "            SOS_token: \"SOS\",\n",
        "            EOS_token: \"EOS\",\n",
        "            UNK_token: \"UNK\",\n",
        "        }\n",
        "        self.num_words = 4  # Count default tokens\n",
        "\n",
        "        for word in keep_words:\n",
        "            self.addWord(word)\n"
      ],
      "metadata": {
        "id": "8dW73MDYUu9g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We begin by converting Unicode strings to ASCII using `unicodeToAscii`.\n",
        "Next, all text is lowercased and non-letter characters are removed while\n",
        "preserving basic punctuation (`normalizeString`).\n",
        "Finally, to promote stable training and reduce unnecessary computation,\n",
        "we filter out sentence pairs whose length exceeds the `MAX_LENGTH`\n",
        "threshold (`filterPairs`).\n"
      ],
      "metadata": {
        "id": "wHrvoycoU1Cq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LENGTH = 10  # Maximum sentence length to consider\n",
        "\n",
        "def unicodeToAscii(s):\n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn'\n",
        "    )\n",
        "\n",
        "def normalizeString(s):\n",
        "    s = unicodeToAscii(s.lower().strip())\n",
        "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
        "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
        "    s = re.sub(r\"\\s+\", r\" \", s).strip()\n",
        "    return s\n",
        "\n",
        "def readVocs(datafile, corpus_name):\n",
        "    print(\"Reading lines...\")\n",
        "    lines = open(datafile, encoding='utf-8').read().strip().split('\\n')\n",
        "    pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
        "    voc = Voc(corpus_name)\n",
        "    return voc, pairs\n",
        "\n",
        "def filterPair(p):\n",
        "    return len(p[0].split(' ')) < MAX_LENGTH and len(p[1].split(' ')) < MAX_LENGTH\n",
        "\n",
        "def filterPairs(pairs):\n",
        "    return [pair for pair in pairs if filterPair(pair)]\n",
        "\n",
        "def loadPrepareData(corpus, corpus_name, datafile, save_dir):\n",
        "    print(\"Start preparing training data ...\")\n",
        "    voc, pairs = readVocs(datafile, corpus_name)\n",
        "    print(\"Read {!s} sentence pairs\".format(len(pairs)))\n",
        "    pairs = filterPairs(pairs)\n",
        "    print(\"Trimmed to {!s} sentence pairs\".format(len(pairs)))\n",
        "    print(\"Counting words...\")\n",
        "    for pair in pairs:\n",
        "        voc.addSentence(pair[0])\n",
        "        voc.addSentence(pair[1])\n",
        "    print(\"Counted words:\", voc.num_words)\n",
        "    return voc, pairs\n",
        "\n",
        "save_dir = os.path.join(\"data\", \"save\")\n",
        "voc, pairs = loadPrepareData(corpus, corpus_name, datafile, save_dir)\n",
        "\n",
        "print(\"\\nSample pairs:\")\n",
        "for pair in pairs[:10]:\n",
        "    print(pair)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gU_8C4XkWjsB",
        "outputId": "dd8a31bc-eab1-4bb2-a61c-af787676edc2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start preparing training data ...\n",
            "Reading lines...\n",
            "Read 221282 sentence pairs\n",
            "Trimmed to 64313 sentence pairs\n",
            "Counting words...\n",
            "Counted words: 18083\n",
            "\n",
            "Sample pairs:\n",
            "['they do to !', 'they do not !']\n",
            "['she okay ?', 'i hope so .']\n",
            "['wow', 'let s go .']\n",
            "['what good stuff ?', 'the real you .']\n",
            "['the real you .', 'like my fear of wearing pastels ?']\n",
            "['do you listen to this crap ?', 'what crap ?']\n",
            "['well no . . .', 'then that s all you had to say .']\n",
            "['then that s all you had to say .', 'but']\n",
            "['but', 'you always been this selfish ?']\n",
            "['have fun tonight ?', 'tons']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Another technique that often improves training efficiency is **trimming\n",
        "rarely used words** from the vocabulary. Intuitively, words that appear\n",
        "only a handful of times contribute little to the learning signal, while\n",
        "significantly increasing the size of the vocabulary.\n",
        "\n",
        "We perform trimming as a two-step process:\n",
        "\n",
        "1. Remove words that appear fewer than `MIN_COUNT` times from the\n",
        "   vocabulary.\n",
        "2. Remove sentence pairs that contain any of the trimmed words, ensuring\n",
        "   that all remaining training examples are fully represented in the\n",
        "   vocabulary.\n",
        "\n",
        "This procedure reduces noise, lowers the dimensionality of the problem,\n",
        "and often leads to faster and more stable convergence during training.\n"
      ],
      "metadata": {
        "id": "xICI7hK6XX48"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MIN_COUNT = 3    # Minimum word count threshold for trimming\n",
        "\n",
        "def trimRareWords(voc, pairs, MIN_COUNT):\n",
        "    # Trim words used under the MIN_COUNT from the voc\n",
        "    voc.trim(MIN_COUNT)\n",
        "    # Filter out pairs with trimmed words\n",
        "    keep_pairs = []\n",
        "    for pair in pairs:\n",
        "        input_sentence = pair[0]\n",
        "        output_sentence = pair[1]\n",
        "        keep_input = True\n",
        "        keep_output = True\n",
        "        # Check input sentence\n",
        "        for word in input_sentence.split(' '):\n",
        "            if word not in voc.word2index:\n",
        "                keep_input = False\n",
        "                break\n",
        "        # Check output sentence\n",
        "        for word in output_sentence.split(' '):\n",
        "            if word not in voc.word2index:\n",
        "                keep_output = False\n",
        "                break\n",
        "\n",
        "        # Only keep pairs that do not contain trimmed word(s) in their input or output sentence\n",
        "        if keep_input and keep_output:\n",
        "            keep_pairs.append(pair)\n",
        "\n",
        "    print(\"Trimmed from {} pairs to {}, {:.4f} of total\".format(\n",
        "        len(pairs), len(keep_pairs), len(keep_pairs) / len(pairs)\n",
        "    ))\n",
        "    return keep_pairs\n",
        "\n",
        "\n",
        "# Trim voc and pairs\n",
        "pairs = trimRareWords(voc, pairs, MIN_COUNT)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y41EzlFrXcGS",
        "outputId": "725cd735-d2bb-4f70-8c0e-e9d11c828e46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "keep_words 7833 / 18079 = 0.4333\n",
            "Trimmed from 64313 pairs to 53131, 0.8261 of total\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preparing Data for the Model"
      ],
      "metadata": {
        "id": "1nXul3gFYtOr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "So far, we have transformed raw conversational text into a cleaned and\n",
        "trimmed set of *(input, response)* sentence pairs, along with a vocabulary\n",
        "that maps words to integer indices.\n",
        "\n",
        "However, neural sequence models do not operate directly on text. Instead,\n",
        "they expect **numerical tensors** as input. In this section, we convert\n",
        "our sentence pairs into padded tensors that can be efficiently processed\n",
        "by the encoder–decoder model.\n",
        "\n",
        "To accelerate training and make effective use of GPU parallelism, we will\n",
        "train the model using **mini-batches** rather than individual sentence\n",
        "pairs. This introduces an additional challenge: sentences within a batch\n",
        "may have different lengths.\n",
        "\n",
        "To handle variable-length sequences, we adopt the following conventions:\n",
        "\n",
        "- Sentences are converted to sequences of word indices and terminated\n",
        "  with an `EOS_token`.\n",
        "- All sequences in a batch are padded to the length of the longest\n",
        "  sequence using the `PAD_token`.\n",
        "- Batched input tensors are shaped as  \n",
        "  **(max_sequence_length, batch_size)**,  \n",
        "  so that each time step can be processed across all sequences in parallel.\n",
        "\n",
        "This layout is particularly convenient for recurrent models, which\n",
        "process input one time step at a time.\n"
      ],
      "metadata": {
        "id": "Ax3M_oAtYreC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In addition to the padded input and target tensors, we also construct:\n",
        "\n",
        "- A **lengths tensor**, which stores the true (unpadded) length of each\n",
        "  input sequence. This will later be used to efficiently process variable-\n",
        "  length sequences.\n",
        "- A **binary mask tensor** for the target sequences, where padded positions\n",
        "  are marked with 0 and valid tokens with 1. This allows us to ignore padded\n",
        "  values when computing the training loss.\n",
        "\n",
        "The following helper functions implement this batching pipeline.\n"
      ],
      "metadata": {
        "id": "yauQ1U9EZHB4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def indexesFromSentence(voc, sentence):\n",
        "    return [voc.word2index.get(word, UNK_token) for word in sentence.split(' ')] + [EOS_token]\n",
        "\n",
        "\n",
        "def zeroPadding(l, fillvalue=PAD_token):\n",
        "    return list(itertools.zip_longest(*l, fillvalue=fillvalue))\n",
        "\n",
        "\n",
        "def binaryMatrix(l, value=PAD_token):\n",
        "    m = []\n",
        "    for i, seq in enumerate(l):\n",
        "        m.append([])\n",
        "        for token in seq:\n",
        "            if token == PAD_token:\n",
        "                m[i].append(0)\n",
        "            else:\n",
        "                m[i].append(1)\n",
        "    return m\n",
        "\n",
        "\n",
        "# Returns padded input sequence tensor and lengths\n",
        "def inputVar(l, voc):\n",
        "    indexes_batch = [indexesFromSentence(voc, sentence) for sentence in l]\n",
        "    lengths = torch.tensor([len(indexes) for indexes in indexes_batch])\n",
        "    padList = zeroPadding(indexes_batch)\n",
        "    padVar = torch.LongTensor(padList)\n",
        "    return padVar, lengths\n",
        "\n",
        "\n",
        "# Returns padded target sequence tensor, padding mask, and max target length\n",
        "def outputVar(l, voc):\n",
        "    indexes_batch = [indexesFromSentence(voc, sentence) for sentence in l]\n",
        "    max_target_len = max(len(indexes) for indexes in indexes_batch)\n",
        "    padList = zeroPadding(indexes_batch)\n",
        "    mask = binaryMatrix(padList)\n",
        "    mask = torch.BoolTensor(mask)\n",
        "    padVar = torch.LongTensor(padList)\n",
        "    return padVar, mask, max_target_len\n",
        "\n",
        "\n",
        "# Returns all items for a given batch of sentence pairs\n",
        "def batch2TrainData(voc, pair_batch):\n",
        "    # Sort pairs by input sentence length (descending)\n",
        "    pair_batch.sort(key=lambda x: len(x[0].split(\" \")), reverse=True)\n",
        "\n",
        "    input_batch, output_batch = [], []\n",
        "    for pair in pair_batch:\n",
        "        input_batch.append(pair[0])\n",
        "        output_batch.append(pair[1])\n",
        "\n",
        "    inp, lengths = inputVar(input_batch, voc)\n",
        "    output, mask, max_target_len = outputVar(output_batch, voc)\n",
        "    return inp, lengths, output, mask, max_target_len\n"
      ],
      "metadata": {
        "id": "F5uPrvpRZFxI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Architecture"
      ],
      "metadata": {
        "id": "hX0acWXjir68"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "#### Sequence-to-Sequence Architecture\n",
        "\n",
        "The core of our chatbot is a **sequence-to-sequence (seq2seq)** model.\n",
        "The objective of a seq2seq model is to map a variable-length input\n",
        "sequence to a variable-length output sequence using a fixed-size neural\n",
        "network.\n",
        "\n",
        "[Sutskever et al. (2014)](https://arxiv.org/abs/1409.3215) demonstrated\n",
        "that this task can be accomplished by composing two recurrent neural\n",
        "networks:\n",
        "\n",
        "- An **encoder**, which processes the input sequence and compresses it\n",
        "  into an internal representation.\n",
        "- A **decoder**, which generates the output sequence one token at a time,\n",
        "  conditioned on the encoder’s representation.\n",
        "\n",
        "In the context of conversational modeling, the encoder reads the input\n",
        "sentence (the query), and the decoder generates the response.\n",
        "\n",
        "This encoder–decoder formulation underlies many modern sequence modeling\n",
        "approaches in machine translation, dialogue systems, and text\n",
        "generation.\n"
      ],
      "metadata": {
        "id": "VGW83R6GiqTu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Encoder\n",
        "\n",
        "In this part, you will implement the **encoder** component of the\n",
        "sequence-to-sequence model.\n",
        "\n",
        "The encoder processes the input sentence one token (word) at a time.\n",
        "At each time step, it produces an output vector and updates an internal\n",
        "**hidden state** that summarizes the sequence observed so far.\n",
        "\n",
        "You should implement the encoder using a **multi-layer Gated Recurrent\n",
        "Unit (GRU)**, introduced by [Cho et al. (2014)](https://arxiv.org/pdf/1406.1078v3.pdf).\n",
        "GRUs extend standard recurrent neural networks by incorporating gating\n",
        "mechanisms that regulate information flow, enabling more effective\n",
        "modeling of long-term dependencies.\n",
        "\n",
        "Your encoder should be **bidirectional**, meaning that the input sequence is processed both forward and backward in time. The outputs of the forward and backward GRUs should be **summed** at each time step to form the final encoder output representation.\n",
        "\n",
        "Before being passed to the GRU, word indices are mapped into a continuous vector space using an **embedding layer**, which is provided to the encoder. The embedding dimension is equal to the GRU hidden size.\n",
        "\n",
        "When processing padded mini-batches, you must correctly pack and unpack\n",
        "sequences using:\n",
        "\n",
        "- `nn.utils.rnn.pack_padded_sequence`\n",
        "- `nn.utils.rnn.pad_packed_sequence`\n",
        "\n",
        "This ensures that the GRU does not perform unnecessary computation over\n",
        "padding tokens.\n"
      ],
      "metadata": {
        "id": "TNvwQHykjUkr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Encoder Inputs**\n",
        "\n",
        "- `input_seq`: Padded batch of input sentences  \n",
        "  Shape: *(max_length, batch_size)*\n",
        "\n",
        "- `input_lengths`: True lengths of each sentence in the batch  \n",
        "  Shape: *(batch_size)*\n",
        "\n",
        "**Encoder Outputs**\n",
        "\n",
        "- `outputs`: Output features from the last hidden layer of the GRU  \n",
        "  (sum of bidirectional outputs)  \n",
        "  Shape: *(max_length, batch_size, hidden_size)*\n",
        "\n",
        "- `hidden`: Final hidden state of the GRU  \n",
        "  Shape: *(n_layers × num_directions, batch_size, hidden_size)*\n"
      ],
      "metadata": {
        "id": "YlhQVEO0lweR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, embedding, n_layers=1, dropout=0):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.n_layers = n_layers\n",
        "        self.hidden_size = hidden_size\n",
        "        self.embedding = embedding\n",
        "\n",
        "        # Initialize GRU; the input_size and hidden_size params are both set to 'hidden_size'\n",
        "        #   because our input size is a word embedding with number of features == hidden_size\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers,\n",
        "                          dropout=(0 if n_layers == 1 else dropout), bidirectional=True)\n",
        "\n",
        "    def forward(self, input_seq, input_lengths, hidden=None):\n",
        "        # Convert word indexes to embeddings\n",
        "        embedded = self.embedding(input_seq)\n",
        "        # Pack padded batch of sequences for RNN module\n",
        "        packed = nn.utils.rnn.pack_padded_sequence(embedded, input_lengths)\n",
        "        # Forward pass through GRU\n",
        "        outputs, hidden = self.gru(packed, hidden)\n",
        "        # Unpack padding\n",
        "        outputs, _ = nn.utils.rnn.pad_packed_sequence(outputs)\n",
        "        # Sum bidirectional GRU outputs\n",
        "        outputs = outputs[:, :, :self.hidden_size] + outputs[:, :, self.hidden_size:]\n",
        "        # Return output and final hidden state\n",
        "        return outputs, hidden"
      ],
      "metadata": {
        "id": "EtkW86wKlI9V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Decoder with Attention\n",
        "\n",
        "The decoder generates the response sequence in a token-by-token manner.\n",
        "At each time step, it predicts the next word based on:\n",
        "\n",
        "- its current hidden state,\n",
        "- the previously generated word, and\n",
        "- a **context vector** computed from the encoder outputs.\n",
        "\n",
        "A limitation of vanilla seq2seq models is their reliance on a single\n",
        "fixed-length context vector to represent the entire input sequence.\n",
        "This bottleneck becomes particularly problematic for long input\n",
        "sentences.\n",
        "\n",
        "To address this issue, [Bahdanau et al. (2015)](https://arxiv.org/abs/1409.0473) introduced the **attention mechanism**, which allows the decoder to focus on different parts of the input sequence at each decoding step.\n",
        "\n",
        "[Luong et al. (2015)](https://arxiv.org/abs/1508.04025) later proposed\n",
        "**Global Attention**, in which the decoder attends to *all* encoder\n",
        "hidden states at every time step. Attention weights are computed using\n",
        "the decoder’s current hidden state and the encoder outputs via a\n",
        "parameterized **score function**.\n",
        "\n",
        "In this assignment, you will implement Luong-style global attention., which computes a soft alignment between the decoder state and the encoder outputs, producing a context vector that guides each decoding step.\n"
      ],
      "metadata": {
        "id": "JSTK4LoclWY_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "At a given decoding step $t$, the attention mechanism computes an\n",
        "alignment score between the current decoder hidden state $h_t$ and each encoder output $\\bar{h_s}$.\n",
        "\n",
        "Luong attention defines three possible **score functions**:\n",
        "\n",
        "- **Dot**:  \n",
        "  $\\text{score}(h_t, \\bar{h}_s) = h_t^\\top \\bar{h}_s$\n",
        "\n",
        "- **General**:  \n",
        "  $\\text{score}(h_t, \\bar{h}_s) = h_t^\\top W \\bar{h}_s$\n",
        "\n",
        "- **Concat**:  \n",
        "  $\\text{score}(h_t, \\bar{h}_s) = v^\\top \\tanh(W [h_t ; \\bar{h}_s])$\n",
        "\n",
        "The attention weights are obtained by applying a softmax over all encoder time steps. These weights are then used to compute a **context vector** as a weighted sum of encoder outputs.\n"
      ],
      "metadata": {
        "id": "V1wE-Y-QlqTG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Attn(nn.Module):\n",
        "    def __init__(self, method, hidden_size):\n",
        "        super(Attn, self).__init__()\n",
        "        self.method = method\n",
        "        if self.method not in ['dot', 'general', 'concat']:\n",
        "            raise ValueError(self.method, \"is not an appropriate attention method.\")\n",
        "        self.hidden_size = hidden_size\n",
        "        if self.method == 'general':\n",
        "            self.attn = nn.Linear(self.hidden_size, hidden_size)\n",
        "        elif self.method == 'concat':\n",
        "            self.attn = nn.Linear(self.hidden_size * 2, hidden_size)\n",
        "            self.v = nn.Parameter(torch.FloatTensor(hidden_size))\n",
        "\n",
        "    def forward(self, hidden, encoder_outputs):\n",
        "        # hidden: (1, batch, hidden_size)\n",
        "        # encoder_outputs: (max_length, batch, hidden_size)\n",
        "        \n",
        "        max_len = encoder_outputs.size(0)\n",
        "        this_batch_size = encoder_outputs.size(1)\n",
        "\n",
        "        # Create variable to store attention energies\n",
        "        attn_energies = torch.zeros(this_batch_size, max_len, device=device) # (B, S)\n",
        "\n",
        "        # Vectorized implementation\n",
        "        encoder_outputs_t = encoder_outputs.transpose(0, 1) # (B, S, H)\n",
        "        hidden_s = hidden[-1] # (B, H)\n",
        "        \n",
        "        if self.method == 'dot':\n",
        "             # (B, 1, H) . (B, H, S) -> (B, 1, S)\n",
        "             attn_energies = torch.bmm(hidden_s.unsqueeze(1), encoder_outputs_t.transpose(1, 2)).squeeze(1)\n",
        "        elif self.method == 'general':\n",
        "             energy = self.attn(encoder_outputs_t) # (B, S, H)\n",
        "             attn_energies = torch.bmm(hidden_s.unsqueeze(1), energy.transpose(1, 2)).squeeze(1)\n",
        "        elif self.method == 'concat':\n",
        "             # Expand hidden_s to (B, S, H)\n",
        "             hidden_ext = hidden_s.unsqueeze(1).expand(-1, max_len, -1)\n",
        "             combined = torch.cat((hidden_ext, encoder_outputs_t), 2) # (B, S, 2H)\n",
        "             energy = torch.tanh(self.attn(combined)) # (B, S, H)\n",
        "             v = self.v.unsqueeze(0).expand(this_batch_size, 1, -1)\n",
        "             attn_energies = torch.sum(v * energy, dim=2)\n",
        "        \n",
        "        return F.softmax(attn_energies, dim=1).unsqueeze(1) # (B, 1, S)"
      ],
      "metadata": {
        "id": "KHmecq2klos8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Decoder with Luong Attention\n",
        "\n",
        "We now define the decoder. Unlike the encoder (which processes the entire input\n",
        "sequence in one forward pass), the decoder is executed **one time step at a time**:\n",
        "at each step it receives a single token and produces a probability distribution\n",
        "over the vocabulary for the next token.\n",
        "\n",
        "Concretely, at decoding step $t$:\n",
        "\n",
        "- The input to the decoder is a tensor `input_step` of shape **(1, batch_size)**,\n",
        "  containing one token per sequence.\n",
        "- After embedding, this becomes **(1, batch_size, hidden_size)**.\n",
        "- The decoder updates its hidden state using a unidirectional GRU.\n",
        "\n",
        "A standard seq2seq decoder compresses the entire input into a fixed vector, which can lead to information loss—especially for longer sequences. To address this, we incorporate an **attention mechanism**.\n",
        "\n",
        "\n",
        "We use **Luong et al. (2015)** global attention:\n",
        "[Luong et al., 2015](https://arxiv.org/abs/1508.04025).\n",
        "At each decoding step, the decoder computes attention weights over all encoder time steps, then uses these weights to form a context vector (a weighted sum of encoder outputs). This context vector summarizes the parts of the input sentence that are most relevant for predicting the next word.\n"
      ],
      "metadata": {
        "id": "kY0UkUFun6KG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "At each decoding step, the decoder should:\n",
        "\n",
        "1. Embed the current input token.\n",
        "2. Run one GRU step to update the decoder hidden state.\n",
        "3. Compute attention weights using the current decoder state and encoder outputs.\n",
        "4. Use the attention weights to compute a context vector.\n",
        "5. Combine the decoder state and context vector.\n",
        "6. Predict a probability distribution over the vocabulary.\n",
        "\n",
        "You are asked to implement both the **initialization** and **forward pass** of the Luong attention decoder according to this description.\n",
        "\n",
        "---\n",
        "\n",
        "**Inputs**\n",
        "\n",
        "- `input_step`: one time step (one token) for each sequence in the batch  \n",
        "  Shape: **(1, batch_size)**\n",
        "\n",
        "- `last_hidden`: previous decoder hidden state  \n",
        "  Shape: **(n_layers, batch_size, hidden_size)**\n",
        "\n",
        "- `encoder_outputs`: encoder outputs for all time steps  \n",
        "  Shape: **(max_length, batch_size, hidden_size)**\n",
        "\n",
        "**Outputs**\n",
        "\n",
        "- `output`: probability distribution over the vocabulary for the next token  \n",
        "  Shape: **(batch_size, voc.num_words)**\n",
        "\n",
        "- `hidden`: updated decoder hidden state  \n",
        "  Shape: **(n_layers, batch_size, hidden_size)**\n"
      ],
      "metadata": {
        "id": "KTLo7QWpoQI3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LuongAttnDecoderRNN(nn.Module):\n",
        "    def __init__(self, attn_model, embedding, hidden_size, output_size, n_layers=1, dropout=0.1):\n",
        "        super(LuongAttnDecoderRNN, self).__init__()\n",
        "        self.attn_model = attn_model\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.n_layers = n_layers\n",
        "        self.dropout = dropout\n",
        "\n",
        "        self.embedding = embedding\n",
        "        self.embedding_dropout = nn.Dropout(dropout)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers, dropout=(0 if n_layers == 1 else dropout))\n",
        "        self.concat = nn.Linear(hidden_size * 2, hidden_size)\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "        self.attn = Attn(attn_model, hidden_size)\n",
        "\n",
        "    def forward(self, input_step, last_hidden, encoder_outputs):\n",
        "        # input_step: (1, B)\n",
        "        # last_hidden: (n_layers, B, H)\n",
        "        # encoder_outputs: (S, B, H)\n",
        "\n",
        "        embedded = self.embedding(input_step)\n",
        "        embedded = self.embedding_dropout(embedded)\n",
        "        \n",
        "        # Forward through unidirectional GRU\n",
        "        rnn_output, hidden = self.gru(embedded, last_hidden)\n",
        "        # rnn_output: (1, B, H)\n",
        "        \n",
        "        # Calculate attention weights\n",
        "        attn_weights = self.attn(rnn_output, encoder_outputs) \n",
        "        # attn_weights: (B, 1, S)\n",
        "        \n",
        "        # Multiply attention weights to encoder outputs to get new \"weighted sum\" context vector\n",
        "        context = attn_weights.bmm(encoder_outputs.transpose(0, 1))\n",
        "        # context: (B, 1, H)\n",
        "        \n",
        "        # Concatenate weighted context vector and GRU output using Luong eq. 5\n",
        "        rnn_output = rnn_output.squeeze(0)\n",
        "        context = context.squeeze(1)\n",
        "        concat_input = torch.cat((rnn_output, context), 1)\n",
        "        concat_output = torch.tanh(self.concat(concat_input))\n",
        "        \n",
        "        # Predict next word using Luong eq. 6\n",
        "        output = self.out(concat_output)\n",
        "        output = F.softmax(output, dim=1)\n",
        "        \n",
        "        return output, hidden"
      ],
      "metadata": {
        "id": "VnQq5GgYoMvF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define Training Procedure\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_MPgDGdroq1T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now define the components required to train our seq2seq chatbot.\n",
        "Training is performed over mini-batches of padded sequences, which\n",
        "requires special care when computing the loss and updating model\n",
        "parameters."
      ],
      "metadata": {
        "id": "R5abIAKLJwpJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Masked Loss\n",
        "\n",
        "Because batches contain padded sequences, not all positions in the target\n",
        "tensor correspond to valid words. We therefore compute the loss only over\n",
        "non-padding positions.\n",
        "\n",
        "The function below computes a **masked negative log-likelihood loss**,\n",
        "given the decoder’s output distribution, the target tokens, and a binary\n",
        "mask that indicates which positions are valid (i.e., not `PAD_token`).\n"
      ],
      "metadata": {
        "id": "HXMD9pMr3SIH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def maskNLLLoss(inp, target, mask):\n",
        "    \"\"\"\n",
        "    inp:    (batch_size, vocab_size) softmax probabilities\n",
        "    target: (batch_size,) target token indices\n",
        "    mask:   (batch_size,) boolean mask (True for valid tokens, False for PAD)\n",
        "    \"\"\"\n",
        "    nTotal = mask.sum()\n",
        "    crossEntropy = -torch.log(torch.gather(inp, 1, target.view(-1, 1)).squeeze(1))\n",
        "    loss = crossEntropy.masked_select(mask).mean()\n",
        "    loss = loss.to(device)\n",
        "    return loss, nTotal.item()"
      ],
      "metadata": {
        "id": "Wmsmc_8D3Ufi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Single Training Iteration\n",
        "\n",
        "We now describe a **single training iteration**, corresponding to one\n",
        "mini-batch update.\n",
        "\n",
        "During each iteration, we apply two commonly used techniques to improve\n",
        "training stability and convergence:\n",
        "\n",
        "- **Teacher forcing**: with some probability, the decoder receives the\n",
        "  ground-truth token as its next input instead of its own prediction.\n",
        "  This can significantly speed up training, but excessive teacher\n",
        "  forcing may lead to poor performance at inference time.\n",
        "\n",
        "- **Gradient clipping**: gradients are clipped to a maximum norm to\n",
        "  prevent the exploding gradient problem, which is particularly common\n",
        "  in recurrent neural networks.\n",
        "\n",
        "The sequence of operations for a single iteration is as follows:\n"
      ],
      "metadata": {
        "id": "ltGbRYR53_Gc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Single Training Iteration (Implement)\n",
        "\n",
        "In this part you will implement a **single mini-batch update** for the\n",
        "seq2seq chatbot.\n",
        "\n",
        "You must handle two key ideas we covered in class:\n",
        "\n",
        "- **Teacher forcing**: with some probability, `teacher_forcing_ratio`, feed the ground-truth token as the next decoder input; otherwise, feed the model’s own prediction.\n",
        "- **Gradient clipping**: clip gradient norms to avoid exploding gradients in RNN training.\n",
        "\n",
        "Your implementation should follow this high-level sequence:\n",
        "\n"
      ],
      "metadata": {
        "id": "Tpn5JL0qGm4b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Sequence of Operations**\n",
        "\n",
        "1. Forward pass the input batch through the encoder.\n",
        "2. Initialize the decoder input with `SOS_token` and the decoder hidden\n",
        "   state with the encoder’s final hidden state.\n",
        "3. Decode one time step at a time:\n",
        "   - apply teacher forcing with probability `teacher_forcing_ratio`\n",
        "   - otherwise, feed the decoder’s own prediction as the next input\n",
        "4. Compute and accumulate masked loss.\n",
        "5. Backpropagate gradients.\n",
        "6. Clip gradients.\n"
      ],
      "metadata": {
        "id": "cz6ux67A_qQ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Complete the function `train(...)` so that it performs one full mini-batch training update according to the sequence of operations described abov"
      ],
      "metadata": {
        "id": "YbN0IZL4HzGf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(input_variable, lengths, target_variable, mask, max_target_len,\n",
        "          encoder, decoder, embedding,\n",
        "          encoder_optimizer, decoder_optimizer,\n",
        "          batch_size, clip, max_length=MAX_LENGTH):\n",
        "    \\\"\\\"\\\"\n",
        "    Performs a single mini-batch update step.\n",
        "\n",
        "    Inputs:\n",
        "        input_variable:  (max_input_len, batch_size)\n",
        "        lengths:         (batch_size,)  (must be on CPU for packing)\n",
        "        target_variable: (max_target_len, batch_size)\n",
        "        mask:            (max_target_len, batch_size) boolean\n",
        "        max_target_len:  int\n",
        "\n",
        "    Returns:\n",
        "        avg_loss: average masked loss per non-PAD token (float)\n",
        "    \\\"\\\"\\\"\n",
        "\n",
        "    # Move tensors to the correct device\n",
        "    input_variable = input_variable.to(device)\n",
        "    target_variable = target_variable.to(device)\n",
        "    mask = mask.to(device)\n",
        "    lengths = lengths.to(\"cpu\")  # required by pack_padded_sequence\n",
        "\n",
        "\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "\n",
        "    encoder_outputs, encoder_hidden = encoder(input_variable, lengths)\n",
        "\n",
        "    decoder_input = torch.LongTensor([[SOS_token for _ in range(batch_size)]]).to(device)\n",
        "    decoder_hidden = encoder_hidden[:decoder.n_layers]\n",
        "\n",
        "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
        "\n",
        "    loss = 0\n",
        "    print_losses = []\n",
        "    n_totals = 0\n",
        "\n",
        "    if use_teacher_forcing:\n",
        "        for t in range(max_target_len):\n",
        "            decoder_output, decoder_hidden = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs\n",
        "            )\n",
        "            decoder_input = target_variable[t].view(1, -1)\n",
        "            mask_loss, nTotal = maskNLLLoss(decoder_output, target_variable[t], mask[t])\n",
        "            loss += mask_loss\n",
        "            print_losses.append(mask_loss.item() * nTotal)\n",
        "            n_totals += nTotal\n",
        "    else:\n",
        "        for t in range(max_target_len):\n",
        "            decoder_output, decoder_hidden = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs\n",
        "            )\n",
        "            _, topi = decoder_output.topk(1)\n",
        "            decoder_input = torch.LongTensor([[topi[i][0] for i in range(batch_size)]]).to(device)\n",
        "            mask_loss, nTotal = maskNLLLoss(decoder_output, target_variable[t], mask[t])\n",
        "            loss += mask_loss\n",
        "            print_losses.append(mask_loss.item() * nTotal)\n",
        "            n_totals += nTotal\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    _ = nn.utils.clip_grad_norm_(encoder.parameters(), clip)\n",
        "    _ = nn.utils.clip_grad_norm_(decoder.parameters(), clip)\n",
        "\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    # Return average loss per non-pad token\n",
        "    return sum(print_losses) / n_totals"
      ],
      "metadata": {
        "id": "slCS1Ee3_cBW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Training Loop\n",
        "\n",
        "We now wrap the single training iteration into a full training loop.\n",
        "At each iteration, a random mini-batch is sampled and one parameter\n",
        "update is performed.\n",
        "\n",
        "We periodically print training statistics and save model checkpoints,\n",
        "which include the encoder and decoder parameters, optimizer states, and\n",
        "vocabulary information.\n"
      ],
      "metadata": {
        "id": "of4H957VF7QE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def trainIters(model_name, voc, pairs, encoder, decoder, encoder_optimizer, decoder_optimizer,\n",
        "               embedding, encoder_n_layers, decoder_n_layers, save_dir, n_iteration,\n",
        "               batch_size, print_every, save_every, clip, corpus_name, loadFilename):\n",
        "\n",
        "    training_batches = [\n",
        "        batch2TrainData(voc, [random.choice(pairs) for _ in range(batch_size)])\n",
        "        for _ in range(n_iteration)\n",
        "    ]\n",
        "\n",
        "    print('Initializing ...')\n",
        "    start_iteration = 1\n",
        "    print_loss = 0\n",
        "\n",
        "    if loadFilename:\n",
        "        start_iteration = checkpoint['iteration'] + 1\n",
        "\n",
        "    print(\"Training...\")\n",
        "    for iteration in range(start_iteration, n_iteration + 1):\n",
        "        input_variable, lengths, target_variable, mask, max_target_len = training_batches[iteration - 1]\n",
        "\n",
        "        loss = train(input_variable, lengths, target_variable, mask, max_target_len,\n",
        "                     encoder, decoder, embedding,\n",
        "                     encoder_optimizer, decoder_optimizer,\n",
        "                     batch_size, clip)\n",
        "\n",
        "        print_loss += loss\n",
        "\n",
        "        if iteration % print_every == 0:\n",
        "            print_loss_avg = print_loss / print_every\n",
        "            print(f\"Iteration: {iteration}; Percent complete: {iteration / n_iteration * 100:.1f}%; Average loss: {print_loss_avg:.4f}\")\n",
        "            print_loss = 0\n",
        "\n",
        "        if iteration % save_every == 0:\n",
        "            directory = os.path.join(\n",
        "                save_dir, model_name, corpus_name,\n",
        "                f'{encoder_n_layers}-{decoder_n_layers}_{hidden_size}'\n",
        "            )\n",
        "            os.makedirs(directory, exist_ok=True)\n",
        "\n",
        "            torch.save({\n",
        "                'iteration': iteration,\n",
        "                'en': encoder.state_dict(),\n",
        "                'de': decoder.state_dict(),\n",
        "                'en_opt': encoder_optimizer.state_dict(),\n",
        "                'de_opt': decoder_optimizer.state_dict(),\n",
        "                'loss': loss,\n",
        "                'voc_dict': voc.__dict__,\n",
        "                'embedding': embedding.state_dict()\n",
        "            }, os.path.join(directory, f'{iteration}_checkpoint.tar'))\n"
      ],
      "metadata": {
        "id": "pu0PNcyoGEvo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Evaluation\n"
      ],
      "metadata": {
        "id": "OMmmeKgWMzbH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After training the chatbot model, we would like to interact with it and\n",
        "generate responses to user-provided input sentences. To do so, we must\n",
        "define how the decoder generates an output sequence from the encoded\n",
        "input.\n",
        "\n",
        "During training, the decoder may receive ground-truth tokens as input\n",
        "(teacher forcing). During evaluation, however, the model must rely\n",
        "entirely on its own predictions. This requires defining an explicit\n",
        "**decoding strategy**."
      ],
      "metadata": {
        "id": "F3lkjGOYJzXO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Greedy Decoding\n",
        "\n",
        "We use **greedy decoding** to generate responses at evaluation time.\n",
        "At each decoding step, greedy decoding selects the word with the highest predicted probability from the decoder’s output distribution.\n",
        "\n",
        "Formally, given the decoder output distribution at time step $t$,\n",
        "greedy decoding chooses:\n",
        "\n",
        "$$\n",
        "\\hat{w}_t = \\arg\\max_w \\; p(w \\mid w_{<t}, x)\n",
        "$$\n",
        "\n",
        "Greedy decoding is optimal at the level of individual time steps, though it does not guarantee a globally optimal sequence. Despite this\n",
        "limitation, it is simple, efficient, and serves as a strong baseline for sequence generation.\n"
      ],
      "metadata": {
        "id": "RLXvRDF3M28Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To implement greedy decoding, we define a `GreedySearchDecoder` module.\n",
        "This module wraps the encoder and decoder and performs decoding one time step at a time.\n",
        "\n",
        "Given an input sentence, decoding proceeds as follows:\n",
        "\n",
        "**Computation Graph**\n",
        "\n",
        "1. Forward the input sequence through the encoder.\n",
        "2. Initialize the decoder hidden state using the encoder’s final hidden state.\n",
        "3. Initialize the decoder input with the `SOS_token`.\n",
        "4. Iteratively decode one token at a time:\n",
        "   - Forward pass through the decoder.\n",
        "   - Select the most probable token.\n",
        "   - Feed the selected token as input to the next step.\n",
        "5. Collect and return the generated tokens and their scores.\n"
      ],
      "metadata": {
        "id": "R-i_qAUFN4bN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GreedySearchDecoder(nn.Module):\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super(GreedySearchDecoder, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "\n",
        "    def forward(self, input_seq, input_length, max_length):\n",
        "        # Forward input through encoder model\n",
        "        encoder_outputs, encoder_hidden = self.encoder(input_seq, input_length)\n",
        "        # Prepare encoder's final hidden layer to be first hidden input to the decoder\n",
        "        decoder_hidden = encoder_hidden[:decoder.n_layers]\n",
        "        # Initialize decoder input with SOS_token\n",
        "        decoder_input = torch.ones(1, 1, device=device, dtype=torch.long) * SOS_token\n",
        "        # Initialize tensors to append decoded words to\n",
        "        all_tokens = torch.zeros([0], device=device, dtype=torch.long)\n",
        "        all_scores = torch.zeros([0], device=device)\n",
        "        # Iteratively decode one word token at a time\n",
        "        for _ in range(max_length):\n",
        "            # Forward pass through decoder\n",
        "            decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
        "            # Obtain most likely word token and its softmax score\n",
        "            decoder_scores, decoder_input = torch.max(decoder_output, dim=1)\n",
        "            # Record token and score\n",
        "            all_tokens = torch.cat((all_tokens, decoder_input), dim=0)\n",
        "            all_scores = torch.cat((all_scores, decoder_scores), dim=0)\n",
        "            # Prepare current token to be next decoder input (add a dimension)\n",
        "            decoder_input = torch.unsqueeze(decoder_input, 0)\n",
        "        # Return collections of word tokens and scores\n",
        "        return all_tokens, all_scores"
      ],
      "metadata": {
        "id": "90nlU-80OB_Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Evaluating Text Input\n",
        "\n",
        "With the decoding procedure defined, we can now evaluate individual input sentences.\n",
        "\n",
        "The `evaluate` function handles the low-level mechanics of evaluation:\n",
        "it converts an input sentence into a tensor of word indices, prepares the appropriate length tensor, and runs greedy decoding to obtain the outputn sequence.\n",
        "\n",
        "The `evaluateInput` function provides a simple interactive interface for the chatbot. It repeatedly prompts the user for input, normalizes the sentence using the same preprocessing pipeline as the training data, and prints the decoded response. The interaction continues until the user enters `\"q\"` or `\"quit\"`.\n"
      ],
      "metadata": {
        "id": "SW5kohCkOE3C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(encoder, decoder, searcher, voc, sentence, max_length=MAX_LENGTH):\n",
        "    \"\"\"\n",
        "    Evaluate a single input sentence using greedy decoding.\n",
        "    Out-of-vocabulary words are mapped to UNK_token.\n",
        "    \"\"\"\n",
        "    # words -> indexes (UNK-safe)\n",
        "    indexes = [\n",
        "        voc.word2index.get(word, UNK_token)\n",
        "        for word in sentence.split(' ')\n",
        "    ] + [EOS_token]\n",
        "\n",
        "    indexes_batch = [indexes]\n",
        "\n",
        "    # Create lengths tensor\n",
        "    lengths = torch.tensor([len(indexes)])\n",
        "\n",
        "    # Prepare input batch (max_length, batch_size=1)\n",
        "    input_batch = torch.LongTensor(indexes_batch).transpose(0, 1)\n",
        "    input_batch = input_batch.to(device)\n",
        "    lengths = lengths.to(\"cpu\")\n",
        "\n",
        "    # Decode sentence\n",
        "    tokens, scores = searcher(input_batch, lengths, max_length)\n",
        "\n",
        "    # indexes -> words\n",
        "    decoded_words = [voc.index2word[token.item()] for token in tokens]\n",
        "    return decoded_words\n",
        "\n",
        "\n",
        "def evaluateInput(encoder, decoder, searcher, voc):\n",
        "    \"\"\"\n",
        "    Interactive chatbot interface.\n",
        "    Type 'q' or 'quit' to exit.\n",
        "    \"\"\"\n",
        "    while True:\n",
        "        # Get input sentence\n",
        "        input_sentence = input('> ')\n",
        "        if input_sentence in ('q', 'quit'):\n",
        "            break\n",
        "\n",
        "        # Normalize input\n",
        "        input_sentence = normalizeString(input_sentence)\n",
        "\n",
        "        # Evaluate sentence\n",
        "        output_words = evaluate(encoder, decoder, searcher, voc, input_sentence)\n",
        "\n",
        "        # Remove EOS and PAD tokens from output\n",
        "        output_words = [w for w in output_words if w not in ('EOS', 'PAD')]\n",
        "\n",
        "        print('Bot:', ' '.join(output_words))\n"
      ],
      "metadata": {
        "id": "XlnrdhjeOnBV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run Model"
      ],
      "metadata": {
        "id": "Zzadv_mbXgpV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, it is time to run our model!\n",
        "\n",
        "Regardless of whether we want to train or test the chatbot model, we\n",
        "must initialize the individual encoder and decoder models. In the\n",
        "following block, we set our desired configurations, choose to start from scratch or set a checkpoint to load from, and build and initialize the models. Feel free to play with different model configurations to\n",
        "optimize performance."
      ],
      "metadata": {
        "id": "TLCOsQwVXf49"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Configure models\n",
        "model_name = 'cb_model'\n",
        "attn_model = 'dot'\n",
        "#attn_model = 'general'\n",
        "#attn_model = 'concat'\n",
        "hidden_size = 500\n",
        "encoder_n_layers = 2\n",
        "decoder_n_layers = 2\n",
        "dropout = 0.1\n",
        "batch_size = 64\n",
        "\n",
        "# Set checkpoint to load from; set to None if starting from scratch\n",
        "loadFilename = None\n",
        "checkpoint_iter = 4000\n",
        "#loadFilename = os.path.join(save_dir, model_name, corpus_name,\n",
        "#                            '{}-{}_{}'.format(encoder_n_layers, decoder_n_layers, hidden_size),\n",
        "#                            '{}_checkpoint.tar'.format(checkpoint_iter))\n",
        "\n",
        "\n",
        "# Load model if a loadFilename is provided\n",
        "if loadFilename:\n",
        "    # If loading on same machine the model was trained on\n",
        "    checkpoint = torch.load(loadFilename)\n",
        "    # If loading a model trained on GPU to CPU\n",
        "    #checkpoint = torch.load(loadFilename, map_location=torch.device('cpu'))\n",
        "    encoder_sd = checkpoint['en']\n",
        "    decoder_sd = checkpoint['de']\n",
        "    encoder_optimizer_sd = checkpoint['en_opt']\n",
        "    decoder_optimizer_sd = checkpoint['de_opt']\n",
        "    embedding_sd = checkpoint['embedding']\n",
        "    voc.__dict__ = checkpoint['voc_dict']\n",
        "\n",
        "\n",
        "print('Building encoder and decoder ...')\n",
        "# Initialize word embeddings\n",
        "embedding = nn.Embedding(voc.num_words, hidden_size)\n",
        "if loadFilename:\n",
        "    embedding.load_state_dict(embedding_sd)\n",
        "# Initialize encoder & decoder models\n",
        "encoder = EncoderRNN(hidden_size, embedding, encoder_n_layers, dropout)\n",
        "decoder = LuongAttnDecoderRNN(attn_model, embedding, hidden_size, voc.num_words, decoder_n_layers, dropout)\n",
        "if loadFilename:\n",
        "    encoder.load_state_dict(encoder_sd)\n",
        "    decoder.load_state_dict(decoder_sd)\n",
        "# Use appropriate device\n",
        "encoder = encoder.to(device)\n",
        "decoder = decoder.to(device)\n",
        "print('Models built and ready to go!')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rVFEfeXfXo2l",
        "outputId": "7a0b2c9b-8107-4d0e-8432-55d91a808fe1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Building encoder and decoder ...\n",
            "Models built and ready to go!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Run Training\n",
        "\n",
        "Run the following block in order to train the model.\n",
        "\n",
        "First we set training parameters, then we initialize our optimizers, and finally we call the ``trainIters`` function to run our training\n",
        "iterations."
      ],
      "metadata": {
        "id": "uk2T7qIYXtPA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Configure training/optimization\n",
        "clip = 50.0\n",
        "teacher_forcing_ratio = 1.0\n",
        "learning_rate = 0.0001\n",
        "decoder_learning_ratio = 5.0\n",
        "n_iteration = 4000\n",
        "print_every = 1\n",
        "save_every = 500\n",
        "\n",
        "# Ensure dropout layers are in train mode\n",
        "encoder.train()\n",
        "decoder.train()\n",
        "\n",
        "# Initialize optimizers\n",
        "print('Building optimizers ...')\n",
        "encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
        "decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate * decoder_learning_ratio)\n",
        "if loadFilename:\n",
        "    encoder_optimizer.load_state_dict(encoder_optimizer_sd)\n",
        "    decoder_optimizer.load_state_dict(decoder_optimizer_sd)\n",
        "\n",
        "# If you have cuda, configure cuda to call\n",
        "for state in encoder_optimizer.state.values():\n",
        "    for k, v in state.items():\n",
        "        if isinstance(v, torch.Tensor):\n",
        "            state[k] = v.cuda()\n",
        "\n",
        "for state in decoder_optimizer.state.values():\n",
        "    for k, v in state.items():\n",
        "        if isinstance(v, torch.Tensor):\n",
        "            state[k] = v.cuda()\n",
        "\n",
        "# Run training iterations\n",
        "print(\"Starting Training!\")\n",
        "trainIters(model_name, voc, pairs, encoder, decoder, encoder_optimizer, decoder_optimizer,\n",
        "           embedding, encoder_n_layers, decoder_n_layers, save_dir, n_iteration, batch_size,\n",
        "           print_every, save_every, clip, corpus_name, loadFilename)"
      ],
      "metadata": {
        "id": "UllNVty_XyH7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Run Evaluation\n",
        "\n",
        "To chat with your model, run the following block."
      ],
      "metadata": {
        "id": "iwLXQmmxX2II"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set dropout layers to eval mode\n",
        "encoder.eval()\n",
        "decoder.eval()\n",
        "\n",
        "# Initialize search module\n",
        "searcher = GreedySearchDecoder(encoder, decoder)\n",
        "\n",
        "# Begin chatting by uncommenting the line:\n",
        "evaluateInput(encoder, decoder, searcher, voc)"
      ],
      "metadata": {
        "id": "kAWQIcv2X4MO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Beam Search Decoding\n"
      ],
      "metadata": {
        "id": "7fkZhXcRvHBm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "So far, we used **greedy decoding**, which selects the most likely next token at each step.\n",
        "Greedy decoding is fast, but it may miss better full sequences because it commits to local choices.\n",
        "\n",
        "In this task you will implement **beam search**, as discussed in class.\n",
        "Beam search keeps the top-*k* partial hypotheses at each decoding step, ranked by their cumulative log-probability.\n",
        "\n",
        "**Requirements**\n",
        "- Implement `BeamSearchDecoder` similar in interface to `GreedySearchDecoder`.\n",
        "- Use beam width `k` (beam size) as a configurable argument.\n",
        "- Use **log-probabilities** (sum of log-probs) to score sequences.\n",
        "- Stop a hypothesis when it emits `EOS_token` (you may keep completed hypotheses in a separate list).\n",
        "- Return the best decoded token sequence (and optionally its scores)."
      ],
      "metadata": {
        "id": "nqckaLMhJ3QE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BeamSearchDecoder(nn.Module):\n",
        "    def __init__(self, encoder, decoder, beam_size=5):\n",
        "        super(BeamSearchDecoder, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.beam_size = beam_size\n",
        "\n",
        "    def forward(self, input_seq, input_length, max_length):\n",
        "        encoder_outputs, encoder_hidden = self.encoder(input_seq, input_length)\n",
        "        decoder_hidden = encoder_hidden[:self.decoder.n_layers]\n",
        "        \n",
        "        # Initialize decoder input with SOS_token\n",
        "        decoder_input = torch.ones(1, 1, device=device, dtype=torch.long) * SOS_token\n",
        "        \n",
        "        # Beam structure: list of (cumulative_score, list_of_tokens, decoder_hidden)\n",
        "        # Note: list_of_tokens includes all tokens including SOS. \n",
        "        # But for output we should match greedy searcher which returns tokens and scores.\n",
        "        \n",
        "        candidates = [(0.0, [SOS_token], decoder_hidden)]\n",
        "        completed_sequences = []\n",
        "        \n",
        "        for _ in range(max_length):\n",
        "            new_candidates = []\n",
        "            for score, seq, hidden in candidates:\n",
        "                # If last token is EOS, it's completed\n",
        "                if seq[-1] == EOS_token:\n",
        "                    completed_sequences.append((score, seq, hidden))\n",
        "                    continue\n",
        "                    \n",
        "                dec_input = torch.LongTensor([[seq[-1]]]).to(device)\n",
        "                decoder_output, next_hidden = self.decoder(dec_input, hidden, encoder_outputs)\n",
        "                \n",
        "                # Get log probabilities\n",
        "                log_probs = torch.log(decoder_output).squeeze(0) # (Vocab)\n",
        "                topv, topi = log_probs.topk(self.beam_size)\n",
        "                \n",
        "                for v, i in zip(topv, topi):\n",
        "                    new_score = score + v.item()\n",
        "                    new_seq = seq + [i.item()]\n",
        "                    new_candidates.append((new_score, new_seq, next_hidden))\n",
        "            \n",
        "            # If all candidates completed, break\n",
        "            if len(new_candidates) == 0:\n",
        "                break\n",
        "\n",
        "            # Sort and prune\n",
        "            new_candidates.sort(key=lambda x: x[0], reverse=True)\n",
        "            candidates = new_candidates[:self.beam_size]\n",
        "        \n",
        "        # Add any remaining candidates to completed (in case max_length reached)\n",
        "        completed_sequences.extend(candidates)\n",
        "        completed_sequences.sort(key=lambda x: x[0], reverse=True)\n",
        "        \n",
        "        best_seq = completed_sequences[0][1]\n",
        "        \n",
        "        # Remove SOS for result tokens\n",
        "        tokens = torch.tensor(best_seq[1:], device=device, dtype=torch.long)\n",
        "        # Dummy scores to match the return signature of GreedySearchDecoder (all_tokens, all_scores)\n",
        "        scores = torch.zeros(len(tokens), device=device) \n",
        "        \n",
        "        return tokens, scores"
      ],
      "metadata": {
        "id": "BmxxhuysvMiY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the following cell and try the **same 10 prompts** with greedy vs. beam search.\n",
        "Briefly comment: do you observe differences? When might beam search help, and when might it hurt?\n"
      ],
      "metadata": {
        "id": "2zZLTGUjvM6U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "greedy_searcher = GreedySearchDecoder(encoder, decoder)\n",
        "beam_searcher = BeamSearchDecoder(encoder, decoder)\n",
        "\n",
        "evaluateInput(encoder, decoder, greedy_searcher, voc)\n",
        "evaluateInput(encoder, decoder, beam_searcher, voc)"
      ],
      "metadata": {
        "id": "q3IWk_5ZvS8d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Understanding & Reflection Questions"
      ],
      "metadata": {
        "id": "DcsE-SjmJsGZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Answer the following questions in your own words.  \n",
        "Your answers should demonstrate conceptual understanding rather than\n",
        "code-level details.\n",
        "\n",
        "1. **Dataset Understanding**  \n",
        "   What is the *Cornell Movie-Dialogs Corpus*?  \n",
        "   Describe the type of data it contains and explain how it is used in\n",
        "   this assignment to formulate a learning problem.\n",
        "   \n",
        "   <span style=\"color:red\">The Cornell Movie-Dialogs Corpus is a collection of fictional conversations extracted from movie scripts. It contains pairs of utterances (lines spoken by characters). In this assignment, we use these pairs to formulate a sequence-to-sequence learning problem where the input is a line of dialogue (query) and the target is the subsequent line (response). The goal is to train a model that can generate a plausible response given an input statement, effectively simulating a chatbot.</span>\n",
        "\n",
        "2. **Vocabulary Construction**  \n",
        "   We implemented a custom `Voc` class to manage the vocabulary.  \n",
        "   What object or mechanism did we use in class (Tutorials 9–10) to obtain similar functionality?  \n",
        "   Compare the two approaches at a high level.\n",
        "   \n",
        "   <span style=\"color:red\">In class tutorials, we typically use the `torchtext` library's `Vocab` object or `build_vocab_from_iterator`. Both the custom `Voc` class and `torchtext`'s approach serve the same purpose: mapping tokens to unique integer indices (numericalization) and maintaining special tokens (like PAD, SOS, EOS). The custom `Voc` class allows for manual control (like the `trim` method for rare words) and is more explicit, whereas `torchtext` provides optimized, higher-level abstractions that handle tokenization and vocabulary building more automatically.</span>\n",
        "\n",
        "3. **Model Architecture**  \n",
        "   Describe the model architecture used in this assignment *abstractly*.  \n",
        "   Your answer should explain the roles of the encoder, decoder, and\n",
        "   attention mechanism, without referring to specific code details.\n",
        "   \n",
        "   <span style=\"color:red\">The architecture is a Sequence-to-Sequence (Seq2Seq) model with Attention. \n",
        "   *   **Encoder:** Reads the variable-length input sequence and compresses it into a set of context vectors (hidden states) that represent the semantic meaning of the input.\n",
        "   *   **Decoder:** Generates the output sequence one token at a time. It initializes its state from the encoder and uses its own previous output to predict the next token.\n",
        "   *   **Attention Mechanism:** Bridges the encoder and decoder. At each decoding step, it allows the decoder to \"look back\" at the entire input sequence and focus (attend) on the specific parts that are most relevant for generating the current word, rather than relying solely on a fixed-length context vector bottleneck.</span>\n",
        "\n",
        "4. **Input Representation**  \n",
        "   What is the dimensionality of each input vector in the sequences fed\n",
        "   into the model?  \n",
        "   Which constant in the implementation determines this dimension, and what does it represent?\n",
        "   \n",
        "   <span style=\"color:red\">The dimensionality of each input vector is determined by the embedding size. In the implementation, this is the `hidden_size` constant (e.g., 500). This represents the size of the dense vector space in which each word from the vocabulary is embedded.</span>\n",
        "\n",
        "5. **Batch Construction**  \n",
        "   How are training batches constructed in this assignment?  \n",
        "   Explain how variable-length sequences are handled and why padding,\n",
        "   masking, and sorting by sequence length are necessary.\n",
        "   \n",
        "   <span style=\"color:red\">Training batches are constructed by grouping a set of sentence pairs. \n",
        "   *   **Variable-length sequences:** Since tensors in a batch must have uniform shape, shorter sequences are padded with a special `PAD` token to match the length of the longest sequence in the batch.\n",
        "   *   **Masking:** A binary mask is used during loss calculation to ensure the model is not penalized for predicting `PAD` tokens correctly or incorrectly; we only care about the actual words.\n",
        "   *   **Sorting:** Batches are often sorted by sequence length (descending) to allow for efficient packing of sequences (using `pack_padded_sequence`), which optimizes computation by skipping processing of pad tokens.</span>\n",
        "\n",
        "6. **Packed Sequences Efficiency**  \n",
        "   The function `torch.nn.utils.rnn.pack_padded_sequence` improves\n",
        "   computational efficiency by removing padding tokens before passing\n",
        "   sequences to the RNN.\n",
        "\n",
        "   Given the following batch of sequences:\n",
        "\n",
        "    ```\n",
        "    [torch.tensor([10]),\n",
        "    torch.tensor([1, 2, 3]),\n",
        "    torch.tensor([4, 5]),\n",
        "    torch.tensor([6, 7, 8, 9])]\n",
        "    ```\n",
        "\n",
        "\n",
        "    By what factor does `pack_padded_sequence` reduce the number of inputs\n",
        "    processed by the RNN, compared to processing the padded batch directly?\n",
        "    Show your reasoning.\n",
        "    \n",
        "    <span style=\"color:red\">\n",
        "    First, we determine the shape of the padded batch. The longest sequence has length 4. There are 4 sequences.\n",
        "    Padded Batch dimensions: 4 (seq_len) x 4 (batch_size) = 16 total steps.\n",
        "    \n",
        "    Packed Sequence processes only valid tokens:\n",
        "    *   Seq 1 (len 1): 1 token\n",
        "    *   Seq 2 (len 3): 3 tokens\n",
        "    *   Seq 3 (len 2): 2 tokens\n",
        "    *   Seq 4 (len 4): 4 tokens\n",
        "    Total valid tokens = 1 + 3 + 2 + 4 = 10 tokens.\n",
        "    \n",
        "    Reduction Factor: 16 (padded) / 10 (packed) = 1.6.\n",
        "    The RNN processes 1.6 times fewer inputs using packed sequences.\n",
        "    </span>\n",
        ""
      ],
      "metadata": {
        "id": "lP0h0J-ws7RY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Neural Machine Translation with Sequence-to-Sequence Models (35 Points)\n",
        "\n",
        "In this question, we turn our attention to **Neural Machine Translation (NMT)** — the task of automatically translating sentences from one natural language to another using neural networks.\n",
        "\n",
        "Machine translation has long been a central problem in artificial intelligence and natural language processing. Early approaches relied on hand-crafted linguistic rules, followed by large-scale **statistical machine translation (SMT)** systems that modeled translation as a probabilistic inference problem over phrase tables and alignment models. While these methods achieved notable success, they required complex pipelines and extensive feature engineering.\n",
        "\n",
        "The advent of deep learning introduced a fundamentally different paradigm: instead of explicitly modeling alignments and translation rules, we can train a neural network to **directly map a sentence in one language to a sentence in another**. This idea was popularized by the *sequence-to-sequence* framework, where an encoder network reads an input sequence and a decoder network generates an output sequence, one token at a time.\n",
        "\n",
        "In this question, we study neural machine translation through a classical and instructive architecture: an **encoder–decoder model with recurrent neural networks**. Concretely, we will work with a **Spanish $\\to$ English** translation task, where each training example consists of a Spanish source sentence and its corresponding English translation.\n",
        "\n",
        "\n",
        "Throughout the question, we will move step by step from data representation, through model construction, to qualitative and quantitative analysis of translation behavior.\n"
      ],
      "metadata": {
        "id": "2pW6UubbC2KZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Start by Importing Libraries:"
      ],
      "metadata": {
        "id": "ZWXf089cnIeW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "\n",
        "import os\n",
        "import re\n",
        "import math\n",
        "import random\n",
        "import time\n",
        "from collections import Counter\n",
        "from dataclasses import dataclass\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.auto import tqdm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, Sampler\n",
        "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence"
      ],
      "metadata": {
        "id": "9tOqmeQt2w4d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Loading & Preprocessing\n"
      ],
      "metadata": {
        "id": "CEGQBb6z8V1g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The next step is to load and organize the data that will serve as the foundation for our translation model.\n",
        "\n",
        "We work with a **Portuguese–English parallel translation dataset derived from TED Talks**, where each example consists of a spoken Portuguese sentence and its corresponding English translation. The dataset contains sentence-level alignments, making it suitable for supervised sequence-to-sequence learning.\n",
        "\n",
        "Our dataset consists of **paired Portuguese–English sentences**, where each example contains a Portuguese source sentence and its corresponding English translation. Together, these pairs form a **parallel corpus**, which is the standard training format for neural machine translation systems.\n",
        "\n",
        "The data spans a wide range of sentence lengths, syntactic structures, and semantic content. Some sentence pairs are short and literal, while others involve rephrasing, reordering, or stylistic adaptation between languages. This variability reflects the nature of real-world translation tasks, where a correct translation often depends on meaning rather than surface-level word alignment.\n",
        "\n",
        "Because both the input and output are **natural language sequences of variable length**, this dataset immediately poses several practical challenges:\n",
        "- sentences must be represented in a numerical form,\n",
        "- different sentence lengths must be handled efficiently,\n",
        "- and the model must learn when to begin and when to stop generating a translation.\n",
        "\n",
        "Before defining any neural architecture, we therefore begin by\n",
        "reformatting the raw sentence pairs into structures that can be consumed by a learning algorithm.\n",
        "\n",
        "We start by examining a small subset of the data to understand its basic structure.\n"
      ],
      "metadata": {
        "id": "0LeZVocKoR-M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Run the following cells to load the translation dataset:"
      ],
      "metadata": {
        "id": "IQRKQxg1LXUt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install -U tensorflow-datasets\n",
        "\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "# sanity: make sure the dataset builder exists\n",
        "print(\"TFDS version:\", tfds.__version__)\n",
        "print(\"Has ted_hrlr_translate?\", \"ted_hrlr_translate\" in tfds.list_builders())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ohht5l8pBhfX",
        "outputId": "68426324-12b2-4450-ccff-268bbb010780"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TFDS version: 4.9.9\n",
            "Has ted_hrlr_translate? True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Portuguese → English TED Talks data\n",
        "train_ds = tfds.load(\"ted_hrlr_translate/pt_to_en\", split=\"train\", as_supervised=True)\n",
        "val_ds   = tfds.load(\"ted_hrlr_translate/pt_to_en\", split=\"validation\", as_supervised=True)\n",
        "test_ds  = tfds.load(\"ted_hrlr_translate/pt_to_en\", split=\"test\", as_supervised=True)\n",
        "\n",
        "print(train_ds, val_ds, test_ds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Qrs_yD4BnDw",
        "outputId": "12e86272-85f5-416a-b567-7a23e99b6f78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<_PrefetchDataset element_spec=(TensorSpec(shape=(), dtype=tf.string, name=None), TensorSpec(shape=(), dtype=tf.string, name=None))> <_PrefetchDataset element_spec=(TensorSpec(shape=(), dtype=tf.string, name=None), TensorSpec(shape=(), dtype=tf.string, name=None))> <_PrefetchDataset element_spec=(TensorSpec(shape=(), dtype=tf.string, name=None), TensorSpec(shape=(), dtype=tf.string, name=None))>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For convenience, the dataset is loaded using **TensorFlow Datasets**.\n",
        "However, all subsequent preprocessing, modeling, and training steps in\n",
        "this question are implemented entirely in **PyTorch**.\n",
        "\n",
        "We therefore begin by converting the TensorFlow tensors into standard\n",
        "Python lists that can be easily manipulated and later transformed into\n",
        "PyTorch tensors."
      ],
      "metadata": {
        "id": "crwxm4ZeLJSR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tfds_to_lists(ds, max_examples=None):\n",
        "    src, tgt = [], []\n",
        "    for i, (s, t) in enumerate(tfds.as_numpy(ds)):\n",
        "        src.append(s.decode(\"utf-8\"))\n",
        "        tgt.append(t.decode(\"utf-8\"))\n",
        "        if max_examples is not None and i + 1 >= max_examples:\n",
        "            break\n",
        "    return src, tgt\n",
        "\n",
        "# Limit sizes to keep training manageable\n",
        "MAX_TRAIN = 60000\n",
        "MAX_VAL   = 2000\n",
        "MAX_TEST  = 2000\n",
        "\n",
        "train_pt, train_en = tfds_to_lists(train_ds, MAX_TRAIN)\n",
        "val_pt,   val_en   = tfds_to_lists(val_ds,   MAX_VAL)\n",
        "test_pt,  test_en  = tfds_to_lists(test_ds,  MAX_TEST)\n",
        "\n",
        "print(\"Train:\", len(train_pt))\n",
        "print(\"Val:\", len(val_pt))\n",
        "print(\"Test:\", len(test_pt))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Z3km23RIe0I",
        "outputId": "f426311a-5ade-4f99-ca10-8e04ec024801"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: 51785\n",
            "Val: 1193\n",
            "Test: 1803\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As a sanity check, we will also print a small number of paired sentences\n",
        "to inspect the raw Portuguese–English data."
      ],
      "metadata": {
        "id": "DUA08EKvLzN1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(5):\n",
        "    print(\"PT:\", train_pt[i])\n",
        "    print(\"EN:\", train_en[i])\n",
        "    print(\"-\" * 60)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ogz2wqeZIxIp",
        "outputId": "9bad05e2-012f-4d11-9d96-b120d4148639"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PT: e quando melhoramos a procura , tiramos a única vantagem da impressão , que é a serendipidade .\n",
            "EN: and when you improve searchability , you actually take away the one advantage of print , which is serendipity .\n",
            "------------------------------------------------------------\n",
            "PT: mas e se estes fatores fossem ativos ?\n",
            "EN: but what if it were active ?\n",
            "------------------------------------------------------------\n",
            "PT: mas eles não tinham a curiosidade de me testar .\n",
            "EN: but they did n't test for curiosity .\n",
            "------------------------------------------------------------\n",
            "PT: e esta rebeldia consciente é a razão pela qual eu , como agnóstica , posso ainda ter fé .\n",
            "EN: and this conscious defiance is why i , as an agnostic , can still have faith .\n",
            "------------------------------------------------------------\n",
            "PT: `` `` '' podem usar tudo sobre a mesa no meu corpo . ''\n",
            "EN: you can use everything on the table on me .\n",
            "------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Tokenization\n"
      ],
      "metadata": {
        "id": "CGrv2VBMQC5o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "As discussed in class, neural networks do not operate directly on raw\n",
        "text. In order to process a sentence with a neural model, we must first\n",
        "convert it into a numerical representation.\n",
        "\n",
        "The standard approach is to represent each sentence as a sequence of\n",
        "**tokens**, where each token corresponds to a word drawn from a fixed\n",
        "vocabulary. Each word is then mapped to a unique integer index, so that a\n",
        "sentence can be viewed as a sequence of integers rather than a sequence\n",
        "of strings.\n",
        "\n",
        "In a translation setting, this process must be carried out **separately**\n",
        "for the source language (Portuguese) and the target language (English),\n",
        "since the two languages have distinct vocabularies and linguistic\n",
        "structure.\n",
        "\n",
        "As we have seen, practical sequence models also rely on a small number of\n",
        "**special tokens** that encode structural information:\n",
        "- a padding token, used to align sequences of different lengths,\n",
        "- a token marking the beginning of a target sentence,\n",
        "- a token marking the end of a target sentence,\n",
        "- and a token representing unknown or rare words.\n",
        "\n",
        "These tokens allow the model to distinguish between meaningful content\n",
        "and structural boundaries, and will play an important role throughout\n",
        "the translation pipeline.\n",
        "\n",
        "We therefore begin by explicitly defining these special tokens and\n",
        "setting up the basic machinery needed to map text sequences into\n",
        "numerical form.\n"
      ],
      "metadata": {
        "id": "e64MNzMMoWgs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this part, you will implement tokenization - the machinery that converts raw text sentences into sequences of integer indices.\n",
        "\n",
        "Specifically, you should:\n",
        "- define a simple tokenization scheme,\n",
        "- build separate vocabularies for Portuguese and English,\n",
        "- reserve indices for special tokens (`<pad>`, `<unk>`, `<bos>`, `<eos>`),\n",
        "- and implement methods for encoding sentences as index sequences and\n",
        "  decoding them back to text.\n"
      ],
      "metadata": {
        "id": "SoeAs6GdSdmB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reuse special tokens defined earlier\n",
        "PAD_TOKEN = \"<pad>\"\n",
        "UNK_TOKEN = \"<unk>\"\n",
        "BOS_TOKEN = \"<bos>\"\n",
        "EOS_TOKEN = \"<eos>\"\n",
        "\n",
        "SPECIAL_TOKENS = [PAD_TOKEN, UNK_TOKEN, BOS_TOKEN, EOS_TOKEN]\n",
        "\n",
        "def tokenize(sentence: str):\n",
        "    return sentence.strip().split()\n",
        "\n",
        "\n",
        "class Vocabulary:\n",
        "    def __init__(self, sentences, min_freq=2):\n",
        "        \"\"\"\n",
        "        sentences: list of raw text sentences\n",
        "        min_freq : minimum frequency for a word to be included\n",
        "        \"\"\"\n",
        "        counter = Counter()\n",
        "        for sent in sentences:\n",
        "            counter.update(tokenize(sent))\n",
        "\n",
        "        # Initialize vocab with special tokens\n",
        "        self.itos = list(SPECIAL_TOKENS)\n",
        "        self.stoi = {tok: i for i, tok in enumerate(self.itos)}\n",
        "\n",
        "        # Add frequent words\n",
        "        for word, freq in counter.items():\n",
        "            if freq >= min_freq and word not in self.stoi:\n",
        "                self.stoi[word] = len(self.itos)\n",
        "                self.itos.append(word)\n",
        "\n",
        "    def encode(self, sentence, add_bos=False, add_eos=False):\n",
        "        tokens = tokenize(sentence)\n",
        "        ids = [self.stoi.get(tok, self.stoi[UNK_TOKEN]) for tok in tokens]\n",
        "\n",
        "        if add_bos:\n",
        "            ids = [self.stoi[BOS_TOKEN]] + ids\n",
        "        if add_eos:\n",
        "            ids = ids + [self.stoi[EOS_TOKEN]]\n",
        "\n",
        "        return ids\n",
        "\n",
        "    def decode(self, ids):\n",
        "        words = []\n",
        "        for idx in ids:\n",
        "            word = self.itos[idx]\n",
        "            if word in {BOS_TOKEN, PAD_TOKEN}:\n",
        "                continue\n",
        "            if word == EOS_TOKEN:\n",
        "                break\n",
        "            words.append(word)\n",
        "        return \" \".join(words)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.itos)\n",
        "\n",
        "\n",
        "# Build vocabularies\n",
        "pt_vocab = Vocabulary(train_pt, min_freq=2)\n",
        "en_vocab = Vocabulary(train_en, min_freq=2)\n",
        "\n",
        "print(\"Portuguese vocab size:\", len(pt_vocab))\n",
        "print(\"English vocab size:\", len(en_vocab))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J4WcSbPXQNax",
        "outputId": "50f9c969-751e-40ea-f9af-5083f6961963"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Portuguese vocab size: 21778\n",
            "English vocab size: 16517\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before proceeding, we perform a simple sanity check to verify that the vocabulary and encoding logic behave as expected.\n",
        "\n",
        "In particular, we encode a sample sentence into a sequence of indices and then decode it back into text. This allows us to confirm that common words are preserved correctly and that rare or unseen words are handled in a consistent manner.\n"
      ],
      "metadata": {
        "id": "3bwOHEDESlwG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode and decode a sample sentence\n",
        "pt_example = train_pt[0]\n",
        "encoded = pt_vocab.encode(pt_example, add_bos=True, add_eos=True)\n",
        "decoded = pt_vocab.decode(encoded)\n",
        "\n",
        "print(\"Original:\", pt_example)\n",
        "print(\"Encoded:\", encoded[:10], \"...\")\n",
        "print(\"Decoded:\", decoded)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y7VRlXQjQkAm",
        "outputId": "c2270929-9a7f-49dc-caa1-e9784783eca3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original: e quando melhoramos a procura , tiramos a única vantagem da impressão , que é a serendipidade .\n",
            "Encoded: [2, 4, 5, 6, 7, 8, 9, 10, 7, 11] ...\n",
            "Decoded: e quando melhoramos a procura , tiramos a única vantagem da impressão , que é a <unk> .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Length-Aware Batching (Bucketing)\n"
      ],
      "metadata": {
        "id": "j0v29u4vcyDC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As discussed in class, natural language sentences vary significantly in\n",
        "length. When such sequences are grouped into mini-batches, shorter\n",
        "sentences must be padded to match the length of the longest sentence in\n",
        "the batch.\n",
        "\n",
        "Excessive padding is computationally wasteful: it increases memory\n",
        "usage, slows down training, and introduces artificial tokens that carry\n",
        "no semantic information. This issue becomes especially pronounced when a\n",
        "batch contains a mix of very short and very long sentences.\n",
        "\n",
        "A common remedy is **bucketing**, where examples of similar sequence\n",
        "lengths are grouped together into the same mini-batch. By batching\n",
        "sentences with comparable lengths, we substantially reduce the amount\n",
        "of padding required while preserving the efficiency benefits of\n",
        "mini-batch training.\n",
        "\n",
        "In this question, we adopt a BucketIterator-style batching strategy. We\n",
        "retain full control over tokenization and vocabulary construction, while\n",
        "using length-aware batching to ensure efficient training of our\n",
        "sequence-to-sequence model.\n"
      ],
      "metadata": {
        "id": "woZuujs5oaND"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TranslationDataset(Dataset):\n",
        "    def __init__(self, src_sentences, tgt_sentences, src_vocab, tgt_vocab):\n",
        "        assert len(src_sentences) == len(tgt_sentences)\n",
        "        self.src = src_sentences\n",
        "        self.tgt = tgt_sentences\n",
        "        self.src_vocab = src_vocab\n",
        "        self.tgt_vocab = tgt_vocab\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.src)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        src_ids = self.src_vocab.encode(self.src[idx])\n",
        "        tgt_ids = self.tgt_vocab.encode(self.tgt[idx], add_bos=True, add_eos=True)\n",
        "        return src_ids, tgt_ids\n",
        "\n",
        "def collate_fn(batch):\n",
        "    src_batch, tgt_batch = zip(*batch)\n",
        "\n",
        "    src_lens = torch.tensor([len(x) for x in src_batch], dtype=torch.long)\n",
        "    tgt_lens = torch.tensor([len(x) for x in tgt_batch], dtype=torch.long)\n",
        "\n",
        "    src_pad = pad_sequence(\n",
        "        [torch.tensor(x, dtype=torch.long) for x in src_batch],\n",
        "        batch_first=True,\n",
        "        padding_value=pt_vocab.stoi[PAD_TOKEN]\n",
        "    )\n",
        "    tgt_pad = pad_sequence(\n",
        "        [torch.tensor(x, dtype=torch.long) for x in tgt_batch],\n",
        "        batch_first=True,\n",
        "        padding_value=en_vocab.stoi[PAD_TOKEN]\n",
        "    )\n",
        "\n",
        "    return src_pad, src_lens, tgt_pad, tgt_lens\n",
        "\n",
        "class BucketBatchSampler(Sampler):\n",
        "    def __init__(self, lengths, batch_size, bucket_size=2048, shuffle=True, drop_last=False):\n",
        "        self.lengths = np.asarray(lengths)\n",
        "        self.batch_size = batch_size\n",
        "        self.bucket_size = bucket_size\n",
        "        self.shuffle = shuffle\n",
        "        self.drop_last = drop_last\n",
        "        self.indices = np.arange(len(self.lengths))\n",
        "\n",
        "    def __iter__(self):\n",
        "        idxs = self.indices.copy()\n",
        "        if self.shuffle:\n",
        "            np.random.shuffle(idxs)\n",
        "\n",
        "        for i in range(0, len(idxs), self.bucket_size):\n",
        "            bucket = idxs[i:i + self.bucket_size]\n",
        "            bucket = bucket[np.argsort(self.lengths[bucket])]\n",
        "\n",
        "            for j in range(0, len(bucket), self.batch_size):\n",
        "                batch = bucket[j:j + self.batch_size]\n",
        "                if self.drop_last and len(batch) < self.batch_size:\n",
        "                    continue\n",
        "                yield batch.tolist()\n",
        "\n",
        "    def __len__(self):\n",
        "        n = len(self.indices) // self.batch_size\n",
        "        if not self.drop_last and len(self.indices) % self.batch_size != 0:\n",
        "            n += 1\n",
        "        return n\n",
        "\n",
        "train_dataset = TranslationDataset(train_pt, train_en, pt_vocab, en_vocab)\n",
        "val_dataset   = TranslationDataset(val_pt,   val_en,   pt_vocab, en_vocab)\n",
        "\n",
        "train_src_lengths = [len(pt_vocab.encode(s)) for s in train_pt]\n",
        "val_src_lengths   = [len(pt_vocab.encode(s)) for s in val_pt]\n",
        "\n",
        "BATCH_SIZE = 16  # You may experiment with other batch sizes\n",
        "\n",
        "train_sampler = BucketBatchSampler(train_src_lengths, batch_size=BATCH_SIZE, bucket_size=2048, shuffle=True)\n",
        "val_sampler   = BucketBatchSampler(val_src_lengths,   batch_size=BATCH_SIZE, bucket_size=2048, shuffle=False)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_sampler=train_sampler, collate_fn=collate_fn)\n",
        "val_loader   = DataLoader(val_dataset,   batch_sampler=val_sampler,   collate_fn=collate_fn)\n",
        "\n",
        "print(\"Train batches:\", len(train_loader))\n",
        "print(\"Val batches:\", len(val_loader))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HWVnH9Dkc1Rd",
        "outputId": "37035f64-f08f-42e5-c76b-d489d81cac25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train batches: 3237\n",
            "Val batches: 75\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before introducing any neural model, we verify that batching behaves as\n",
        "expected.\n",
        "\n",
        "In particular, we check that:\n",
        "- sequences are padded correctly,\n",
        "- padding uses the `<pad>` token,\n",
        "- and the reported sequence lengths match the unpadded data.\n"
      ],
      "metadata": {
        "id": "f7hmaiYHdZbx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "src_pad, src_lens, tgt_pad, tgt_lens = next(iter(train_loader))\n",
        "\n",
        "print(\"Source batch shape:\", src_pad.shape)\n",
        "print(\"Target batch shape:\", tgt_pad.shape)\n",
        "print(\"Source lengths (first 5):\", src_lens[:5].tolist())\n",
        "print(\"Target lengths (first 5):\", tgt_lens[:5].tolist())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M2im-cXGdSAx",
        "outputId": "9e9ef2ef-1c64-4376-ff04-39c46df26161"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Source batch shape: torch.Size([16, 2])\n",
            "Target batch shape: torch.Size([16, 5])\n",
            "Source lengths (first 5): [1, 2, 2, 2, 2]\n",
            "Target lengths (first 5): [5, 5, 4, 5, 4]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Architecture\n"
      ],
      "metadata": {
        "id": "x082L1fQgX4Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now describe the complete neural machine translation model that we will implement in this question.\n",
        "\n",
        "Our system follows the classical **sequence-to-sequence (Seq2Seq)** framework with attention. The model consists of an **encoder** that reads a source sentence in Portuguese and a **decoder** that generates an English translation one token at a time. An **attention mechanism** connects the two, allowing the decoder to selectively focus on different parts of the source sentence during generation.\n",
        "\n",
        "The figure below illustrates the model at a single decoding timestep.\n",
        "The encoder processes the entire source sentence and produces a sequence of hidden states. At each decoding step, the decoder computes attention weights over these encoder states, forms a context vector, and combines it with its current hidden state to predict the next target word.\n",
        "\n",
        "In the remainder of this question, we will implement this model hierarchically, breaking it down into the following components.\n"
      ],
      "metadata": {
        "id": "bbICfqQ1oIWg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Model Architecture](https://raw.githubusercontent.com/NaamanKopty/Deep-Learning-Course/main/HW3/model_architecture.png)\n",
        "\n"
      ],
      "metadata": {
        "id": "Jjb3MljOyNxp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Encoder\n"
      ],
      "metadata": {
        "id": "TAMOeVizo5IY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "In this part, you will implement the **encoder**, which reads a Portuguese source sentence\n",
        "and produces contextual hidden representations for each source token.\n",
        "\n",
        "**What to define (layers):**\n",
        "1. **Source embedding**\n",
        "   - An embedding layer that maps token indices to vectors:\n",
        "     - input: `(B, T_src)` integer token ids\n",
        "     - output: `(B, T_src, E)` embeddings\n",
        "   - Make sure the `<pad>` token uses `padding_idx` so it does not get trained as a normal token.\n",
        "\n",
        "2. **Bidirectional LSTM**\n",
        "   - A 1-layer bidirectional LSTM that processes the embedded sequence.\n",
        "   - Use packed sequences (with `src_lens`) so the LSTM does not waste computation on padding.\n",
        "   - It should output encoder hidden states:\n",
        "     - `enc_hiddens`: `(B, T_src, 2H)`  (because bidirectional)\n",
        "\n",
        "3. **Dropout**\n",
        "   - Apply dropout to embeddings (and/or to the LSTM outputs) for regularization.\n",
        "\n",
        "**Forward input:**\n",
        "- `src_pad`: padded source batch of shape `(B, T_src)`\n",
        "- `src_lens`: true lengths of shape `(B,)`\n",
        "\n",
        "**Forward output:**\n",
        "- `enc_hiddens`: `(B, T_src, 2H)`\n",
        "- raw final states `(h_n, c_n)` from the BiLSTM:\n",
        "  - each of shape `(2, B, H)` (forward + backward)\n"
      ],
      "metadata": {
        "id": "jsgP5FEaoFTg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderBiLSTM(nn.Module):\n",
        "    def __init__(self, src_vocab_size, emb_dim, hidden_dim, pad_idx, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(src_vocab_size, emb_dim, padding_idx=pad_idx)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.lstm = nn.LSTM(emb_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
        "        self.pad_idx = pad_idx\n",
        "\n",
        "    def forward(self, src_pad, src_lens):\n",
        "        # src_pad: (B, T_src)\n",
        "        # src_lens: (B,)\n",
        "        \n",
        "        embedded = self.dropout(self.embedding(src_pad))\n",
        "        \n",
        "        # Pack padded sequence\n",
        "        # Note: enforce_sorted=False is generally safer if not pre-sorted\n",
        "        packed = pack_padded_sequence(embedded, src_lens.cpu(), batch_first=True, enforce_sorted=False)\n",
        "        \n",
        "        outputs, (hidden, cell) = self.lstm(packed)\n",
        "        \n",
        "        # Unpack\n",
        "        outputs, _ = pad_packed_sequence(outputs, batch_first=True)\n",
        "        \n",
        "        return outputs, (hidden, cell)"
      ],
      "metadata": {
        "id": "vTyAkF0jo-Ue"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Attention Mechanism\n"
      ],
      "metadata": {
        "id": "nnKyR-fguRXg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "In this part, we implement the **attention mechanism** that connects the decoder to the encoder.\n",
        "\n",
        "At each decoding timestep, the decoder has a hidden state `h_t` of shape `(B, H)`.\n",
        "The encoder produced a sequence of hidden states `enc_hiddens` of shape `(B, T_src, 2H)`.\n",
        "\n",
        "We compute a relevance score between `h_t` and every encoder state, normalize these scores with a softmax to obtain attention weights over the source positions, and then compute a **context vector** as a weighted sum of encoder states.\n",
        "\n",
        "Because the encoder states have dimension `2H` while the decoder state has dimension `H`, we first project encoder states to dimension `H` so that the dot product is well-defined.\n",
        "\n",
        "Padding positions in the source should not receive attention, so we apply a mask based on `src_lens`."
      ],
      "metadata": {
        "id": "fdE2gf0LoAL_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DotProductAttention(nn.Module):\n",
        "    def __init__(self, hidden_dim):\n",
        "        super().__init__()\n",
        "        # Project encoder states (2H) to decoder hidden dim (H)\n",
        "        self.W_p = nn.Linear(hidden_dim * 2, hidden_dim)\n",
        "\n",
        "    def forward(self, dec_h, enc_hiddens, src_lens):\n",
        "        # dec_h: (B, H)\n",
        "        # enc_hiddens: (B, T_src, 2H)\n",
        "        # src_lens: (B,)\n",
        "        \n",
        "        # Project encoder hiddens\n",
        "        enc_hiddens_proj = self.W_p(enc_hiddens) # (B, T_src, H)\n",
        "        \n",
        "        # Compute scores: (B, T_src)\n",
        "        # (B, H) unsqueeze -> (B, 1, H)\n",
        "        # (B, T_src, H) transpose -> (B, H, T_src)\n",
        "        scores = torch.bmm(enc_hiddens_proj, dec_h.unsqueeze(2)).squeeze(2)\n",
        "        \n",
        "        # Masking\n",
        "        max_len = enc_hiddens.size(1)\n",
        "        # mask[i, j] is True if j < len[i] (valid), False if padding\n",
        "        mask = torch.arange(max_len, device=scores.device).expand(len(src_lens), max_len) < src_lens.unsqueeze(1)\n",
        "        \n",
        "        scores = scores.masked_fill(~mask, -1e9)\n",
        "        \n",
        "        attn_weights = F.softmax(scores, dim=1) # (B, T_src)\n",
        "        \n",
        "        # Context vector: (B, 2H)\n",
        "        # (B, 1, T_src) x (B, T_src, 2H) -> (B, 1, 2H)\n",
        "        context = torch.bmm(attn_weights.unsqueeze(1), enc_hiddens).squeeze(1)\n",
        "        \n",
        "        return context, attn_weights"
      ],
      "metadata": {
        "id": "ULxOg2cHujEy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Decoder\n"
      ],
      "metadata": {
        "id": "JVYvYqzZumJl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "We now implement the **decoder**, which generates the English sentence autoregressively.\n",
        "\n",
        "During training we use **teacher forcing**:\n",
        "- the decoder input at time `t` is the *ground-truth* target token at time `t`\n",
        "- the model predicts the token at time `t+1`\n",
        "\n",
        "Before decoding begins, the decoder must be provided with an **initial hidden state** and **initial cell state**. Our encoder is a *bidirectional* LSTM, so its final states\n",
        "come from two directions (forward and backward). The decoder, however, is a *unidirectional* LSTM and therefore expects a single initial hidden/cell state.\n",
        "\n",
        "To bridge this mismatch, we define a small **decoder initializer** module that:\n",
        "1. Takes the encoder’s final forward and backward states,\n",
        "2. Concatenates them into a `(B, 2H)` vector,\n",
        "3. Projects them to `(B, H)` to obtain the decoder’s initial hidden and cell states.\n",
        "\n",
        "Once initialized, decoding proceeds as follows:\n",
        "1. Embed the input target tokens (excluding the last token).\n",
        "2. Run a unidirectional LSTM to update the decoder hidden state at each timestep.\n",
        "3. Use attention to compute a context vector from the encoder hidden states.\n",
        "4. Combine the decoder hidden state and context vector to form an **attentional vector** that will later be mapped to vocabulary logits.\n",
        "\n",
        "The decoder returns a sequence of attentional vectors of shape `(B, T_tgt-1, H)`.\n"
      ],
      "metadata": {
        "id": "NxPuFPMrn6ur"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderInit(nn.Module):\n",
        "    \\\"\\\"\\\"\n",
        "    Projects final BiLSTM encoder states -> initial UniLSTM decoder states.\n",
        "\n",
        "    Input:\n",
        "      h_n, c_n: (2, B, H)  (forward + backward)\n",
        "    Output:\n",
        "      h0_dec, c0_dec: (B, H)\n",
        "    \\\"\\\"\\\"\n",
        "    def __init__(self, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.hidden_proj = nn.Linear(hidden_dim * 2, hidden_dim)\n",
        "        self.cell_proj = nn.Linear(hidden_dim * 2, hidden_dim)\n",
        "\n",
        "    def forward(self, h_n, c_n):\n",
        "        # Concatenate forward and backward states along dim 1\n",
        "        # h_n shape is (num_directions, batch, hidden_size) -> (2, B, H)\n",
        "        # Cat (B, H) and (B, H) -> (B, 2H)\n",
        "        h_cat = torch.cat((h_n[0], h_n[1]), dim=1)\n",
        "        c_cat = torch.cat((c_n[0], c_n[1]), dim=1)\n",
        "        \n",
        "        h0_dec = torch.tanh(self.hidden_proj(h_cat))\n",
        "        c0_dec = torch.tanh(self.cell_proj(c_cat))\n",
        "        \n",
        "        return h0_dec, c0_dec"
      ],
      "metadata": {
        "id": "wG5tH0Wn6fLG"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, tgt_vocab_size, emb_dim, hidden_dim, pad_idx, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(tgt_vocab_size, emb_dim, padding_idx=pad_idx)\n",
        "        self.lstm = nn.LSTMCell(emb_dim, hidden_dim)\n",
        "        self.attention = DotProductAttention(hidden_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "        # Combine hidden (H) + context (2H) -> H\n",
        "        self.combine_proj = nn.Linear(hidden_dim + hidden_dim * 2, hidden_dim)\n",
        "\n",
        "    def forward(self, tgt_pad, dec_init_state, enc_hiddens, src_lens):\n",
        "        \\\"\\\"\\\"\n",
        "        tgt_pad:       (B, T_tgt) includes <bos> ... <eos>\n",
        "        dec_init_state: (h0_dec, c0_dec) each (B, H)\n",
        "        enc_hiddens:   (B, T_src, 2H)\n",
        "        src_lens:      (B,)\n",
        "\n",
        "        returns:\n",
        "          attn_vecs: (B, T_tgt-1, H)\n",
        "        \\\"\\\"\\\"\n",
        "        batch_size, seq_len = tgt_pad.size()\n",
        "        h, c = dec_init_state\n",
        "        \n",
        "        embedded = self.embedding(tgt_pad) # (B, T, E)\n",
        "        embedded = self.dropout(embedded)\n",
        "        \n",
        "        attn_vecs = []\n",
        "        \n",
        "        # Decode T-1 steps (predicting 2nd token onwards)\n",
        "        # Using teacher forcing: input at t is tgt_pad[:, t]\n",
        "        for t in range(seq_len - 1):\n",
        "            x_t = embedded[:, t, :] # (B, E)\n",
        "            \n",
        "            h, c = self.lstm(x_t, (h, c))\n",
        "            \n",
        "            context, _ = self.attention(h, enc_hiddens, src_lens)\n",
        "            \n",
        "            combined = torch.cat((h, context), dim=1)\n",
        "            attn_vec = torch.tanh(self.combine_proj(combined))\n",
        "            \n",
        "            attn_vecs.append(attn_vec)\n",
        "            \n",
        "        attn_vecs = torch.stack(attn_vecs, dim=1) # (B, T-1, H)\n",
        "        return attn_vecs"
      ],
      "metadata": {
        "id": "Fty4zSN4uuZ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Output Projection\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5KpEQEjIuwxA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "At each decoding timestep, the decoder produces an attentional vector of shape `(B, H)`.\n",
        "This vector is a continuous representation, but the task requires predicting a discrete token from the target vocabulary.\n",
        "\n",
        "We therefore apply a learned linear transformation that maps:\n",
        "- from hidden dimension `H`\n",
        "- to target vocabulary size `|V_tgt|`\n",
        "\n",
        "This produces **logits** (unnormalized scores) of shape `(B, T_tgt-1, |V_tgt|)`.\n",
        "During training, these logits are used with the cross-entropy loss (while ignoring `<pad>` tokens)."
      ],
      "metadata": {
        "id": "eCe24yXYn0yk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class OutputProjection(nn.Module):\n",
        "    def __init__(self, hidden_dim, tgt_vocab_size):\n",
        "        super().__init__()\n",
        "        self.proj = nn.Linear(hidden_dim, tgt_vocab_size)\n",
        "\n",
        "    def forward(self, attn_vecs):\n",
        "        \\\"\\\"\\\"\n",
        "        attn_vecs: (B, T, H)\n",
        "        returns:\n",
        "          logits: (B, T, |V_tgt|)\n",
        "        \\\"\\\"\\\"\n",
        "        return self.proj(attn_vecs)"
      ],
      "metadata": {
        "id": "HIhXEk-Tu2Tt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Full Model Definition\n"
      ],
      "metadata": {
        "id": "i9JxeR0SxDrr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "At this stage, all individual components of the translation model have been implemented: the encoder, the decoder initialization, the decoder with attention, and the output projection.\n",
        "\n",
        "The full translation model simply **connects these components into a single forward computation**. It does not introduce any new architectural ideas; rather, it defines how data flows through the model during training.\n",
        "\n",
        "Concretely, the full model performs the following steps:\n",
        "1. Encode the source sentence using the encoder to obtain encoder hidden states.\n",
        "2. Use the final encoder states to initialize the decoder’s hidden and cell states.\n",
        "3. Run the decoder with attention over the encoder states to produce a sequence of decoder representations.\n",
        "4. Apply the output projection to map decoder representations to vocabulary logits.\n",
        "\n",
        "The model’s forward method returns the unnormalized logits for all target\n",
        "positions, which will later be used to compute the training loss.\n"
      ],
      "metadata": {
        "id": "tVxm5_u2nxsX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Seq2SeqNMT(nn.Module):\n",
        "    def __init__(self, encoder, decoder, dec_init, out_proj):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.dec_init = dec_init\n",
        "        self.out_proj = out_proj\n",
        "\n",
        "    def forward(self, src_pad, src_lens, tgt_pad):\n",
        "        enc_hiddens, (h_n, c_n) = self.encoder(src_pad, src_lens)\n",
        "        dec_init_state = self.dec_init(h_n, c_n)\n",
        "        attn_vecs = self.decoder(tgt_pad, dec_init_state, enc_hiddens, src_lens)\n",
        "        logits = self.out_proj(attn_vecs)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "yRaWbKbswAiw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training\n"
      ],
      "metadata": {
        "id": "XLrZ0Wtjxo5b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "We now train our neural machine translation model end-to-end.\n",
        "\n",
        "The model is trained for **25 epochs** using the **Adam** optimizer. The training objective is **token-level cross-entropy loss**, computed over the target sequence under **teacher forcing**. Padding tokens (`<pad>`) do not correspond to real words and are therefore ignored when computing the loss.\n",
        "\n",
        "After each training epoch, the model is evaluated on a held-out **validation set** (without updating parameters). We record:\n",
        "- the average **training loss per token**\n",
        "- the average **validation loss per token**\n",
        "\n",
        "Monitoring both quantities allows us to track learning progress and identify potential overfitting.\n",
        "\n",
        "Finally, we plot the training and validation loss curves as a function of epoch. Both curves should appear on the same graph for comparison.\n"
      ],
      "metadata": {
        "id": "zt0GGn4eoL9Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "EMB_DIM = 256\n",
        "HIDDEN_DIM = 512\n",
        "DROPOUT = 0.5\n",
        "N_EPOCHS = 25\n",
        "LEARNING_RATE = 0.001\n",
        "CLIP = 1.0\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Initialize components\n",
        "encoder = EncoderBiLSTM(len(pt_vocab), EMB_DIM, HIDDEN_DIM, pt_vocab.stoi[PAD_TOKEN], DROPOUT)\n",
        "decoder = Decoder(len(en_vocab), EMB_DIM, HIDDEN_DIM, en_vocab.stoi[PAD_TOKEN], DROPOUT)\n",
        "dec_init = DecoderInit(HIDDEN_DIM)\n",
        "out_proj = OutputProjection(HIDDEN_DIM, len(en_vocab))\n",
        "\n",
        "model = Seq2SeqNMT(encoder, decoder, dec_init, out_proj).to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=en_vocab.stoi[PAD_TOKEN])\n",
        "\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "print(f\"Training on {device}...\")\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    \n",
        "    # Train Loop\n",
        "    for src_pad, src_lens, tgt_pad, tgt_lens in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{N_EPOCHS}\", leave=False):\n",
        "        src_pad, tgt_pad = src_pad.to(device), tgt_pad.to(device)\n",
        "        # Note: src_lens should stay on CPU for pack_padded_sequence if required by version, \n",
        "        # but my EncoderBiLSTM calls .cpu() on it, so it's safe to be on any device.\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # Forward\n",
        "        logits = model(src_pad, src_lens, tgt_pad)\n",
        "        \n",
        "        # Targets are shifted by 1 (exclude BOS)\n",
        "        targets = tgt_pad[:, 1:]\n",
        "        \n",
        "        # Flatten\n",
        "        logits = logits.reshape(-1, logits.shape[-1])\n",
        "        targets = targets.reshape(-1)\n",
        "        \n",
        "        loss = criterion(logits, targets)\n",
        "        loss.backward()\n",
        "        \n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), CLIP)\n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "        \n",
        "    avg_train_loss = epoch_loss / len(train_loader)\n",
        "    train_losses.append(avg_train_loss)\n",
        "    \n",
        "    # Validation Loop\n",
        "    model.eval()\n",
        "    epoch_val_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for src_pad, src_lens, tgt_pad, tgt_lens in val_loader:\n",
        "            src_pad, tgt_pad = src_pad.to(device), tgt_pad.to(device)\n",
        "            logits = model(src_pad, src_lens, tgt_pad)\n",
        "            targets = tgt_pad[:, 1:]\n",
        "            \n",
        "            logits = logits.reshape(-1, logits.shape[-1])\n",
        "            targets = targets.reshape(-1)\n",
        "            \n",
        "            loss = criterion(logits, targets)\n",
        "            epoch_val_loss += loss.item()\n",
        "            \n",
        "    avg_val_loss = epoch_val_loss / len(val_loader)\n",
        "    val_losses.append(avg_val_loss)\n",
        "    \n",
        "    print(f\"Epoch {epoch+1}: Train Loss = {avg_train_loss:.4f} | Val Loss = {avg_val_loss:.4f}\")\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(train_losses, label='Train Loss')\n",
        "plt.plot(val_losses, label='Val Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "anHH0EC7xs0K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### BLEU Evaluation\n",
        "\n",
        "After training is complete, we evaluate the translation quality of the model using the **BLEU (Bilingual Evaluation Understudy)** score.\n",
        "\n",
        "BLEU is a **corpus-level** evaluation metric for machine translation. It is computed by comparing a set of model-generated translations (*hypotheses*) against ground-truth translations (*references*). BLEU measures the overlap of n-grams (typically 1- to 4-grams) between the hypotheses and references, and applies a **brevity penalty** to discourage\n",
        "overly short translations.\n",
        "\n",
        "Importantly, BLEU is **not computed during training** and is **not differentiable**. It is used purely for evaluation.\n",
        "\n",
        "</br>\n",
        "</br>\n",
        "\n",
        "**How BLEU is computed in this question**\n",
        "\n",
        "1. The trained model is used to **decode full translations** for each source sentence in the **test set**, using an autoregressive decoding strategy. For this part, use beam search.\n",
        "2. For each test example, the generated translation is treated as the hypothesis, and the ground-truth English sentence is treated as the reference.\n",
        "3. The BLEU score is computed over the entire test set (corpus-level BLEU), and the final BLEU score is reported.\n",
        "\n",
        "\n",
        "**Implement a BLEU evaluation routine** that decodes translations for the test set using beam search and computes the corpus-level BLEU score against the reference translations.\n",
        "\n",
        "> You may use your implementation of beam search from Q1."
      ],
      "metadata": {
        "id": "Gm96Mp3NmDDT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def beam_search_decode(model, src_sentence, beam_size=5, max_len=50):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        # Encode\n",
        "        src_ids = pt_vocab.encode(src_sentence)\n",
        "        # Add batch dim\n",
        "        src_tensor = torch.tensor([src_ids], dtype=torch.long, device=device)\n",
        "        src_len = torch.tensor([len(src_ids)], dtype=torch.long)\n",
        "        \n",
        "        enc_hiddens, (h_n, c_n) = model.encoder(src_tensor, src_len)\n",
        "        h, c = model.dec_init(h_n, c_n)\n",
        "        \n",
        "        # Candidates: (score, [token_ids], (h, c))\n",
        "        # Start with BOS\n",
        "        candidates = [(0.0, [en_vocab.stoi[BOS_TOKEN]], (h, c))]\n",
        "        completed = []\n",
        "        \n",
        "        for _ in range(max_len):\n",
        "            new_candidates = []\n",
        "            for score, seq, (curr_h, curr_c) in candidates:\n",
        "                if seq[-1] == en_vocab.stoi[EOS_TOKEN]:\n",
        "                    completed.append((score, seq))\n",
        "                    continue\n",
        "                \n",
        "                inp_token = torch.tensor([seq[-1]], dtype=torch.long, device=device)\n",
        "                \n",
        "                # Decoder Step (manual expansion of Decoder logic)\n",
        "                # 1. Embed\n",
        "                embed = model.decoder.embedding(inp_token) # (1, E)\n",
        "                embed = model.decoder.dropout(embed) # Apply dropout? Usually not in eval, but model.eval() handles it.\n",
        "                \n",
        "                # 2. LSTM\n",
        "                new_h, new_c = model.decoder.lstm(embed, (curr_h, curr_c))\n",
        "                \n",
        "                # 3. Attention\n",
        "                context, _ = model.decoder.attention(new_h, enc_hiddens, src_len.to(device))\n",
        "                \n",
        "                # 4. Combine\n",
        "                combined = torch.cat((new_h, context), dim=1)\n",
        "                attn_vec = torch.tanh(model.decoder.combine_proj(combined))\n",
        "                \n",
        "                # 5. Output Proj\n",
        "                logits = model.out_proj(attn_vec)\n",
        "                log_probs = F.log_softmax(logits, dim=1)\n",
        "                \n",
        "                topv, topi = log_probs.topk(beam_size)\n",
        "                \n",
        "                for v, i in zip(topv[0], topi[0]):\n",
        "                    new_candidates.append((score + v.item(), seq + [i.item()], (new_h, new_c)))\n",
        "                    \n",
        "            if not new_candidates:\n",
        "                break\n",
        "                \n",
        "            new_candidates.sort(key=lambda x: x[0], reverse=True)\n",
        "            candidates = new_candidates[:beam_size]\n",
        "            \n",
        "        if not completed:\n",
        "            completed = [(c[0], c[1]) for c in candidates]\n",
        "            \n",
        "        completed.sort(key=lambda x: x[0], reverse=True)\n",
        "        best_seq = completed[0][1]\n",
        "        \n",
        "        return en_vocab.decode(best_seq)\n",
        "\n",
        "def compute_bleu(references, hypotheses):\n",
        "    # Calculate BLEU-4 score\n",
        "    precisions = []\n",
        "    for n in range(1, 5):\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for ref, hyp in zip(references, hypotheses):\n",
        "            ref_tokens = ref.split()\n",
        "            hyp_tokens = hyp.split()\n",
        "            \n",
        "            ref_counts = Counter([tuple(ref_tokens[i:i+n]) for i in range(len(ref_tokens)-n+1)])\n",
        "            hyp_counts = Counter([tuple(hyp_tokens[i:i+n]) for i in range(len(hyp_tokens)-n+1)])\n",
        "            \n",
        "            for gram, count in hyp_counts.items():\n",
        "                total += count\n",
        "                correct += min(count, ref_counts.get(gram, 0))\n",
        "        \n",
        "        if total > 0:\n",
        "            precisions.append(correct / total)\n",
        "        else:\n",
        "            precisions.append(0)\n",
        "    \n",
        "    if min(precisions) == 0:\n",
        "        return 0.0\n",
        "        \n",
        "    geo_mean = math.exp(sum(math.log(p) for p in precisions) / 4)\n",
        "    \n",
        "    ref_len = sum(len(r.split()) for r in references)\n",
        "    hyp_len = sum(len(h.split()) for h in hypotheses)\n",
        "    \n",
        "    if hyp_len > ref_len:\n",
        "        bp = 1.0\n",
        "    else:\n",
        "        bp = math.exp(1 - ref_len / hyp_len) if hyp_len > 0 else 0\n",
        "        \n",
        "    return bp * geo_mean\n",
        "\n",
        "# Evaluate on Test Set\n",
        "print(\"Evaluating on Test Set...\")\n",
        "hypotheses = []\n",
        "references = []\n",
        "\n",
        "# Take a subset if needed, but instructions imply full test set (or what is loaded)\n",
        "# We loaded test_pt and test_en (2000 examples)\n",
        "for pt, en in tqdm(zip(test_pt, test_en), total=len(test_pt), desc=\"Decoding\"):\n",
        "    hyp = beam_search_decode(model, pt, beam_size=5)\n",
        "    hypotheses.append(hyp)\n",
        "    references.append(en)\n",
        "\n",
        "bleu_score = compute_bleu(references, hypotheses)\n",
        "print(f\"Corpus-Level BLEU Score: {bleu_score*100:.2f}\")\n",
        "\n",
        "# Show some examples\n",
        "print(\"\\nExample Translations:\")\n",
        "for i in range(5):\n",
        "    print(f\"Src: {test_pt[i]}\")\n",
        "    print(f\"Ref: {test_en[i]}\")\n",
        "    print(f\"Hyp: {hypotheses[i]}\")\n",
        "    print(\"-\" * 30)"
      ],
      "metadata": {
        "id": "Bd8Fg-MhmGAB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Understanding & Reflection Questions\n"
      ],
      "metadata": {
        "id": "cbgaB8oPtGsT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Answer the following questions in your own words.  \n",
        "Your answers should demonstrate conceptual understanding rather than\n",
        "code-level details.\n",
        "\n",
        "1. **Parallel Corpus Understanding**  \n",
        "   What is a *parallel corpus* in machine translation?  \n",
        "   In this assignment, what constitutes a single training example, and why is this format appropriate for NMT?\n",
        "   \n",
        "   <span style=\"color:red\">A parallel corpus is a dataset consisting of texts in one language aligned with their translations in another language. In this assignment, a single training example constitutes a pair of sentences: one in Portuguese (source) and its corresponding translation in English (target). This format provides the direct supervision needed for the model to learn the mapping from the source language domain to the target language domain.</span>\n",
        "\n",
        "2. **Vocabulary & Special Tokens**  \n",
        "   Which special tokens are required for training and decoding (e.g., `<pad>`, `<unk>`, `<sos>`, `<eos>`)?  \n",
        "   Explain the role of each token in **both** training (teacher forcing) and inference (decoding).\n",
        "   \n",
        "   <span style=\"color:red\">\n",
        "   *   `<pad>`: Used to fill shorter sequences to match batch length. Ignored during training loss and inference.\n",
        "   *   `<unk>`: Represents unknown words not in the vocabulary, allowing the model to handle out-of-vocabulary terms gracefully.\n",
        "   *   `<bos>` (or `<sos>`): Start-of-sentence. In training, it's the first input to the decoder. In inference, it initializes the decoding process.\n",
        "   *   `<eos>`: End-of-sentence. In training, it marks the end of the target. In inference, the model generates this to signal it has finished the translation.\n",
        "   </span>\n",
        "\n",
        "3. **Encoder–Decoder Interface**  \n",
        "   Our encoder is a **bidirectional** LSTM while our decoder is **unidirectional**.  \n",
        "   Explain why this creates a mismatch in the final states, and how we resolve it to initialize the decoder.\n",
        "   \n",
        "   <span style=\"color:red\">A bidirectional LSTM produces two final hidden states (one for forward, one for backward), effectively doubling the hidden state dimension (or count) compared to a unidirectional LSTM of the same size. The decoder is unidirectional and expects a single hidden state vector. We resolve this mismatch by concatenating the forward and backward encoder states and passing them through a linear projection layer (often with a non-linearity like tanh) to compress/transform them into the shape required to initialize the decoder's hidden state.</span>\n",
        "\n",
        "4. **Attention Mechanism (High-level)**  \n",
        "   What problem does attention solve in neural machine translation?  \n",
        "   Describe (in words) how the decoder uses attention at a single timestep to choose which source positions to focus on.\n",
        "   \n",
        "   <span style=\"color:red\">Attention solves the \"information bottleneck\" problem where the encoder must compress the entire source sentence into a single fixed-length vector. At each timestep, the decoder uses its current hidden state to compute a relevance score for every encoder hidden state (source word). These scores are normalized into probabilities (weights). The decoder then calculates a weighted sum of the encoder states (context vector), effectively \"focusing\" on the most relevant parts of the source sentence to help generate the next word.</span>\n",
        "\n",
        "5. **Training vs. Decoding**  \n",
        "   During training we use teacher forcing, but at test time the model must generate translations autoregressively.  \n",
        "   Explain the difference between these two settings, and describe one potential consequence of this mismatch.\n",
        "   \n",
        "   <span style=\"color:red\">**Training (Teacher Forcing):** The model is fed the *ground-truth* previous token as input for the current step, regardless of what it predicted.\n",
        "   **Decoding (Autoregressive):** The model is fed its *own predicted* token from the previous step as input.\n",
        "   **Consequence:** This creates \"exposure bias.\" The model may become reliant on perfect inputs during training and fail to recover from its own errors during inference, leading to compounding errors as generation proceeds.</span>\n",
        "\n",
        "6. **BLEU Interpretation**  \n",
        "   What does BLEU measure at a high level, and why is it computed at the **corpus level** rather than per sentence?  \n",
        "   Briefly explain how the decoding strategy (greedy vs. beam search) can affect BLEU.\n",
        "   \n",
        "   <span style=\"color:red\">BLEU measures the n-gram overlap (precision) between the generated translation and reference translations, penalized for brevity. It is computed at the corpus level to provide a statistically robust metric, as n-gram matches for individual sentences can be sparse and noisy. Beam search typically yields higher BLEU scores than greedy decoding because it explores multiple valid paths to find a sequence with higher overall probability, often resulting in more coherent and grammatical translations that better match the references.</span>\n",
        "   \n",
        "7. **Length Effects**  \n",
        "   In practice, how does translation quality tend to change as sentence length increases?  \n",
        "   Give one reason why longer sentences are harder for sequence-to-sequence models.\n",
        "   \n",
        "   <span style=\"color:red\">Translation quality generally degrades as sentence length increases. One reason is that the fixed-size context vector (in non-attention models) cannot capture all information from a long sequence. Even with attention, long-range dependencies become harder to model, and the attention mechanism might become \"distracted\" or unfocused over very long sequences, leading to repetitions or omissions.</span>\n",
        ""
      ],
      "metadata": {
        "id": "tLsoH5UEtjHX"
      }
    }
  ]
}