{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AvCDNp3quvSK"
      },
      "source": [
        "# Deep Learning: Assignment #2\n",
        "## Submission date: 24/12/2025, 23:59.\n",
        "### Topics:\n",
        "- Regularization\n",
        "- Batch Normalization\n",
        "- Convolutional Neural Networks\n",
        "- Semantic Segmentation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d_Es654DuyKr"
      },
      "source": [
        "**Submitted by:**\n",
        "\n",
        "- **Student 1 (Name, ID)**\n",
        "- **Student 2 (Name, ID)**  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-JwMUBs3u0Wh"
      },
      "source": [
        "**Assignment Instructions:**\n",
        "\n",
        "· Submissions are in **pairs only**. Write both names + IDs at the top of the notebook.\n",
        "\n",
        "· Keep your code **clean, concise, and readable**.\n",
        "\n",
        "· You may work in your IDE, but you **must** paste the final code back into the **matching notebook cells** and run it there.  \n",
        "\n",
        "\n",
        "· <font color='red'>Write your textual answers in red.</font>  \n",
        "(e.g., `<span style=\"color:red\">your answer here</span>`)\n",
        "\n",
        "· All figures, printed results, and outputs should remain visible in the notebook.  \n",
        "Run **all cells** before submitting and **do not clear outputs**.\n",
        "\n",
        "· Use relative paths — **no absolute file paths** pointing to local machines.\n",
        "\n",
        "· **Important:** Your submission must be entirely your own.  \n",
        "Any form of plagiarism (including uncredited use of ChatGPT or AI tools) will result in **grade 0** and disciplinary action.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36af53b6"
      },
      "source": [
        "# Task\n",
        "Set up Kaggle API credentials by creating the `~/.kaggle` directory, writing the provided username and API key to `~/.kaggle/kaggle.json`, setting appropriate file permissions for `kaggle.json`, and then confirming the successful setup."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7ff3da4"
      },
      "source": [
        "## Create Kaggle directory\n",
        "\n",
        "### Subtask:\n",
        "Create the ~/.kaggle directory if it doesn't already exist to store the Kaggle API credentials.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "352cbaf8"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask is to create the `~/.kaggle` directory if it doesn't already exist. The provided instructions specify using `!mkdir -p ~/.kaggle` to achieve this. This command will be executed in a code cell.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2eb6e187",
        "outputId": "fa68a5d3-2144-423a-c9aa-8b8327aeb4ff"
      },
      "source": [
        "!pip install -q kaggle\n",
        "!mkdir -p ~/.kaggle\n",
        "print(\"Kaggle directory created or already exists.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Kaggle directory created or already exists.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3d24c2ed"
      },
      "source": [
        "## Create kaggle.json and write credentials\n",
        "\n",
        "### Subtask:\n",
        "Create the kaggle.json file inside ~/.kaggle and write the provided username ('mahmoud abade') and API key ('KGAT_9252f19df0b8b2f8c8407589e61caf35') into it in the correct JSON format.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74bb06ec"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask is to create the kaggle.json file with the specified credentials. This code block will define the content, create the file, and write the JSON data to it.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ebb5541",
        "outputId": "04e59a66-651d-4d97-8cff-e38382b4cd6b"
      },
      "source": [
        "import json\n",
        "import os\n",
        "\n",
        "# Define Kaggle credentials\n",
        "kaggle_username = 'mahmoud abade'\n",
        "kaggle_api_key = 'KGAT_9252f19df0b8b2f8c8407589e61caf35'\n",
        "\n",
        "# Create the JSON content\n",
        "kaggle_json_content = {\n",
        "    \"username\": kaggle_username,\n",
        "    \"key\": kaggle_api_key\n",
        "}\n",
        "\n",
        "# Define the path to kaggle.json\n",
        "kaggle_dir = os.path.expanduser('~/.kaggle')\n",
        "kaggle_json_path = os.path.join(kaggle_dir, 'kaggle.json')\n",
        "\n",
        "# Write the JSON content to the file\n",
        "with open(kaggle_json_path, 'w') as f:\n",
        "    json.dump(kaggle_json_content, f)\n",
        "\n",
        "print(f\"Created {kaggle_json_path} with Kaggle credentials.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created /root/.kaggle/kaggle.json with Kaggle credentials.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5973a763"
      },
      "source": [
        "**Reasoning**:\n",
        "The next step in setting up Kaggle API credentials is to set the appropriate file permissions for the `kaggle.json` file to ensure its security.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "75416e1d",
        "outputId": "d0d92f52-17fa-490a-e763-bf9c8d8606f1"
      },
      "source": [
        "import os\n",
        "\n",
        "kaggle_json_path = os.path.join(os.path.expanduser('~/.kaggle'), 'kaggle.json')\n",
        "os.chmod(kaggle_json_path, 0o600)\n",
        "\n",
        "print(f\"Set permissions for {kaggle_json_path} to 0o600 (read/write for owner only).\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Set permissions for /root/.kaggle/kaggle.json to 0o600 (read/write for owner only).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "517480a0"
      },
      "source": [
        "## Final Task\n",
        "\n",
        "### Subtask:\n",
        "Confirm that the Kaggle API key has been successfully set up.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! kaggle datasets download jcoral02/camvid\n",
        "! unzip -q camvid.zip -d Question2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3egUCISMLsaZ",
        "outputId": "5419f1fa-33f5-48de-ce72-c9cccec4c3aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/jcoral02/camvid\n",
            "License(s): unknown\n",
            "camvid.zip: Skipping, found more recently modified local copy (use --force to force download)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6Ctiv2VvlD4"
      },
      "source": [
        "# Question 1: Convolutional Digit Classification on SVHN (25 Points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MMzp0-8qvkIw"
      },
      "source": [
        "In this question, our goal is to implement a Convolutional Neural Network (CNN) for image classification on the The Street View House Numbers (SVHN) Dataset. The dataset consists of read-world house number images.\n",
        "\n",
        "**source:** http://ufldl.stanford.edu/housenumbers/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-FDi1Yegv3iK"
      },
      "source": [
        "### Data Loading and Preprocessing\n",
        "\n",
        "In this section we will load, explore and preprocess the dataset for training.\n",
        "\n",
        "You are given the **SVHN** (Street View House Numbers) dataset: a collection of real-world images of digits (0–9) captured from house numbers in Google Street View. Each image is 32×32 pixels and contains three color channels (RGB). The goal is to classify each image into one of the 10 digit classes (0 – 9).\n",
        "\n",
        "The dataset will be downloaded automatically to the local environment using the `torchvision.datasets.SVHN` class.\n",
        "\n",
        "For this section, implement the preprocessing procedure and explain your choice, then create the loaders for the train and test sets.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q35C2NHnzXvv"
      },
      "outputs": [],
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    # TODO: Define your desired transformation\n",
        "])\n",
        "\n",
        "# Load the SVHN Letters dataset\n",
        "train_dataset = datasets.SVHN(root='./data', split='train', download=True, transform=transform)\n",
        "test_dataset  = datasets.SVHN(root='./data', split='test', download=True, transform=transform)\n",
        "\n",
        "# Inspect the dataset\n",
        "print(f\"Number of training samples: {len(train_dataset)}\")\n",
        "print(f\"Number of testing samples: {len(test_dataset)}\")\n",
        "\n",
        "# Get one image and label\n",
        "image, label = train_dataset[0]\n",
        "print(f\"Shape of one image: {image.shape} (C x H x W)\")\n",
        "print(f\"Label of first image: {label}\")\n",
        "\n",
        "batch_size = 128  # you can tweak\n",
        "\n",
        "train_loader = # Implement\n",
        "test_loader = # Implement"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NAiMm-AqzrJh"
      },
      "source": [
        "Finally, run the cell below to take a look at a few sample images to better understand the dataset we're working with."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3S5idgUvzwct"
      },
      "outputs": [],
      "source": [
        "images, labels = next(iter(train_loader))\n",
        "\n",
        "# Show first 4 images\n",
        "fig, axes = plt.subplots(1, 4, figsize=(10, 4))\n",
        "for i in range(4):\n",
        "  img = images[i].permute(1, 2, 0).numpy()\n",
        "  #img = np.clip(img * 0.229 + 0.485, 0, 1)  # unnormalize for display\n",
        "  axes[i].imshow(img)\n",
        "  axes[i].set_title(f\"Class {labels[i].item()}\")\n",
        "  axes[i].axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NgS5vYD4gxHc"
      },
      "source": [
        "**Answer the following Questions:**\n",
        "\n",
        "<font color=\"red\">1. How is SVHN fundamentally harder than MNIST?</font>  \n",
        "<font color=\"red\">2. Which preprocessing or architectural choices become necessary because of this difference?</font>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HMLJXzhP0Ghv"
      },
      "source": [
        "### CNN Architecture Design\n",
        "\n",
        "We will design a convolutional neural network (inspired by AlexNet) for digit classification.\n",
        "The network consists of **three convolutional feature-extraction stages**, followed by a **two-layer fully connected classifier**.\n",
        "\n",
        "Your architecture must follow the structure below:\n",
        "\n",
        "- Convolutional Layer with 32 output channels, kernel size = 3×3, stride = 1, padding = 1\n",
        "- ReLU activation function\n",
        "- MaxPooling Layer with a kernel size of 3×3 and a stride of 2\n",
        "\n",
        "- Convolutional Layer with 64 output channels, kernel size = 3×3, stride = 1, padding = 1\n",
        "- ReLU activation function.\n",
        "- Convolutional Layer with 128 output channels, kernel size = 3×3, stride = 1, padding = 1\n",
        "\n",
        "- ReLU activation function.\n",
        "\n",
        "- MaxPooling Layer with a kernel size of 3×3 and a stride of 2.\n",
        "\n",
        "- Dropout Layer  with a dropout probability of 0.5.\n",
        "\n",
        "- Fully Connected Layer with output size of 128.\n",
        "\n",
        "- ReLU activation function\n",
        "\n",
        "- Dropout Layer  with a dropout probability of 0.5.\n",
        "\n",
        "- Fully Connected Layer with output size of 10 (for the 10 digit classes, 0–9).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c-aaOrhKvpee"
      },
      "outputs": [],
      "source": [
        "# TODO: Define the CNN architecture\n",
        "\n",
        "class SVHN_CNN(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        #TODO: Implement\n",
        "        raise error NOTImplemented\n",
        "\n",
        "    def forward(self, x):\n",
        "       #TODO: Implement\n",
        "       raise error NOTImplemented\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tjIEBDcag7F1"
      },
      "source": [
        "**Answer the following Questions:**\n",
        "\n",
        "<font color=\"red\">1. Which part(s) of your CNN most strongly influence receptive field size?</font>  \n",
        "<font color=\"red\">2. Why does receptive field matter for recognizing digits embedded in cluttered scenes?</font>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NrwTJ9YUyBk8"
      },
      "source": [
        "Now we will setup all training parameters and train the model.\n",
        "\n",
        "Your tasks in this section are to create an instance of the model and choose and explain your choice of optimizer and loss function.\n",
        "3. Fill the missing code in the training function.\n",
        "4. Train the model for ~6 epochs on the training set (in colab CPU should take ~30 mins)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HuW8_o4YyRGg"
      },
      "outputs": [],
      "source": [
        "# TODO: Implement"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AZQu8_K9yWle"
      },
      "source": [
        "Now, fill in the missing code for the training function and train the model for 10 epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wrOJzfCmzF2Q"
      },
      "outputs": [],
      "source": [
        "#TODO fill the missing code in the training function\n",
        "\n",
        "def train_model(model, train_loader, criterion, optimizer, device, num_epochs=10):\n",
        "  model.to(device)\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for images, labels in train_loader:\n",
        "      # images, labels = images.to(device), labels.to(device) #for GPU\n",
        "\n",
        "\n",
        "      running_loss += loss.item()\n",
        "      _, predicted = torch.max(outputs, 1)\n",
        "      total += labels.size(0)\n",
        "      correct += (predicted == labels).sum().item()\n",
        "\n",
        "    train_acc = 100 * correct / total\n",
        "    train_loss = running_loss / len(train_loader)\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}] \"\n",
        "          f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A-utsMxIzPdR"
      },
      "outputs": [],
      "source": [
        "#TODO: Implement Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LxQ72vaEzzm6"
      },
      "source": [
        "The following function evaluates a given model on the loaded test set. Use it to evaluate your trained model on the test set loader."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uEp6EQZ8z6yH"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, loader):\n",
        "  model.eval()\n",
        "  correct, total = 0, 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for images, labels in loader:\n",
        "      images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
        "      outputs = model(images)\n",
        "      _, predicted = outputs.max(1)\n",
        "      total += labels.size(0)\n",
        "      correct += (predicted == labels).sum().item()\n",
        "\n",
        "  accuracy = 100 * correct / total\n",
        "  return accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qmWPDpi1ifn5"
      },
      "source": [
        "**Answer the following Question:**\n",
        "\n",
        "<font color=\"red\">1. If training loss drops very quickly in the early epochs, is that always a good sign — or could it signal a potential problem?</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rd7U5r2O8DfR"
      },
      "source": [
        "### Visualizing Feature Maps\n",
        "\n",
        "To deepen our understanding of what the CNN learns, we will visualize **feature maps** (activations) produced inside the network when passing a single image forward.\n",
        "\n",
        "Feature maps show *where* the network detects edges, curves, textures, and higher-level structures.  \n",
        "\n",
        "In this section, select one test image, pass it through the CNN and finally visualize activation maps from different convolution layers.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "txcnY3Cy8asX"
      },
      "outputs": [],
      "source": [
        "# TODO: Implement"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aLDFkiDF8dI_"
      },
      "source": [
        "### Architecture Modification Experiment\n",
        "\n",
        "Modify your `SVHN_CNN` model by removing or relocating different kinds of layers.\n",
        "\n",
        "1. Propose two significant architectural changes.\n",
        "2. Implement your modified models as\n",
        "  - `SVHN_CNN_v2`\n",
        "  - `SVHN_CNN_v3`\n",
        "3. Train and evaluate both models using the same setup as the original."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ASdx_gzy8wp1"
      },
      "outputs": [],
      "source": [
        "# TODO: Implemnt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eBr2VJXLEWDo"
      },
      "source": [
        "# Question 2: The One Hundred Layers Tiramisu (45 Points)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K1_ZH_lmU2qz"
      },
      "source": [
        "In this question we explore the problem of **semantic segmentation**: assigning a class label to **every pixel** in an image.\n",
        "\n",
        "We base our work on the paper:\n",
        "\n",
        "> Jégou, S., Drozdzal, M., Vázquez, D., Romero, A., & Bengio, Y. (2017).  \n",
        "> **The One Hundred Layers Tiramisu: Fully Convolutional DenseNets for Semantic Segmentation.**  \n",
        "> *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)*.  \n",
        "> [[PDF link](https://arxiv.org/pdf/1611.09326.pdf)]\n",
        "\n",
        "\n",
        "For those interested, I highly recommend reading the paper to expand your understanding of DenseNet-based architectures and generally in deep learning literature. That said, reading it is **not required** — the tools and concepts needed for this assignment are introduced gradually throughout the steps.\n",
        "\n",
        "Our goal is to replicate the architecture of DenseNets described in the paper, aiming for comparable behaviour while using a **smaller variant** (e.g., DenseNet-67 instead of DenseNet-103) to ensure runtime feasibility on your GPUs.\n",
        "\n",
        "We work with the **CamVid** dataset, which consists of urban driving scenes captured from a moving vehicle. Each image is paired with a pixel-wise annotation map indicating semantic classes such as road, sidewalk, building, sky, tree, fence, poles, traffic signs or lights, vehicles, pedestrians, and bicyclists.\n",
        "\n",
        "Conceptually, semantic segmentation transforms an image into a **grid of classification tasks** — one small prediction problem per pixel — requiring the network to recognize objects and localize them throughout the scene.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJaES4hyE5ci"
      },
      "source": [
        "### Data Loading & Preprocessing\n",
        "\n",
        "Before building the model, we must ensure that the dataset is represented in a form a neural network can learn from.\n",
        "\n",
        "Let:\n",
        "- $X$ denote the RGB input images from CamVid.\n",
        "- $Y$ denote the corresponding color-coded annotation masks, where each pixel encodes a semantic class via an RGB value.\n",
        "\n",
        "The raw CamVid annotations contain **over 30 distinct colors**, including rare and fine-grained categories.  \n",
        "To make learning tractable and consistent with common practice, we collapse these into a compact set of **11 semantic classes**, and assign all remaining labels to a single **void class**, which is ignored during training.\n",
        "\n",
        "In this section we will:\n",
        "\n",
        "1. Define or load the RGB-to-label mapping.\n",
        "2. Convert each colored mask into a 2D array of integer class IDs.\n",
        "3. Visualize sample inputs and their mapped labels to verify correctness.\n",
        "\n",
        "With this mapping in place, segmentation becomes a **pixel-wise classification task** over the label space $\\{0, \\dots, C-1\\}$, rather than operating directly on raw RGB annotation images.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lSRggB_OKP3l"
      },
      "source": [
        "We will use the **CamVid dataset**, which contains street-scene RGB images and their corresponding pixel-wise annotations.  \n",
        "Please upload the provided `CamVid.zip` dataset to your own google drive. Then, run the following cells by mounting to your drive and unzipping the data.\n",
        "The archive will be automatically extracted into `/content/CamVid/`.\n",
        "\n",
        "> `CamVid.zip` is provided to you in `DL-HW2.zip`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7rLF7i3CLjHa",
        "outputId": "0137f2de-4f2c-45d0-c279-9ed3050bb682"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y8iX1NC0NYJS"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "zip_path = \"/content/camvid.zip\"  # wherever you uploaded it\n",
        "extract_root = \"/content/Question1\"\n",
        "\n",
        "os.makedirs(extract_root, exist_ok=True)\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as z:\n",
        "    z.extractall(extract_root)\n",
        "\n",
        "print(\"Extracted to Drive:\", extract_root)\n",
        "!ls \"/content/Question1\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vsMaEsp6LFX0"
      },
      "source": [
        "CamVid annotations are stored as **RGB color masks**, where each distinct color corresponds to a semantic category.  \n",
        "To train a segmentation model, we must convert these colors into **integer class IDs**.\n",
        "\n",
        "The mapping used here collapses ~32 original colors into **11 trainable categories** (Sky, Building, Road, etc.), with a separate **Void class** assigned label 255 and excluded from the loss.\n",
        "\n",
        "Below we define:\n",
        "- an RGB-to-label mapping,\n",
        "- a PyTorch dataset class that:\n",
        "  - reads images and masks,\n",
        "  - applies cropping and flipping,\n",
        "  - converts masks into numeric class IDs,\n",
        "  - normalizes images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1-VTNg0QnfP-"
      },
      "outputs": [],
      "source": [
        "# Data Loading & Preprocessing\n",
        "\n",
        "# Convert 32 -> 11 CamVid mapping: RGB -> label name\n",
        "RGBLabel2LabelName = {\n",
        "    (128, 128, 128): \"Sky\",\n",
        "\n",
        "    (0,   128,  64): \"Building\",\n",
        "    (128,   0,   0): \"Building\",\n",
        "    (64,  192,   0): \"Building\",\n",
        "    (64,    0,  64): \"Building\",\n",
        "    (192,   0, 128): \"Building\",\n",
        "\n",
        "    (192, 192, 128): \"Pole\",\n",
        "    (0,     0,  64): \"Pole\",\n",
        "\n",
        "    (128,  64, 128): \"Road\",\n",
        "    (128,   0, 192): \"Road\",\n",
        "    (192,   0,  64): \"Road\",\n",
        "\n",
        "    (0,     0, 192): \"Sidewalk\",\n",
        "    (64,  192, 128): \"Sidewalk\",\n",
        "    (128, 128, 192): \"Sidewalk\",\n",
        "\n",
        "    (128, 128,   0): \"Tree\",\n",
        "    (192, 192,   0): \"Tree\",\n",
        "\n",
        "    (192, 128, 128): \"SignSymbol\",\n",
        "    (128, 128,  64): \"SignSymbol\",\n",
        "    (0,    64,  64): \"SignSymbol\",\n",
        "\n",
        "    (64,   64, 128): \"Fence\",\n",
        "\n",
        "    (64,    0, 128): \"Car\",\n",
        "    (64,  128, 192): \"Car\",\n",
        "    (192, 128, 192): \"Car\",\n",
        "    (192,  64, 128): \"Car\",\n",
        "    (128,  64,  64): \"Car\",\n",
        "\n",
        "    (64,   64,   0): \"Pedestrian\",\n",
        "    (192, 128,  64): \"Pedestrian\",\n",
        "    (64,    0, 192): \"Pedestrian\",\n",
        "    (64,  128,  64): \"Pedestrian\",\n",
        "\n",
        "    (0,   128, 192): \"Bicyclist\",\n",
        "    (192,   0, 192): \"Bicyclist\",\n",
        "\n",
        "    (0,     0,   0): \"Void\"\n",
        "}\n",
        "\n",
        "# Define the 11 train classes and the void index\n",
        "TRAIN_CLASSES = [\n",
        "    \"Sky\",\n",
        "    \"Building\",\n",
        "    \"Pole\",\n",
        "    \"Road\",\n",
        "    \"Sidewalk\",\n",
        "    \"Tree\",\n",
        "    \"SignSymbol\",\n",
        "    \"Fence\",\n",
        "    \"Car\",\n",
        "    \"Pedestrian\",\n",
        "    \"Bicyclist\"\n",
        "]\n",
        "\n",
        "LABEL_NAME_TO_ID = {name: i for i, name in enumerate(TRAIN_CLASSES)}\n",
        "VOID_LABEL_NAME = \"Void\"\n",
        "VOID_INDEX = 255\n",
        "\n",
        "\n",
        "class CamVidDataset(Dataset):\n",
        "    \"\"\"\n",
        "    CamVid dataset loader that:\n",
        "    - reads RGB images from e.g. CamVid/train\n",
        "    - reads RGB masks from e.g. CamVid/train_labels\n",
        "    - uses the RGBLabel2LabelName mapping to:\n",
        "        32+ RGB colors -> 11 train classes (0..10) + Void (255)\n",
        "    - applies normalization and simple augmentations\n",
        "\n",
        "    Output:\n",
        "      image: float tensor (3, H, W), normalized (ImageNet stats)\n",
        "      mask:  long tensor (H, W) with values in {0..10, 255}\n",
        "             where 255 is the ignore_index for the loss.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 image_dir,\n",
        "                 mask_dir,\n",
        "                 crop_size=(224, 224),\n",
        "                 is_train=True):\n",
        "        self.image_dir = image_dir\n",
        "        self.mask_dir = mask_dir\n",
        "        self.is_train = is_train\n",
        "        self.crop_h, self.crop_w = crop_size\n",
        "\n",
        "        # Collect image paths\n",
        "        self.image_paths = sorted(\n",
        "            glob(os.path.join(image_dir, \"*.png\")) +\n",
        "            glob(os.path.join(image_dir, \"*.jpg\")) +\n",
        "            glob(os.path.join(image_dir, \"*.jpeg\"))\n",
        "        )\n",
        "        if len(self.image_paths) == 0:\n",
        "            raise RuntimeError(f\"No images found in {image_dir}\")\n",
        "\n",
        "        # Build corresponding mask paths (same filename, different folder)\n",
        "        self.mask_paths = []\n",
        "        for p in self.image_paths:\n",
        "            base = os.path.basename(p)\n",
        "            name, ext = os.path.splitext(base)\n",
        "\n",
        "\n",
        "            candidate = os.path.join(mask_dir, name + \"_L\" + ext)\n",
        "            if os.path.exists(candidate):\n",
        "                self.mask_paths.append(candidate)\n",
        "            else:\n",
        "                candidate2 = os.path.join(mask_dir, base)\n",
        "                if not os.path.exists(candidate2):\n",
        "                    raise FileNotFoundError(\n",
        "                        f\"Could not find mask for image {p}. \"\n",
        "                        f\"Tried: {candidate} and {candidate2}\"\n",
        "                    )\n",
        "                self.mask_paths.append(candidate2)\n",
        "\n",
        "        # Build color -> train_id mapping from RGBLabel2LabelName\n",
        "        self.num_classes = len(TRAIN_CLASSES)\n",
        "        self.train_id_to_name = TRAIN_CLASSES\n",
        "        self.void_index = VOID_INDEX\n",
        "\n",
        "        self.color_to_train_id = {}\n",
        "        for (r, g, b), label_name in RGBLabel2LabelName.items():\n",
        "            if label_name == VOID_LABEL_NAME:\n",
        "                # Void will be handled by default (everything starts as VOID_INDEX)\n",
        "                continue\n",
        "            train_id = LABEL_NAME_TO_ID[label_name]\n",
        "            self.color_to_train_id[(r, g, b)] = train_id\n",
        "\n",
        "        # Normalization\n",
        "        self.mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
        "        self.std  = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Load image & mask as numpy arrays\n",
        "        img_path = self.image_paths[idx]\n",
        "        mask_path = self.mask_paths[idx]\n",
        "\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "        mask = Image.open(mask_path).convert(\"RGB\")  # color-coded mask\n",
        "\n",
        "        img = np.array(img, dtype=np.uint8)   # (H,W,3)\n",
        "        mask = np.array(mask, dtype=np.uint8) # (H,W,3)\n",
        "\n",
        "        # random data augmentation\n",
        "        if self.is_train:\n",
        "            img, mask = self.random_crop(img, mask, self.crop_h, self.crop_w)\n",
        "            img, mask = self.random_horizontal_flip(img, mask)\n",
        "\n",
        "        # Convert color mask -> class index mask (0..10, 255)\n",
        "        class_mask = self.rgb_to_class_indices(mask)  # (H,W), int64\n",
        "\n",
        "        # Convert image to tensor and normalize\n",
        "        img = torch.from_numpy(img).float().permute(2, 0, 1) / 255.0  # (3,H,W)\n",
        "        img = (img - self.mean) / self.std\n",
        "\n",
        "        class_mask = torch.from_numpy(class_mask).long()  # (H,W)\n",
        "\n",
        "        return img, class_mask\n",
        "\n",
        "    def rgb_to_class_indices(self, mask_rgb):\n",
        "        \"\"\"\n",
        "        mask_rgb: (H,W,3) uint8\n",
        "        returns: (H,W) int64 with values in {0..num_classes-1, void_index}\n",
        "\n",
        "        Any pixel whose color is not in RGBLabel2LabelName or not in the 11\n",
        "        train classes is assigned void_index (255), same idea as CamVidGray.\n",
        "        \"\"\"\n",
        "        h, w, _ = mask_rgb.shape\n",
        "        class_mask = np.full((h, w), fill_value=self.void_index, dtype=np.int64)\n",
        "\n",
        "        # iterate over all known label colors\n",
        "        for (r, g, b), train_id in self.color_to_train_id.items():\n",
        "            matches = (\n",
        "                (mask_rgb[:, :, 0] == r) &\n",
        "                (mask_rgb[:, :, 1] == g) &\n",
        "                (mask_rgb[:, :, 2] == b)\n",
        "            )\n",
        "            class_mask[matches] = train_id\n",
        "\n",
        "        # Any remaining colors (including weird mislabels) stay as void_index\n",
        "        return class_mask\n",
        "\n",
        "    @staticmethod\n",
        "    def random_crop(img, mask, crop_h, crop_w):\n",
        "        \"\"\"Randomly crop the same region from image and mask.\"\"\"\n",
        "        H, W, _ = img.shape\n",
        "        if (H <= crop_h) or (W <= crop_w):\n",
        "            # Fallback: center crop if image is smaller than the crop\n",
        "            top = max(0, (H - crop_h) // 2)\n",
        "            left = max(0, (W - crop_w) // 2)\n",
        "        else:\n",
        "            top = np.random.randint(0, H - crop_h + 1)\n",
        "            left = np.random.randint(0, W - crop_w + 1)\n",
        "\n",
        "        img_crop = img[top:top + crop_h, left:left + crop_w, :]\n",
        "        mask_crop = mask[top:top + crop_h, left:left + crop_w, :]\n",
        "\n",
        "        return img_crop, mask_crop\n",
        "\n",
        "    @staticmethod\n",
        "    def random_horizontal_flip(img, mask, p=0.5):\n",
        "        \"\"\"Randomly flip image and mask horizontally with probability p.\"\"\"\n",
        "        if np.random.rand() < p:\n",
        "            img = np.ascontiguousarray(img[:, ::-1, :])   # flip width\n",
        "            mask = np.ascontiguousarray(mask[:, ::-1, :])\n",
        "        return img, mask"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gk2_8V3fLTCQ"
      },
      "source": [
        "We now instantiate our dataset class over the train/validation splits and wrap them with PyTorch `DataLoader`s for batching.\n",
        "\n",
        "We verify:\n",
        "- tensor shapes,\n",
        "- expected number of classes,\n",
        "- that label values fall within `{0..10, 255}`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZcTX42p210ca"
      },
      "outputs": [],
      "source": [
        "# Base directory\n",
        "base_dir = \"/content/Question1\"\n",
        "\n",
        "train_images = f\"{base_dir}/train\"\n",
        "train_masks  = f\"{base_dir}/train_labels\"\n",
        "\n",
        "val_images   = f\"{base_dir}/val\"\n",
        "val_masks    = f\"{base_dir}/val_labels\"\n",
        "\n",
        "test_images  = f\"{base_dir}/test\"\n",
        "test_masks   = f\"{base_dir}/test_labels\"\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = CamVidDataset(\n",
        "    image_dir=train_images,\n",
        "    mask_dir=train_masks,\n",
        "    crop_size=(224, 224),\n",
        "    is_train=True\n",
        ")\n",
        "\n",
        "val_dataset = CamVidDataset(\n",
        "    image_dir=val_images,\n",
        "    mask_dir=val_masks,\n",
        "    crop_size=(224, 224),\n",
        "    is_train=False\n",
        ")\n",
        "\n",
        "# Dataloaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=3, shuffle=True, num_workers=2)\n",
        "val_loader   = DataLoader(val_dataset,   batch_size=3, shuffle=False, num_workers=2)\n",
        "\n",
        "# Quick sanity check: shapes + labels\n",
        "imgs, masks = next(iter(train_loader))\n",
        "print(\"Images:\", imgs.shape)   # (B,3,H,W)\n",
        "print(\"Masks:\", masks.shape)   # (B,H,W)\n",
        "print(\"Num classes:\", train_dataset.num_classes)\n",
        "print(\"Unique labels in this batch:\", torch.unique(masks))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XSb2fh7tLgyY"
      },
      "source": [
        "To ensure that preprocessing worked as expected, we decode the class IDs back into colors and visualize:\n",
        "- the input RGB image,\n",
        "- the processed 11-class segmentation mask,\n",
        "- an overlay.\n",
        "\n",
        "This provides a quick visual confirmation that label mapping and crops are applied correctly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jXz5mNVN4CgD"
      },
      "outputs": [],
      "source": [
        "# Build a display color for each of the 11 train classes:\n",
        "# take the first RGB that maps to that label.\n",
        "TRAIN_ID_TO_COLOR = {}\n",
        "for (r, g, b), label_name in RGBLabel2LabelName.items():\n",
        "    if label_name == VOID_LABEL_NAME:\n",
        "        continue\n",
        "    train_id = LABEL_NAME_TO_ID[label_name]\n",
        "    if train_id not in TRAIN_ID_TO_COLOR:\n",
        "        TRAIN_ID_TO_COLOR[train_id] = (r, g, b)\n",
        "\n",
        "\n",
        "def decode_class_mask(class_mask, train_id_to_color, void_index=VOID_INDEX):\n",
        "    \"\"\"\n",
        "    class_mask: (H,W) int64 in {0..C-1, void_index}\n",
        "    returns: (H,W,3) uint8 color mask for visualization\n",
        "    \"\"\"\n",
        "    h, w = class_mask.shape\n",
        "    color_mask = np.zeros((h, w, 3), dtype=np.uint8)\n",
        "\n",
        "    for train_id, color in train_id_to_color.items():\n",
        "        color_mask[class_mask == train_id] = np.array(color, dtype=np.uint8)\n",
        "\n",
        "    # void pixels stay black (0,0,0);\n",
        "    return color_mask\n",
        "\n",
        "\n",
        "def visualize_camvid_sample(dataset, idx=0):\n",
        "    \"\"\"\n",
        "    Show:\n",
        "      - input image\n",
        "      - merged 11-class mask\n",
        "      - overlay + legend (class key)\n",
        "    \"\"\"\n",
        "    img, mask = dataset[idx]  # img: normalized tensor, mask: (H,W) long\n",
        "\n",
        "    # Denormalize for display\n",
        "    img_np = img.clone()\n",
        "    img_np = (img_np * dataset.std + dataset.mean).clamp(0, 1)\n",
        "    img_np = img_np.numpy().transpose(1, 2, 0)  # (H,W,3), [0,1]\n",
        "\n",
        "    mask_np = mask.numpy()\n",
        "    color_mask = decode_class_mask(mask_np, TRAIN_ID_TO_COLOR, VOID_INDEX)\n",
        "\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "    axes[0].imshow(img_np)\n",
        "    axes[0].set_title(\"Input image\")\n",
        "    axes[0].axis(\"off\")\n",
        "\n",
        "    axes[1].imshow(color_mask)\n",
        "    axes[1].set_title(\"GT mask (11 classes)\")\n",
        "    axes[1].axis(\"off\")\n",
        "\n",
        "    axes[2].imshow(img_np)\n",
        "    axes[2].imshow(color_mask, alpha=0.5)\n",
        "    axes[2].set_title(\"Overlay\")\n",
        "    axes[2].axis(\"off\")\n",
        "\n",
        "    # Legend / key\n",
        "    patches = [\n",
        "        mpatches.Patch(\n",
        "            color=np.array(TRAIN_ID_TO_COLOR[i]) / 255.0,\n",
        "            label=f\"{i}: {TRAIN_CLASSES[i]}\"\n",
        "        )\n",
        "        for i in range(len(TRAIN_CLASSES))\n",
        "    ]\n",
        "    axes[2].legend(handles=patches, bbox_to_anchor=(1.05, 1.0),\n",
        "                   loc=\"upper left\", borderaxespad=0.)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# Try a couple of samples\n",
        "visualize_camvid_sample(train_dataset, idx=0)\n",
        "visualize_camvid_sample(train_dataset, idx=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gUUUYd5xPz2Z"
      },
      "source": [
        "### Network Architecture Overview\n",
        "\n",
        "We now design the architecture of our DenseNet model — a fully convolutional encoder–decoder network tailored for semantic segmentation.\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://raw.githubusercontent.com/SimJeg/FC-DenseNet/cf2375bf9f6ed20ba029a5ee540261aad89732d5/DenseNet.jpg\" width=\"650\"/>\n",
        "</p>\n",
        "\n",
        "Conceptually, the network processes the image through a **downsampling path** (encoder), reaches a compressed representation (bottleneck), and then reconstructs a dense prediction map through an **upsampling path** (decoder). Lateral skip connections link encoder features to their corresponding decoder levels, ensuring fine spatial detail is preserved.\n",
        "\n",
        "The architecture is built from the following components:\n",
        "\n",
        "**(a) Dense Layer**\n",
        "\n",
        "**(b) Dense Block**\n",
        "\n",
        "**(c) Transition Down**\n",
        "\n",
        "**(d) Transition Up**\n",
        "\n",
        "**(e) Bottleneck Block**\n",
        "\n",
        "**(f) Final Classifier**\n",
        "\n",
        "We next implement each component in modular form and assemble them into a complete FC-DenseNet for CamVid segmentation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u8g3cAvFw1is"
      },
      "source": [
        "#### **(a) Dense Layer**\n",
        "\n",
        "The basic computational unit in DenseNet is the **dense layer**.  \n",
        "Each layer receives as input **all previous feature maps** in the block, applies:\n",
        "\n",
        "$$\n",
        "\\text{BN} \\rightarrow \\text{ReLU} \\rightarrow 3 \\times 3 \\text{Conv}\n",
        "$$\n",
        "\n",
        "and produces `k` new feature maps (the **growth rate**).  \n",
        "These outputs are concatenated with the input along the channel axis.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ZwFClFdw6LD"
      },
      "outputs": [],
      "source": [
        "class DenseLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    BN -> ReLU -> 3x3 Conv -> Dropout,\n",
        "    then concatenate input and output feature maps.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, growth_rate, drop_prob=0.2):\n",
        "        # TODO: Implement\n",
        "\n",
        "    def forward(self, x):\n",
        "        # TODO: Implement"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uB-Hv_mqw6fv"
      },
      "source": [
        "#### **(b) Dense Block**  \n",
        "\n",
        "\n",
        "A dense block stacks several dense layers sequentially.  \n",
        "Each layer receives **all feature maps from previous layers** and contributes new ones:\n",
        "\n",
        "$$\n",
        "C_{\\text{out}} = C_{\\text{in}} + L \\cdot k\n",
        "$$\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://raw.githubusercontent.com/SimJeg/FC-DenseNet/master/DenseBlock.jpg\" width=\"300\" height=\"600\"/>\n",
        "</p>\n",
        "\n",
        "This connectivity pattern promotes feature reuse, stabilizes gradients, and forms the core building unit of our network.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yre9Rh0tw6-f"
      },
      "outputs": [],
      "source": [
        "class DenseBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    A sequence of DenseLayer modules.\n",
        "    Input channels grow by `growth_rate` at each layer.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, num_layers, growth_rate, drop_prob=0.2):\n",
        "        # TODO: Implement\n",
        "\n",
        "    def forward(self, x):\n",
        "        # TODO: Implement"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mLOknqisw7Ts"
      },
      "source": [
        "#### **(c) Transition Down**\n",
        "\n",
        "\n",
        "At the end of each encoder stage, we reduce spatial resolution:\n",
        "\n",
        "$$\n",
        "\\text{BN} \\rightarrow \\text{ReLU} \\rightarrow 1\\times1\\text{ Conv} \\rightarrow \\text{Dropout} \\rightarrow \\text{MaxPool}(2)\n",
        "$$\n",
        "\n",
        "This halves width and height while keeping channels unchanged.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_yRehsC8w7l9"
      },
      "outputs": [],
      "source": [
        "class TransitionDown(nn.Module):\n",
        "    \"\"\"\n",
        "    BN + ReLU + 1x1 Conv + Dropout + MaxPool(2x2)\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, drop_prob=0.2):\n",
        "        # TODO: Implement\n",
        "\n",
        "    def forward(self, x):\n",
        "        # TODO: Implement"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bsvJZt-Cx-bB"
      },
      "source": [
        "#### **(d) Transition Up**  \n",
        "\n",
        "\n",
        "In the decoder, we restore resolution using learned upsampling via transposed convolution:\n",
        "\n",
        "$$\n",
        "\\text{ConvTranspose}(3 \\times 3, \\text{stride}=2)\n",
        "$$\n",
        "\n",
        "which doubles spatial resolution before concatenation with skip features.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-gJRuTTAx-ry"
      },
      "outputs": [],
      "source": [
        "class TransitionUp(nn.Module):\n",
        "    \"\"\"\n",
        "    Transposed convolution for upsampling by a factor of 2.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        # TODO: Implement\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.transposed_conv(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TYNps3ya2T8c"
      },
      "source": [
        "#### **(e) Bottleneck Block**\n",
        "\n",
        "\n",
        "At the deepest level, the network operates at the lowest spatial resolution but highest channel width.\n",
        "\n",
        "A final dense block processes this information before reconstruction begins in the decoder.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qbhwOdCA4rsI"
      },
      "source": [
        "### Our FC-DenseNet-lite Architecture\n",
        "\n",
        "We implement a compact FC-DenseNet architecture inspired by *The One Hundred Layers Tiramisu*.\n",
        "All convolutional **dense layers** use growth rate $k = 16$. At each dense block:\n",
        "\n",
        "- Input has $m$ feature maps.\n",
        "- The block has $n$ layers.\n",
        "- Each layer adds $k$ new feature maps.\n",
        "- The output of the block therefore has $m + n \\cdot k$ feature maps (because we concatenate all newly created features with the input).\n",
        "\n",
        "Transition Down (TD) blocks keep the number of channels $m$ but downsample in space via 2×2 max pooling.\n",
        "Transition Up (TU) blocks use a 3×3 transposed convolution with stride 2 to upsample.\n",
        "\n",
        "Our **homework architecture (\"FC-DenseNet-lite\")** is defined as follows:\n",
        "\n",
        "- Input: RGB image, $m = 3$\n",
        "- Initial 3×3 convolution: $m = 48$\n",
        "\n",
        "</br>\n",
        "\n",
        "**Downsampling path (encoder)**\n",
        "\n",
        "- Dense Block 1: 4 layers  \n",
        "  $m = 48 + 4 \\cdot 16 = 112$  \n",
        "  + Transition Down → spatial size /2, channels stay 112\n",
        "- Dense Block 2: 5 layers  \n",
        "  $m = 112 + 5 \\cdot 16 = 192$  \n",
        "  + Transition Down → channels 192\n",
        "- Dense Block 3: 7 layers  \n",
        "  $m = 192 + 7 \\cdot 16 = 304$  \n",
        "  + Transition Down → channels 304\n",
        "- Dense Block 4: 10 layers  \n",
        "  $m = 304 + 10 \\cdot 16 = 464$  \n",
        "  + Transition Down → channels 464\n",
        "\n",
        "</br>\n",
        "\n",
        "\n",
        "**Bottleneck**\n",
        "\n",
        "- Dense Block (bottleneck): 15 layers  \n",
        "  $m = 464 + 15 \\cdot 16 = 704$\n",
        "\n",
        "</br>\n",
        "\n",
        "\n",
        "**Upsampling path (decoder)**\n",
        "\n",
        "At each level, we:\n",
        "1. Apply a Transition Up (TU) to upsample the current feature maps.\n",
        "2. Concatenate with the skip connection from the corresponding encoder level.\n",
        "3. Apply a dense block at that resolution.\n",
        "\n",
        "We use symmetric numbers of layers in the decoder:\n",
        "\n",
        "- TU from bottleneck + skip from Dense Block 4 $\\to$ Dense Block (10 layers)\n",
        "- TU + skip from Dense Block 3 $\\to$ Dense Block (7 layers)\n",
        "- TU + skip from Dense Block 2 $\\to$ Dense Block (5 layers)\n",
        "- TU + skip from Dense Block 1 $\\to$ Dense Block (4 layers)\n",
        "\n",
        "\n",
        "</br>\n",
        "\n",
        "**Final classifier**\n",
        "\n",
        "- 1×1 convolution maps the decoder output to $C = 11$ class logits per pixel.\n",
        "\n",
        "> We call network `FC-DenseNet67` which is a smaller version of `FC-DenseNet103`, the architecture used in the paper."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bcvHmyXk4xr8"
      },
      "outputs": [],
      "source": [
        "class FCDenseNet(nn.Module):\n",
        "    \"\"\"\n",
        "    Fully Convolutional DenseNet for semantic segmentation.\n",
        "    Uses:\n",
        "      - initial 3x3 conv\n",
        "      - sequence of DenseBlock + TransitionDown (encoder)\n",
        "      - bottleneck DenseBlock\n",
        "      - sequence of TransitionUp + DenseBlock with skip connections (decoder)\n",
        "      - final 1x1 conv to class logits\n",
        "    \"\"\"\n",
        "    # TODO: Implement"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pVWosHBP7iqC"
      },
      "source": [
        "**Sanity Check!**\n",
        "\n",
        "Before training, we verify that the model:\n",
        "\n",
        "- accepts a tensor of shape `(B, 3, H, W)`,\n",
        "- returns logits of shape `(B, C, H, W)` matching the number of classes.\n",
        "\n",
        "This ensures that channel propagation, skip concatenation, and upsampling were implemented correctly.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mpfG0_x-40-o"
      },
      "outputs": [],
      "source": [
        "model = FCDenseNet(in_channels=3, n_classes=len(TRAIN_CLASSES), growth_rate=16)\n",
        "x = torch.randn(1, 3, 224, 224)\n",
        "y = model(x)\n",
        "print(\"Output shape:\", y.shape)  # expected: (1, 11, 224, 224)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W3IMxPNzhSV-"
      },
      "source": [
        "**Answer the following Questions:**\n",
        "\n",
        "<font color=\"red\">1. Why do segmentation networks need spatial priors that classification networks can ignore?</font>  \n",
        "\n",
        "<font color=\"red\">2. What changes architecturally when we move from \"what is in the image?\" to \"where is it?\"</font>\n",
        "\n",
        "<font color=\"red\">3. What failure mode would you expect if skip connections were removed from Tiramisu?</font>  \n",
        "\n",
        "<font color=\"red\">4. How do skip connections influence gradient flow and spatial detail recovery?</font>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQ-4u7d3-UQG"
      },
      "source": [
        "### Evaluation & Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Jx94cYx-Wet"
      },
      "source": [
        "We now train our network on CamVid and assess its performance. Before launching training, we first define **evaluation metrics** suited for semantic segmentation, followed by the standard training procedure we are used to."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2RngQ3_j_H3f"
      },
      "source": [
        "#### Evaluation Metrics\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "usvpob8z_Jyi"
      },
      "source": [
        "Semantic segmentation predictions assign a class to **every pixel**.  \n",
        "Therefore, our evaluation must measure how well the network labels individual pixels and how well it segments regions belonging to different semantic categories."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aCR4yJTu_NlZ"
      },
      "source": [
        "**1. Pixel-wise Accuracy**\n",
        "\n",
        "Pixel accuracy measures the fraction of correctly classified pixels:\n",
        "\n",
        "$$\n",
        "\\text{PixelAcc} =\n",
        "\\frac{\\sum_{(i,j)} \\mathbf{1}\\left[ \\hat{Y}_{ij} = Y_{ij} \\right]}\n",
        "     {\\sum_{(i,j)} 1},\n",
        "$$\n",
        "\n",
        "where $\\hat{Y}_{ij}$ is the predicted label and $Y_{ij}$ is the ground truth at pixel $(i,j)$.\n",
        "\n",
        "This metric is intuitive and easy to interpret, but can be misleading in imbalanced datasets:\n",
        "large regions like “road” or “sky” dominate, masking poor performance on rare classes (e.g., pedestrians or signs).\n",
        "\n",
        "</br>\n",
        "</br>\n",
        "\n",
        "**2. Intersection over Union (IoU)**\n",
        "\n",
        "IoU evaluates segmentation quality by comparing overlap between prediction and ground truth.\n",
        "\n",
        "For a given class $c$, IoU is:\n",
        "\n",
        "$$\n",
        "\\text{IoU}_c =\n",
        "\\frac{\n",
        "|\\{\\hat{Y} = c\\} \\cap \\{Y = c\\}|\n",
        "}{\n",
        "|\\{\\hat{Y} = c\\} \\cup \\{Y = c\\}|\n",
        "}.\n",
        "$$\n",
        "\n",
        "IoU penalizes:\n",
        "\n",
        "- over-segmentation (predicting class $c$ where it does not exist), and  \n",
        "- under-segmentation (missing regions belonging to class $c$).\n",
        "\n",
        "To evaluate the entire model, we compute **mean IoU (mIoU)**:\n",
        "\n",
        "$$\n",
        "\\text{mIoU} = \\frac{1}{C}\\sum_{c=1}^C \\text{IoU}_c,\n",
        "$$\n",
        "\n",
        "where $C$ is the number of semantic classes.  \n",
        "mIoU treats **all classes equally**, even rare ones, making it a standard research metric for segmentation benchmarks including CamVid."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jqAgs-FeAEMW"
      },
      "outputs": [],
      "source": [
        "def pixel_accuracy(pred, target, ignore_index=255):\n",
        "    # TODO: Implement\n",
        "\n",
        "\n",
        "def intersection_and_union(pred, target, num_classes, ignore_index=255):\n",
        "    # TODO: Implement\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1tu4R3xo-GzY"
      },
      "source": [
        "#### Training\n",
        "\n",
        "We train the network using a **pixel-wise cross-entropy loss**, treating segmentation as per-pixel classification.  \n",
        "Pixels belonging to the “void” class (255) are ignored:\n",
        "\n",
        "$$\n",
        "\\mathcal{L} = -\\frac{1}{N} \\sum_{(i,j)\\;|\\;Y_{ij}\\neq 255}\n",
        "\\log p\\left(\\, Y_{ij} \\mid X \\, \\right).\n",
        "$$\n",
        "\n",
        "Following the original FC-DenseNet paper:\n",
        "\n",
        "- We use **RMSProp** as the optimizer.\n",
        "- We include **weight decay (L2 regularization)** to encourage small parameter norms and improve stability.\n",
        "\n",
        "The RMSProp update maintains a moving average of squared gradients $v$ and performs:\n",
        "\n",
        "$$\n",
        "\\theta \\leftarrow \\theta - \\alpha \\cdot \\frac{\n",
        "\\nabla_\\theta \\mathcal{L}\n",
        "}{\n",
        "\\sqrt{v + \\epsilon}\n",
        "},\n",
        "$$\n",
        "\n",
        "which adaptively scales learning rates per parameter — particularly useful in deep architectures like DenseNets.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zoUkRwzzdnYO"
      },
      "source": [
        "**Training Procedure:**\n",
        "\n",
        "We train the network end-to-end over multiple epochs:\n",
        "\n",
        "1. Read a mini-batch of input images and ground-truth masks.\n",
        "2. Forward pass through `FCDenseNet`.\n",
        "3. Compute loss using cross-entropy (ignoring void pixels).\n",
        "4. Backpropagate gradients.\n",
        "5. Update weights using RMSProp.\n",
        "6. Accumulate accuracy and IoU statistics.\n",
        "7. Validate periodically to observe generalization.\n",
        "\n",
        "We repeat this process for 85 epochs, monitoring loss, pixel accuracy, and mIoU to evaluate convergence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "srwh_V3GAORE"
      },
      "outputs": [],
      "source": [
        "# TODO: Implement"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05DhLpPbaEfr"
      },
      "source": [
        "Beyond numerical metrics, it is important to **visually inspect** the model’s predictions. In semantic segmentation, this usually means comparing:\n",
        "\n",
        "1. the input RGB image,\n",
        "2. the **ground-truth** segmentation mask,\n",
        "3. the **predicted** segmentation mask.\n",
        "\n",
        "By looking at these side by side, you can quickly see which classes the model recognizes well, where it struggles, typical failure modes.\n",
        "\n",
        "Sample a few Images from the test set, and visualize them in comparison to the ground truth and your own prediction.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_FD6och8aj5L"
      },
      "outputs": [],
      "source": [
        "# TODO: Implement"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9o_a1EohvHF"
      },
      "source": [
        "**Answer the following Questions:**\n",
        "\n",
        "<font color=\"red\">1. Do the errors your model makes seem semantic (wrong class) or spatial (wrong localization)?</font>  \n",
        "<font color=\"red\">2. Which component of the architecture most likely causes that type of error?</font>  \n",
        "<font color=\"red\">3. If you could change one design choice to address it, what would you alter?</font>\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "36af53b6",
        "e6Ctiv2VvlD4",
        "-FDi1Yegv3iK",
        "HMLJXzhP0Ghv",
        "rd7U5r2O8DfR",
        "aLDFkiDF8dI_",
        "gUUUYd5xPz2Z"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}