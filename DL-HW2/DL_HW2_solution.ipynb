{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AvCDNp3quvSK"
      },
      "source": [
        "# Deep Learning: Assignment #2\n",
        "## Submission date: 24/12/2025, 23:59.\n",
        "### Topics:\n",
        "- Regularization\n",
        "- Batch Normalization\n",
        "- Convolutional Neural Networks\n",
        "- Semantic Segmentation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d_Es654DuyKr"
      },
      "source": [
        "**Submitted by:**\n",
        "\n",
        "- **Student 1 (Name, ID)**\n",
        "- **Student 2 (Name, ID)**  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-JwMUBs3u0Wh"
      },
      "source": [
        "**Assignment Instructions:**\n",
        "\n",
        "\u00b7 Submissions are in **pairs only**. Write both names + IDs at the top of the notebook.\n",
        "\n",
        "\u00b7 Keep your code **clean, concise, and readable**.\n",
        "\n",
        "\u00b7 You may work in your IDE, but you **must** paste the final code back into the **matching notebook cells** and run it there.  \n",
        "\n",
        "\n",
        "\u00b7 <font color='red'>Write your textual answers in red.</font>  \n",
        "(e.g., `<span style=\"color:red\">your answer here</span>`)\n",
        "\n",
        "\u00b7 All figures, printed results, and outputs should remain visible in the notebook.  \n",
        "Run **all cells** before submitting and **do not clear outputs**.\n",
        "\n",
        "\u00b7 Use relative paths \u2014 **no absolute file paths** pointing to local machines.\n",
        "\n",
        "\u00b7 **Important:** Your submission must be entirely your own.  \n",
        "Any form of plagiarism (including uncredited use of ChatGPT or AI tools) will result in **grade 0** and disciplinary action.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Suggesed uploading script\n",
        "! pip install -q kaggle\n",
        "! mkdir ~/.kaggle\n",
        "! kaggle datasets download jcoral02/camvid\n",
        "! unzip -q camvid.zip -d data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pb5tM3UBurUU"
      },
      "outputs": [],
      "source": [
        "# --- Global Setup ---\n",
        "\n",
        "# Import Libraries\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "import itertools\n",
        "import random\n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader, random_split, Dataset\n",
        "from torchvision import datasets, transforms\n",
        "import torch.optim as optim\n",
        "import torch.nn.init as init\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "from glob import glob\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "from google.colab import files\n",
        "import zipfile\n",
        "\n",
        "\n",
        "# Device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6Ctiv2VvlD4"
      },
      "source": [
        "## Question 1: Convolutional Digit Classification on SVHN (25 Points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MMzp0-8qvkIw"
      },
      "source": [
        "In this question, our goal is to implement a Convolutional Neural Network (CNN) for image classification on the The Street View House Numbers (SVHN) Dataset. The dataset consists of read-world house number images.\n",
        "\n",
        "**source:** http://ufldl.stanford.edu/housenumbers/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-FDi1Yegv3iK"
      },
      "source": [
        "### Data Loading and Preprocessing\n",
        "\n",
        "In this section we will load, explore and preprocess the dataset for training.\n",
        "\n",
        "You are given the **SVHN** (Street View House Numbers) dataset: a collection of real-world images of digits (0\u20139) captured from house numbers in Google Street View. Each image is 32\u00d732 pixels and contains three color channels (RGB). The goal is to classify each image into one of the 10 digit classes (0 \u2013 9).\n",
        "\n",
        "The dataset will be downloaded automatically to the local environment using the `torchvision.datasets.SVHN` class.\n",
        "\n",
        "For this section, implement the preprocessing procedure and explain your choice, then create the loaders for the train and test sets.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q35C2NHnzXvv"
      },
      "outputs": [],
      "source": [
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "# Load the SVHN Letters dataset\n",
        "train_dataset = datasets.SVHN(root='./data', split='train', download=True, transform=transform)\n",
        "test_dataset  = datasets.SVHN(root='./data', split='test', download=True, transform=transform)\n",
        "\n",
        "# Inspect the dataset\n",
        "print(f\"Number of training samples: {len(train_dataset)}\")\n",
        "print(f\"Number of testing samples: {len(test_dataset)}\")\n",
        "\n",
        "# Get one image and label\n",
        "image, label = train_dataset[0]\n",
        "print(f\"Shape of one image: {image.shape} (C x H x W)\")\n",
        "print(f\"Label of first image: {label}\")\n",
        "\n",
        "batch_size = 128\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NAiMm-AqzrJh"
      },
      "source": [
        "Finally, run the cell below to take a look at a few sample images to better understand the dataset we're working with."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3S5idgUvzwct"
      },
      "outputs": [],
      "source": [
        "images, labels = next(iter(train_loader))\n",
        "\n",
        "# Show first 4 images\n",
        "fig, axes = plt.subplots(1, 4, figsize=(10, 4))\n",
        "for i in range(4):\n",
        "  img = images[i].permute(1, 2, 0).numpy()\n",
        "  #img = np.clip(img * 0.229 + 0.485, 0, 1)  # unnormalize for display\n",
        "  axes[i].imshow(img)\n",
        "  axes[i].set_title(f\"Class {labels[i].item()}\")\n",
        "  axes[i].axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NgS5vYD4gxHc"
      },
      "source": [
        "**Answer the following Questions:**\n",
        "\n",
        "<font color=\"red\">1. How is SVHN fundamentally harder than MNIST?</font>  \n",
        "<span style=\"color:red\">SVHN is fundamentally harder than MNIST because it consists of real-world images from Google Street View, meaning the digits are embedded in natural scenes with cluttered backgrounds, variable lighting conditions, and different fonts/styles. Unlike MNIST, which has centered, clean, handwritten digits on a black background, SVHN images are RGB (3 color channels) and often contain parts of neighboring digits.</span>\n",
        "\n",
        "<font color=\"red\">2. Which preprocessing or architectural choices become necessary because of this difference?</font>\n",
        "<span style=\"color:red\">Because of the color complexity and background clutter, we need deeper architectures with convolutional layers to extract hierarchical features (edges, textures, shapes) rather than simple fully connected networks. We also need to process 3 input channels (RGB) instead of 1. Preprocessing steps like Normalization (mean subtraction and scaling) are crucial to help the model converge faster given the varying pixel intensity distributions in real-world images.</span>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HMLJXzhP0Ghv"
      },
      "source": [
        "### CNN Architecture Design\n",
        "\n",
        "We will design a convolutional neural network (inspired by AlexNet) for digit classification.\n",
        "The network consists of **three convolutional feature-extraction stages**, followed by a **two-layer fully connected classifier**.\n",
        "\n",
        "Your architecture must follow the structure below:\n",
        "\n",
        "- Convolutional Layer with 32 output channels, kernel size = 3\u00d73, stride = 1, padding = 1\n",
        "- ReLU activation function\n",
        "- MaxPooling Layer with a kernel size of 3\u00d73 and a stride of 2\n",
        "\n",
        "- Convolutional Layer with 64 output channels, kernel size = 3\u00d73, stride = 1, padding = 1\n",
        "- ReLU activation function.\n",
        "- Convolutional Layer with 128 output channels, kernel size = 3\u00d73, stride = 1, padding = 1\n",
        "\n",
        "- ReLU activation function.\n",
        "\n",
        "- MaxPooling Layer with a kernel size of 3\u00d73 and a stride of 2.\n",
        "\n",
        "- Dropout Layer  with a dropout probability of 0.5.\n",
        "\n",
        "- Fully Connected Layer with output size of 128.\n",
        "\n",
        "- ReLU activation function\n",
        "\n",
        "- Dropout Layer  with a dropout probability of 0.5.\n",
        "\n",
        "- Fully Connected Layer with output size of 10 (for the 10 digit classes, 0\u20139).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c-aaOrhKvpee"
      },
      "outputs": [],
      "source": [
        "\n",
        "class SVHN_CNN(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(SVHN_CNN, self).__init__()\n",
        "        \n",
        "        # Block 1: Conv -> ReLU -> MaxPool\n",
        "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1)\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
        "        \n",
        "        # Block 2: Conv -> ReLU\n",
        "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
        "        \n",
        "        # Block 3: Conv -> ReLU -> MaxPool\n",
        "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
        "        self.pool3 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
        "        \n",
        "        # Dropout\n",
        "        self.dropout1 = nn.Dropout(p=0.5)\n",
        "        \n",
        "        # FC layers\n",
        "        # Calculation for input size:\n",
        "        # Input: 32x32\n",
        "        # After pool1 (k=3, s=2): floor((32-3)/2 + 1) = 15 -> 15x15\n",
        "        # After pool3 (k=3, s=2): floor((15-3)/2 + 1) = 7 -> 7x7\n",
        "        self.flatten_dim = 128 * 7 * 7\n",
        "        \n",
        "        self.fc1 = nn.Linear(self.flatten_dim, 128)\n",
        "        self.dropout2 = nn.Dropout(p=0.5)\n",
        "        self.fc2 = nn.Linear(128, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Block 1\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = self.pool1(x)\n",
        "        \n",
        "        # Block 2\n",
        "        x = F.relu(self.conv2(x))\n",
        "        \n",
        "        # Block 3\n",
        "        x = F.relu(self.conv3(x))\n",
        "        x = self.pool3(x)\n",
        "        \n",
        "        # Flatten\n",
        "        x = x.view(x.size(0), -1)\n",
        "        \n",
        "        # FC layers\n",
        "        x = self.dropout1(x)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout2(x)\n",
        "        x = self.fc2(x)\n",
        "        \n",
        "        return x\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tjIEBDcag7F1"
      },
      "source": [
        "**Answer the following Questions:**\n",
        "\n",
        "<font color=\"red\">1. Which part(s) of your CNN most strongly influence receptive field size?</font>  \n",
        "<span style=\"color:red\">The convolutional kernel sizes, strides, and especially the pooling layers (which increase the stride effectively) most strongly influence the receptive field size. Stacking multiple convolutional layers also increases the receptive field linearly, but pooling/strided convolutions increase it multiplicatively (relative to the input).</span>\n",
        "\n",
        "<font color=\"red\">2. Why does receptive field matter for recognizing digits embedded in cluttered scenes?</font>\n",
        "<span style=\"color:red\">A larger receptive field allows the neuron to \"see\" more of the input image at once. This is critical for context. In cluttered scenes, the network needs to distinguish the digit from the background noise. If the receptive field is too small, the network might only see a curve or a line and confuse it with background texture. A sufficiently large receptive field ensures the network captures the entire digit and its immediate context to make a correct classification.</span>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NrwTJ9YUyBk8"
      },
      "source": [
        "Now we will setup all training parameters and train the model.\n",
        "\n",
        "Your tasks in this section are to create an instance of the model and choose and explain your choice of optimizer and loss function.\n",
        "3. Fill the missing code in the training function.\n",
        "4. Train the model for ~6 epochs on the training set (in colab CPU should take ~30 mins)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HuW8_o4YyRGg"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Create model instance\n",
        "model = SVHN_CNN(num_classes=10)\n",
        "\n",
        "# Loss function: CrossEntropyLoss is standard for multi-class classification\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Optimizer: Adam is a good default choice for faster convergence compared to SGD\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "print(model)\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AZQu8_K9yWle"
      },
      "source": [
        "Now, fill in the missing code for the training function and train the model for 10 epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wrOJzfCmzF2Q"
      },
      "outputs": [],
      "source": [
        "\n",
        "def train_model(model, train_loader, criterion, optimizer, device, num_epochs=10):\n",
        "  model.to(device)\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    # Progress bar for better visualization\n",
        "    loop = tqdm(train_loader, leave=True)\n",
        "    loop.set_description(f\"Epoch [{epoch+1}/{num_epochs}]\")\n",
        "\n",
        "    for images, labels in loop:\n",
        "      images, labels = images.to(device), labels.to(device) \n",
        "\n",
        "      # Zero gradients\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      # Forward pass\n",
        "      outputs = model(images)\n",
        "      loss = criterion(outputs, labels)\n",
        "\n",
        "      # Backward pass and optimize\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      running_loss += loss.item()\n",
        "      _, predicted = torch.max(outputs, 1)\n",
        "      total += labels.size(0)\n",
        "      correct += (predicted == labels).sum().item()\n",
        "      \n",
        "      # Update progress bar\n",
        "      loop.set_postfix(loss=loss.item())\n",
        "\n",
        "    train_acc = 100 * correct / total\n",
        "    train_loss = running_loss / len(train_loader)\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}] \"\n",
        "          f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A-utsMxIzPdR"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Train for 10 epochs\n",
        "train_model(model, train_loader, criterion, optimizer, device, num_epochs=10)\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LxQ72vaEzzm6"
      },
      "source": [
        "The following function evaluates a given model on the loaded test set. Use it to evaluate your trained model on the test set loader."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uEp6EQZ8z6yH"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, loader):\n",
        "  model.eval()\n",
        "  correct, total = 0, 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for images, labels in loader:\n",
        "      images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
        "      outputs = model(images)\n",
        "      _, predicted = outputs.max(1)\n",
        "      total += labels.size(0)\n",
        "      correct += (predicted == labels).sum().item()\n",
        "\n",
        "  accuracy = 100 * correct / total\n",
        "  return accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qmWPDpi1ifn5"
      },
      "source": [
        "**Answer the following Question:**\n",
        "\n",
        "<font color=\"red\">1. If training loss drops very quickly in the early epochs, is that always a good sign \u2014 or could it signal a potential problem?</font>\n",
        "<span style=\"color:red\">It is generally a good sign that learning is happening, but it's not *always* good. If it drops too precipitously and then plateaus immediately, it might indicate that the learning rate is too high (potentially causing instability later) or that the model is overfitting to the easy examples very quickly. If the validation loss doesn't follow the training loss (i.e., val loss stays high or goes up), then a sharp drop in training loss signals overfitting. However, in the early stages of training a CNN on a dataset like SVHN, a quick initial drop is expected as the model learns basic features.</span>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rd7U5r2O8DfR"
      },
      "source": [
        "### Visualizing Feature Maps\n",
        "\n",
        "To deepen our understanding of what the CNN learns, we will visualize **feature maps** (activations) produced inside the network when passing a single image forward.\n",
        "\n",
        "Feature maps show *where* the network detects edges, curves, textures, and higher-level structures.  \n",
        "\n",
        "In this section, select one test image, pass it through the CNN and finally visualize activation maps from different convolution layers.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "txcnY3Cy8asX"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Get a single image\n",
        "model.eval()\n",
        "image, label = test_dataset[0]\n",
        "input_tensor = image.unsqueeze(0).to(device)\n",
        "\n",
        "# Function to get activations\n",
        "activations = {}\n",
        "def get_activation(name):\n",
        "    def hook(model, input, output):\n",
        "        activations[name] = output.detach()\n",
        "    return hook\n",
        "\n",
        "# Register hooks\n",
        "model.conv1.register_forward_hook(get_activation('conv1'))\n",
        "model.conv2.register_forward_hook(get_activation('conv2'))\n",
        "model.conv3.register_forward_hook(get_activation('conv3'))\n",
        "\n",
        "# Forward pass\n",
        "output = model(input_tensor)\n",
        "\n",
        "# Visualize\n",
        "def plot_feature_maps(layer_name, num_maps=8):\n",
        "    act = activations[layer_name].cpu().squeeze()\n",
        "    fig, axes = plt.subplots(1, num_maps, figsize=(15, 3))\n",
        "    for i in range(num_maps):\n",
        "        if i < act.size(0):\n",
        "            axes[i].imshow(act[i], cmap='viridis')\n",
        "            axes[i].axis('off')\n",
        "    plt.suptitle(f\"Feature Maps from {layer_name}\")\n",
        "    plt.show()\n",
        "\n",
        "# Show input image\n",
        "plt.imshow(image.permute(1, 2, 0) * 0.5 + 0.5) # unnormalize\n",
        "plt.title(f\"Input Image (Class {label})\")\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "\n",
        "plot_feature_maps('conv1')\n",
        "plot_feature_maps('conv2')\n",
        "plot_feature_maps('conv3')\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aLDFkiDF8dI_"
      },
      "source": [
        "### Architecture Modification Experiment\n",
        "\n",
        "Modify your `SVHN_CNN` model by removing or relocating different kinds of layers.\n",
        "\n",
        "1. Propose two significant architectural changes.\n",
        "2. Implement your modified models as\n",
        "  - `SVHN_CNN_v2`\n",
        "  - `SVHN_CNN_v3`\n",
        "3. Train and evaluate both models using the same setup as the original."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ASdx_gzy8wp1"
      },
      "outputs": [],
      "source": [
        "\n",
        "# --- Model V2: Remove Dropout ---\n",
        "class SVHN_CNN_v2(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(SVHN_CNN_v2, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)\n",
        "        self.pool1 = nn.MaxPool2d(3, 2)\n",
        "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
        "        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)\n",
        "        self.pool3 = nn.MaxPool2d(3, 2)\n",
        "        \n",
        "        self.flatten_dim = 128 * 7 * 7\n",
        "        self.fc1 = nn.Linear(self.flatten_dim, 128)\n",
        "        self.fc2 = nn.Linear(128, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool1(F.relu(self.conv1(x)))\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = self.pool3(F.relu(self.conv3(x)))\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x) # No dropout\n",
        "        return x\n",
        "\n",
        "# --- Model V3: Add Batch Normalization ---\n",
        "class SVHN_CNN_v3(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(SVHN_CNN_v3, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "        self.pool1 = nn.MaxPool2d(3, 2)\n",
        "        \n",
        "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "        \n",
        "        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)\n",
        "        self.bn3 = nn.BatchNorm2d(128)\n",
        "        self.pool3 = nn.MaxPool2d(3, 2)\n",
        "        \n",
        "        self.flatten_dim = 128 * 7 * 7\n",
        "        self.dropout1 = nn.Dropout(0.5)\n",
        "        self.fc1 = nn.Linear(self.flatten_dim, 128)\n",
        "        self.dropout2 = nn.Dropout(0.5)\n",
        "        self.fc2 = nn.Linear(128, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool1(F.relu(self.bn1(self.conv1(x))))\n",
        "        x = F.relu(self.bn2(self.conv2(x)))\n",
        "        x = self.pool3(F.relu(self.bn3(self.conv3(x))))\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.dropout1(x)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout2(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "print(\"Training SVHN_CNN_v2 (No Dropout)...\")\n",
        "model_v2 = SVHN_CNN_v2(num_classes=10)\n",
        "optimizer_v2 = optim.Adam(model_v2.parameters(), lr=0.001)\n",
        "train_model(model_v2, train_loader, criterion, optimizer_v2, device, num_epochs=6)\n",
        "print(f\"Test Accuracy v2: {evaluate_model(model_v2, test_loader):.2f}%\")\n",
        "\n",
        "print(\"\\nTraining SVHN_CNN_v3 (With BatchNorm)...\")\n",
        "model_v3 = SVHN_CNN_v3(num_classes=10)\n",
        "optimizer_v3 = optim.Adam(model_v3.parameters(), lr=0.001)\n",
        "train_model(model_v3, train_loader, criterion, optimizer_v3, device, num_epochs=6)\n",
        "print(f\"Test Accuracy v3: {evaluate_model(model_v3, test_loader):.2f}%\")\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eBr2VJXLEWDo"
      },
      "source": [
        "## Question 2: The One Hundred Layers Tiramisu (45 Points)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K1_ZH_lmU2qz"
      },
      "source": [
        "In this question we explore the problem of **semantic segmentation**: assigning a class label to **every pixel** in an image.\n",
        "\n",
        "We base our work on the paper:\n",
        "\n",
        "> J\u00e9gou, S., Drozdzal, M., V\u00e1zquez, D., Romero, A., & Bengio, Y. (2017).  \n",
        "> **The One Hundred Layers Tiramisu: Fully Convolutional DenseNets for Semantic Segmentation.**  \n",
        "> *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)*.  \n",
        "> [[PDF link](https://arxiv.org/pdf/1611.09326.pdf)]\n",
        "\n",
        "\n",
        "For those interested, I highly recommend reading the paper to expand your understanding of DenseNet-based architectures and generally in deep learning literature. That said, reading it is **not required** \u2014 the tools and concepts needed for this assignment are introduced gradually throughout the steps.\n",
        "\n",
        "Our goal is to replicate the architecture of DenseNets described in the paper, aiming for comparable behaviour while using a **smaller variant** (e.g., DenseNet-67 instead of DenseNet-103) to ensure runtime feasibility on your GPUs.\n",
        "\n",
        "We work with the **CamVid** dataset, which consists of urban driving scenes captured from a moving vehicle. Each image is paired with a pixel-wise annotation map indicating semantic classes such as road, sidewalk, building, sky, tree, fence, poles, traffic signs or lights, vehicles, pedestrians, and bicyclists.\n",
        "\n",
        "Conceptually, semantic segmentation transforms an image into a **grid of classification tasks** \u2014 one small prediction problem per pixel \u2014 requiring the network to recognize objects and localize them throughout the scene.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJaES4hyE5ci"
      },
      "source": [
        "### Data Loading & Preprocessing\n",
        "\n",
        "Before building the model, we must ensure that the dataset is represented in a form a neural network can learn from.\n",
        "\n",
        "Let:\n",
        "- $X$ denote the RGB input images from CamVid.\n",
        "- $Y$ denote the corresponding color-coded annotation masks, where each pixel encodes a semantic class via an RGB value.\n",
        "\n",
        "The raw CamVid annotations contain **over 30 distinct colors**, including rare and fine-grained categories.  \n",
        "To make learning tractable and consistent with common practice, we collapse these into a compact set of **11 semantic classes**, and assign all remaining labels to a single **void class**, which is ignored during training.\n",
        "\n",
        "In this section we will:\n",
        "\n",
        "1. Define or load the RGB-to-label mapping.\n",
        "2. Convert each colored mask into a 2D array of integer class IDs.\n",
        "3. Visualize sample inputs and their mapped labels to verify correctness.\n",
        "\n",
        "With this mapping in place, segmentation becomes a **pixel-wise classification task** over the label space $\\{0, \\dots, C-1\\}$, rather than operating directly on raw RGB annotation images.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lSRggB_OKP3l"
      },
      "source": [
        "We will use the **CamVid dataset**, which contains street-scene RGB images and their corresponding pixel-wise annotations.  \n",
        "Please upload the provided `CamVid.zip` dataset to your own google drive. Then, run the following cells by mounting to your drive and unzipping the data.\n",
        "The archive will be automatically extracted into `/content/CamVid/`.\n",
        "\n",
        "> `CamVid.zip` is provided to you in `DL-HW2.zip`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7rLF7i3CLjHa",
        "outputId": "0137f2de-4f2c-45d0-c279-9ed3050bb682"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y8iX1NC0NYJS"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "zip_path = \"/content/drive/MyDrive/CamVid.zip\"  # wherever you uploaded it\n",
        "extract_root = \"/content/drive/MyDrive/CamVid\"\n",
        "\n",
        "os.makedirs(extract_root, exist_ok=True)\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as z:\n",
        "    z.extractall(extract_root)\n",
        "\n",
        "print(\"Extracted to Drive:\", extract_root)\n",
        "!ls \"/content/drive/MyDrive/CamVid\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vsMaEsp6LFX0"
      },
      "source": [
        "CamVid annotations are stored as **RGB color masks**, where each distinct color corresponds to a semantic category.  \n",
        "To train a segmentation model, we must convert these colors into **integer class IDs**.\n",
        "\n",
        "The mapping used here collapses ~32 original colors into **11 trainable categories** (Sky, Building, Road, etc.), with a separate **Void class** assigned label 255 and excluded from the loss.\n",
        "\n",
        "Below we define:\n",
        "- an RGB-to-label mapping,\n",
        "- a PyTorch dataset class that:\n",
        "  - reads images and masks,\n",
        "  - applies cropping and flipping,\n",
        "  - converts masks into numeric class IDs,\n",
        "  - normalizes images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1-VTNg0QnfP-"
      },
      "outputs": [],
      "source": [
        "# Data Loading & Preprocessing\n",
        "\n",
        "# Convert 32 -> 11 CamVid mapping: RGB -> label name\n",
        "RGBLabel2LabelName = {\n",
        "    (128, 128, 128): \"Sky\",\n",
        "\n",
        "    (0,   128,  64): \"Building\",\n",
        "    (128,   0,   0): \"Building\",\n",
        "    (64,  192,   0): \"Building\",\n",
        "    (64,    0,  64): \"Building\",\n",
        "    (192,   0, 128): \"Building\",\n",
        "\n",
        "    (192, 192, 128): \"Pole\",\n",
        "    (0,     0,  64): \"Pole\",\n",
        "\n",
        "    (128,  64, 128): \"Road\",\n",
        "    (128,   0, 192): \"Road\",\n",
        "    (192,   0,  64): \"Road\",\n",
        "\n",
        "    (0,     0, 192): \"Sidewalk\",\n",
        "    (64,  192, 128): \"Sidewalk\",\n",
        "    (128, 128, 192): \"Sidewalk\",\n",
        "\n",
        "    (128, 128,   0): \"Tree\",\n",
        "    (192, 192,   0): \"Tree\",\n",
        "\n",
        "    (192, 128, 128): \"SignSymbol\",\n",
        "    (128, 128,  64): \"SignSymbol\",\n",
        "    (0,    64,  64): \"SignSymbol\",\n",
        "\n",
        "    (64,   64, 128): \"Fence\",\n",
        "\n",
        "    (64,    0, 128): \"Car\",\n",
        "    (64,  128, 192): \"Car\",\n",
        "    (192, 128, 192): \"Car\",\n",
        "    (192,  64, 128): \"Car\",\n",
        "    (128,  64,  64): \"Car\",\n",
        "\n",
        "    (64,   64,   0): \"Pedestrian\",\n",
        "    (192, 128,  64): \"Pedestrian\",\n",
        "    (64,    0, 192): \"Pedestrian\",\n",
        "    (64,  128,  64): \"Pedestrian\",\n",
        "\n",
        "    (0,   128, 192): \"Bicyclist\",\n",
        "    (192,   0, 192): \"Bicyclist\",\n",
        "\n",
        "    (0,     0,   0): \"Void\"\n",
        "}\n",
        "\n",
        "# Define the 11 train classes and the void index\n",
        "TRAIN_CLASSES = [\n",
        "    \"Sky\",\n",
        "    \"Building\",\n",
        "    \"Pole\",\n",
        "    \"Road\",\n",
        "    \"Sidewalk\",\n",
        "    \"Tree\",\n",
        "    \"SignSymbol\",\n",
        "    \"Fence\",\n",
        "    \"Car\",\n",
        "    \"Pedestrian\",\n",
        "    \"Bicyclist\"\n",
        "]\n",
        "\n",
        "LABEL_NAME_TO_ID = {name: i for i, name in enumerate(TRAIN_CLASSES)}\n",
        "VOID_LABEL_NAME = \"Void\"\n",
        "VOID_INDEX = 255\n",
        "\n",
        "\n",
        "class CamVidDataset(Dataset):\n",
        "    \"\"\"\n",
        "    CamVid dataset loader that:\n",
        "    - reads RGB images from e.g. CamVid/train\n",
        "    - reads RGB masks from e.g. CamVid/train_labels\n",
        "    - uses the RGBLabel2LabelName mapping to:\n",
        "        32+ RGB colors -> 11 train classes (0..10) + Void (255)\n",
        "    - applies normalization and simple augmentations\n",
        "\n",
        "    Output:\n",
        "      image: float tensor (3, H, W), normalized (ImageNet stats)\n",
        "      mask:  long tensor (H, W) with values in {0..10, 255}\n",
        "             where 255 is the ignore_index for the loss.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 image_dir,\n",
        "                 mask_dir,\n",
        "                 crop_size=(224, 224),\n",
        "                 is_train=True):\n",
        "        self.image_dir = image_dir\n",
        "        self.mask_dir = mask_dir\n",
        "        self.is_train = is_train\n",
        "        self.crop_h, self.crop_w = crop_size\n",
        "\n",
        "        # Collect image paths\n",
        "        self.image_paths = sorted(\n",
        "            glob(os.path.join(image_dir, \"*.png\")) +\n",
        "            glob(os.path.join(image_dir, \"*.jpg\")) +\n",
        "            glob(os.path.join(image_dir, \"*.jpeg\"))\n",
        "        )\n",
        "        if len(self.image_paths) == 0:\n",
        "            raise RuntimeError(f\"No images found in {image_dir}\")\n",
        "\n",
        "        # Build corresponding mask paths (same filename, different folder)\n",
        "        self.mask_paths = []\n",
        "        for p in self.image_paths:\n",
        "            base = os.path.basename(p)\n",
        "            name, ext = os.path.splitext(base)\n",
        "\n",
        "\n",
        "            candidate = os.path.join(mask_dir, name + \"_L\" + ext)\n",
        "            if os.path.exists(candidate):\n",
        "                self.mask_paths.append(candidate)\n",
        "            else:\n",
        "                candidate2 = os.path.join(mask_dir, base)\n",
        "                if not os.path.exists(candidate2):\n",
        "                    raise FileNotFoundError(\n",
        "                        f\"Could not find mask for image {p}. \"\n",
        "                        f\"Tried: {candidate} and {candidate2}\"\n",
        "                    )\n",
        "                self.mask_paths.append(candidate2)\n",
        "\n",
        "        # Build color -> train_id mapping from RGBLabel2LabelName\n",
        "        self.num_classes = len(TRAIN_CLASSES)\n",
        "        self.train_id_to_name = TRAIN_CLASSES\n",
        "        self.void_index = VOID_INDEX\n",
        "\n",
        "        self.color_to_train_id = {}\n",
        "        for (r, g, b), label_name in RGBLabel2LabelName.items():\n",
        "            if label_name == VOID_LABEL_NAME:\n",
        "                # Void will be handled by default (everything starts as VOID_INDEX)\n",
        "                continue\n",
        "            train_id = LABEL_NAME_TO_ID[label_name]\n",
        "            self.color_to_train_id[(r, g, b)] = train_id\n",
        "\n",
        "        # Normalization\n",
        "        self.mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
        "        self.std  = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Load image & mask as numpy arrays\n",
        "        img_path = self.image_paths[idx]\n",
        "        mask_path = self.mask_paths[idx]\n",
        "\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "        mask = Image.open(mask_path).convert(\"RGB\")  # color-coded mask\n",
        "\n",
        "        img = np.array(img, dtype=np.uint8)   # (H,W,3)\n",
        "        mask = np.array(mask, dtype=np.uint8) # (H,W,3)\n",
        "\n",
        "        # random data augmentation\n",
        "        if self.is_train:\n",
        "            img, mask = self.random_crop(img, mask, self.crop_h, self.crop_w)\n",
        "            img, mask = self.random_horizontal_flip(img, mask)\n",
        "\n",
        "        # Convert color mask -> class index mask (0..10, 255)\n",
        "        class_mask = self.rgb_to_class_indices(mask)  # (H,W), int64\n",
        "\n",
        "        # Convert image to tensor and normalize\n",
        "        img = torch.from_numpy(img).float().permute(2, 0, 1) / 255.0  # (3,H,W)\n",
        "        img = (img - self.mean) / self.std\n",
        "\n",
        "        class_mask = torch.from_numpy(class_mask).long()  # (H,W)\n",
        "\n",
        "        return img, class_mask\n",
        "\n",
        "    def rgb_to_class_indices(self, mask_rgb):\n",
        "        \"\"\"\n",
        "        mask_rgb: (H,W,3) uint8\n",
        "        returns: (H,W) int64 with values in {0..num_classes-1, void_index}\n",
        "\n",
        "        Any pixel whose color is not in RGBLabel2LabelName or not in the 11\n",
        "        train classes is assigned void_index (255), same idea as CamVidGray.\n",
        "        \"\"\"\n",
        "        h, w, _ = mask_rgb.shape\n",
        "        class_mask = np.full((h, w), fill_value=self.void_index, dtype=np.int64)\n",
        "\n",
        "        # iterate over all known label colors\n",
        "        for (r, g, b), train_id in self.color_to_train_id.items():\n",
        "            matches = (\n",
        "                (mask_rgb[:, :, 0] == r) &\n",
        "                (mask_rgb[:, :, 1] == g) &\n",
        "                (mask_rgb[:, :, 2] == b)\n",
        "            )\n",
        "            class_mask[matches] = train_id\n",
        "\n",
        "        # Any remaining colors (including weird mislabels) stay as void_index\n",
        "        return class_mask\n",
        "\n",
        "    @staticmethod\n",
        "    def random_crop(img, mask, crop_h, crop_w):\n",
        "        \"\"\"Randomly crop the same region from image and mask.\"\"\"\n",
        "        H, W, _ = img.shape\n",
        "        if (H <= crop_h) or (W <= crop_w):\n",
        "            # Fallback: center crop if image is smaller than the crop\n",
        "            top = max(0, (H - crop_h) // 2)\n",
        "            left = max(0, (W - crop_w) // 2)\n",
        "        else:\n",
        "            top = np.random.randint(0, H - crop_h + 1)\n",
        "            left = np.random.randint(0, W - crop_w + 1)\n",
        "\n",
        "        img_crop = img[top:top + crop_h, left:left + crop_w, :]\n",
        "        mask_crop = mask[top:top + crop_h, left:left + crop_w, :]\n",
        "\n",
        "        return img_crop, mask_crop\n",
        "\n",
        "    @staticmethod\n",
        "    def random_horizontal_flip(img, mask, p=0.5):\n",
        "        \"\"\"Randomly flip image and mask horizontally with probability p.\"\"\"\n",
        "        if np.random.rand() < p:\n",
        "            img = np.ascontiguousarray(img[:, ::-1, :])   # flip width\n",
        "            mask = np.ascontiguousarray(mask[:, ::-1, :])\n",
        "        return img, mask"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gk2_8V3fLTCQ"
      },
      "source": [
        "We now instantiate our dataset class over the train/validation splits and wrap them with PyTorch `DataLoader`s for batching.\n",
        "\n",
        "We verify:\n",
        "- tensor shapes,\n",
        "- expected number of classes,\n",
        "- that label values fall within `{0..10, 255}`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZcTX42p210ca"
      },
      "outputs": [],
      "source": [
        "# Base directory\n",
        "base_dir = \"/content/drive/MyDrive/CamVid\"\n",
        "\n",
        "train_images = f\"{base_dir}/train\"\n",
        "train_masks  = f\"{base_dir}/train_labels\"\n",
        "\n",
        "val_images   = f\"{base_dir}/val\"\n",
        "val_masks    = f\"{base_dir}/val_labels\"\n",
        "\n",
        "test_images  = f\"{base_dir}/test\"\n",
        "test_masks   = f\"{base_dir}/test_labels\"\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = CamVidDataset(\n",
        "    image_dir=train_images,\n",
        "    mask_dir=train_masks,\n",
        "    crop_size=(224, 224),\n",
        "    is_train=True\n",
        ")\n",
        "\n",
        "val_dataset = CamVidDataset(\n",
        "    image_dir=val_images,\n",
        "    mask_dir=val_masks,\n",
        "    crop_size=(224, 224),\n",
        "    is_train=False\n",
        ")\n",
        "\n",
        "# Dataloaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=3, shuffle=True, num_workers=2)\n",
        "val_loader   = DataLoader(val_dataset,   batch_size=3, shuffle=False, num_workers=2)\n",
        "\n",
        "# Quick sanity check: shapes + labels\n",
        "imgs, masks = next(iter(train_loader))\n",
        "print(\"Images:\", imgs.shape)   # (B,3,H,W)\n",
        "print(\"Masks:\", masks.shape)   # (B,H,W)\n",
        "print(\"Num classes:\", train_dataset.num_classes)\n",
        "print(\"Unique labels in this batch:\", torch.unique(masks))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XSb2fh7tLgyY"
      },
      "source": [
        "To ensure that preprocessing worked as expected, we decode the class IDs back into colors and visualize:\n",
        "- the input RGB image,\n",
        "- the processed 11-class segmentation mask,\n",
        "- an overlay.\n",
        "\n",
        "This provides a quick visual confirmation that label mapping and crops are applied correctly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jXz5mNVN4CgD"
      },
      "outputs": [],
      "source": [
        "# Build a display color for each of the 11 train classes:\n",
        "# take the first RGB that maps to that label.\n",
        "TRAIN_ID_TO_COLOR = {}\n",
        "for (r, g, b), label_name in RGBLabel2LabelName.items():\n",
        "    if label_name == VOID_LABEL_NAME:\n",
        "        continue\n",
        "    train_id = LABEL_NAME_TO_ID[label_name]\n",
        "    if train_id not in TRAIN_ID_TO_COLOR:\n",
        "        TRAIN_ID_TO_COLOR[train_id] = (r, g, b)\n",
        "\n",
        "\n",
        "def decode_class_mask(class_mask, train_id_to_color, void_index=VOID_INDEX):\n",
        "    \"\"\"\n",
        "    class_mask: (H,W) int64 in {0..C-1, void_index}\n",
        "    returns: (H,W,3) uint8 color mask for visualization\n",
        "    \"\"\"\n",
        "    h, w = class_mask.shape\n",
        "    color_mask = np.zeros((h, w, 3), dtype=np.uint8)\n",
        "\n",
        "    for train_id, color in train_id_to_color.items():\n",
        "        color_mask[class_mask == train_id] = np.array(color, dtype=np.uint8)\n",
        "\n",
        "    # void pixels stay black (0,0,0);\n",
        "    return color_mask\n",
        "\n",
        "\n",
        "def visualize_camvid_sample(dataset, idx=0):\n",
        "    \"\"\"\n",
        "    Show:\n",
        "      - input image\n",
        "      - merged 11-class mask\n",
        "      - overlay + legend (class key)\n",
        "    \"\"\"\n",
        "    img, mask = dataset[idx]  # img: normalized tensor, mask: (H,W) long\n",
        "\n",
        "    # Denormalize for display\n",
        "    img_np = img.clone()\n",
        "    img_np = (img_np * dataset.std + dataset.mean).clamp(0, 1)\n",
        "    img_np = img_np.numpy().transpose(1, 2, 0)  # (H,W,3), [0,1]\n",
        "\n",
        "    mask_np = mask.numpy()\n",
        "    color_mask = decode_class_mask(mask_np, TRAIN_ID_TO_COLOR, VOID_INDEX)\n",
        "\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "    axes[0].imshow(img_np)\n",
        "    axes[0].set_title(\"Input image\")\n",
        "    axes[0].axis(\"off\")\n",
        "\n",
        "    axes[1].imshow(color_mask)\n",
        "    axes[1].set_title(\"GT mask (11 classes)\")\n",
        "    axes[1].axis(\"off\")\n",
        "\n",
        "    axes[2].imshow(img_np)\n",
        "    axes[2].imshow(color_mask, alpha=0.5)\n",
        "    axes[2].set_title(\"Overlay\")\n",
        "    axes[2].axis(\"off\")\n",
        "\n",
        "    # Legend / key\n",
        "    patches = [\n",
        "        mpatches.Patch(\n",
        "            color=np.array(TRAIN_ID_TO_COLOR[i]) / 255.0,\n",
        "            label=f\"{i}: {TRAIN_CLASSES[i]}\"\n",
        "        )\n",
        "        for i in range(len(TRAIN_CLASSES))\n",
        "    ]\n",
        "    axes[2].legend(handles=patches, bbox_to_anchor=(1.05, 1.0),\n",
        "                   loc=\"upper left\", borderaxespad=0.)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# Try a couple of samples\n",
        "visualize_camvid_sample(train_dataset, idx=0)\n",
        "visualize_camvid_sample(train_dataset, idx=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gUUUYd5xPz2Z"
      },
      "source": [
        "### Network Architecture Overview\n",
        "\n",
        "We now design the architecture of our DenseNet model \u2014 a fully convolutional encoder\u2013decoder network tailored for semantic segmentation.\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://raw.githubusercontent.com/SimJeg/FC-DenseNet/cf2375bf9f6ed20ba029a5ee540261aad89732d5/DenseNet.jpg\" width=\"650\"/>\n",
        "</p>\n",
        "\n",
        "Conceptually, the network processes the image through a **downsampling path** (encoder), reaches a compressed representation (bottleneck), and then reconstructs a dense prediction map through an **upsampling path** (decoder). Lateral skip connections link encoder features to their corresponding decoder levels, ensuring fine spatial detail is preserved.\n",
        "\n",
        "The architecture is built from the following components:\n",
        "\n",
        "**(a) Dense Layer**\n",
        "\n",
        "**(b) Dense Block**\n",
        "\n",
        "**(c) Transition Down**\n",
        "\n",
        "**(d) Transition Up**\n",
        "\n",
        "**(e) Bottleneck Block**\n",
        "\n",
        "**(f) Final Classifier**\n",
        "\n",
        "We next implement each component in modular form and assemble them into a complete FC-DenseNet for CamVid segmentation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u8g3cAvFw1is"
      },
      "source": [
        "#### **(a) Dense Layer**\n",
        "\n",
        "The basic computational unit in DenseNet is the **dense layer**.  \n",
        "Each layer receives as input **all previous feature maps** in the block, applies:\n",
        "\n",
        "$$\n",
        "\\text{BN} \\rightarrow \\text{ReLU} \\rightarrow 3 \\times 3 \\text{Conv}\n",
        "$$\n",
        "\n",
        "and produces `k` new feature maps (the **growth rate**).  \n",
        "These outputs are concatenated with the input along the channel axis.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ZwFClFdw6LD"
      },
      "outputs": [],
      "source": [
        "\n",
        "class DenseLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    BN -> ReLU -> 3x3 Conv -> Dropout,\n",
        "    then concatenate input and output feature maps.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, growth_rate, drop_prob=0.2):\n",
        "        super(DenseLayer, self).__init__()\n",
        "        self.bn = nn.BatchNorm2d(in_channels)\n",
        "        self.conv = nn.Conv2d(in_channels, growth_rate, kernel_size=3, padding=1, bias=False)\n",
        "        self.dropout = nn.Dropout(drop_prob)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Apply BN -> ReLU -> Conv -> Dropout\n",
        "        out = self.bn(x)\n",
        "        out = F.relu(out)\n",
        "        out = self.conv(out)\n",
        "        out = self.dropout(out)\n",
        "        \n",
        "        # Concatenate input and output\n",
        "        return torch.cat([x, out], 1)\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uB-Hv_mqw6fv"
      },
      "source": [
        "#### **(b) Dense Block**  \n",
        "\n",
        "\n",
        "A dense block stacks several dense layers sequentially.  \n",
        "Each layer receives **all feature maps from previous layers** and contributes new ones:\n",
        "\n",
        "$$\n",
        "C_{\\text{out}} = C_{\\text{in}} + L \\cdot k\n",
        "$$\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://raw.githubusercontent.com/SimJeg/FC-DenseNet/master/DenseBlock.jpg\" width=\"300\" height=\"600\"/>\n",
        "</p>\n",
        "\n",
        "This connectivity pattern promotes feature reuse, stabilizes gradients, and forms the core building unit of our network.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yre9Rh0tw6-f"
      },
      "outputs": [],
      "source": [
        "\n",
        "class DenseBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    A sequence of DenseLayer modules.\n",
        "    Input channels grow by `growth_rate` at each layer.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, num_layers, growth_rate, drop_prob=0.2):\n",
        "        super(DenseBlock, self).__init__()\n",
        "        layers = []\n",
        "        for i in range(num_layers):\n",
        "            # Input channels for layer i is in_channels + i * growth_rate\n",
        "            layers.append(DenseLayer(in_channels + i * growth_rate, growth_rate, drop_prob))\n",
        "        self.layers = nn.ModuleList(layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "        return x\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mLOknqisw7Ts"
      },
      "source": [
        "#### **(c) Transition Down**\n",
        "\n",
        "\n",
        "At the end of each encoder stage, we reduce spatial resolution:\n",
        "\n",
        "$$\n",
        "\\text{BN} \\rightarrow \\text{ReLU} \\rightarrow 1\\times1\\text{ Conv} \\rightarrow \\text{Dropout} \\rightarrow \\text{MaxPool}(2)\n",
        "$$\n",
        "\n",
        "This halves width and height while keeping channels unchanged.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_yRehsC8w7l9"
      },
      "outputs": [],
      "source": [
        "\n",
        "class TransitionDown(nn.Module):\n",
        "    \"\"\"\n",
        "    BN + ReLU + 1x1 Conv + Dropout + MaxPool(2x2)\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, drop_prob=0.2):\n",
        "        super(TransitionDown, self).__init__()\n",
        "        self.bn = nn.BatchNorm2d(in_channels)\n",
        "        self.conv = nn.Conv2d(in_channels, in_channels, kernel_size=1, bias=False)\n",
        "        self.dropout = nn.Dropout(drop_prob)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.bn(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.pool(x)\n",
        "        return x\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bsvJZt-Cx-bB"
      },
      "source": [
        "#### **(d) Transition Up**  \n",
        "\n",
        "\n",
        "In the decoder, we restore resolution using learned upsampling via transposed convolution:\n",
        "\n",
        "$$\n",
        "\\text{ConvTranspose}(3 \\times 3, \\text{stride}=2)\n",
        "$$\n",
        "\n",
        "which doubles spatial resolution before concatenation with skip features.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-gJRuTTAx-ry"
      },
      "outputs": [],
      "source": [
        "\n",
        "class TransitionUp(nn.Module):\n",
        "    \"\"\"\n",
        "    Transposed convolution for upsampling by a factor of 2.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(TransitionUp, self).__init__()\n",
        "        self.transposed_conv = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=3, stride=2, padding=1, output_padding=1, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.transposed_conv(x)\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TYNps3ya2T8c"
      },
      "source": [
        "#### **(e) Bottleneck Block**\n",
        "\n",
        "\n",
        "At the deepest level, the network operates at the lowest spatial resolution but highest channel width.\n",
        "\n",
        "A final dense block processes this information before reconstruction begins in the decoder.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qbhwOdCA4rsI"
      },
      "source": [
        "### Our FC-DenseNet-lite Architecture\n",
        "\n",
        "We implement a compact FC-DenseNet architecture inspired by *The One Hundred Layers Tiramisu*.\n",
        "All convolutional **dense layers** use growth rate $k = 16$. At each dense block:\n",
        "\n",
        "- Input has $m$ feature maps.\n",
        "- The block has $n$ layers.\n",
        "- Each layer adds $k$ new feature maps.\n",
        "- The output of the block therefore has $m + n \\cdot k$ feature maps (because we concatenate all newly created features with the input).\n",
        "\n",
        "Transition Down (TD) blocks keep the number of channels $m$ but downsample in space via 2\u00d72 max pooling.\n",
        "Transition Up (TU) blocks use a 3\u00d73 transposed convolution with stride 2 to upsample.\n",
        "\n",
        "Our **homework architecture (\"FC-DenseNet-lite\")** is defined as follows:\n",
        "\n",
        "- Input: RGB image, $m = 3$\n",
        "- Initial 3\u00d73 convolution: $m = 48$\n",
        "\n",
        "</br>\n",
        "\n",
        "**Downsampling path (encoder)**\n",
        "\n",
        "- Dense Block 1: 4 layers  \n",
        "  $m = 48 + 4 \\cdot 16 = 112$  \n",
        "  + Transition Down \u2192 spatial size /2, channels stay 112\n",
        "- Dense Block 2: 5 layers  \n",
        "  $m = 112 + 5 \\cdot 16 = 192$  \n",
        "  + Transition Down \u2192 channels 192\n",
        "- Dense Block 3: 7 layers  \n",
        "  $m = 192 + 7 \\cdot 16 = 304$  \n",
        "  + Transition Down \u2192 channels 304\n",
        "- Dense Block 4: 10 layers  \n",
        "  $m = 304 + 10 \\cdot 16 = 464$  \n",
        "  + Transition Down \u2192 channels 464\n",
        "\n",
        "</br>\n",
        "\n",
        "\n",
        "**Bottleneck**\n",
        "\n",
        "- Dense Block (bottleneck): 15 layers  \n",
        "  $m = 464 + 15 \\cdot 16 = 704$\n",
        "\n",
        "</br>\n",
        "\n",
        "\n",
        "**Upsampling path (decoder)**\n",
        "\n",
        "At each level, we:\n",
        "1. Apply a Transition Up (TU) to upsample the current feature maps.\n",
        "2. Concatenate with the skip connection from the corresponding encoder level.\n",
        "3. Apply a dense block at that resolution.\n",
        "\n",
        "We use symmetric numbers of layers in the decoder:\n",
        "\n",
        "- TU from bottleneck + skip from Dense Block 4 $\\to$ Dense Block (10 layers)\n",
        "- TU + skip from Dense Block 3 $\\to$ Dense Block (7 layers)\n",
        "- TU + skip from Dense Block 2 $\\to$ Dense Block (5 layers)\n",
        "- TU + skip from Dense Block 1 $\\to$ Dense Block (4 layers)\n",
        "\n",
        "\n",
        "</br>\n",
        "\n",
        "**Final classifier**\n",
        "\n",
        "- 1\u00d71 convolution maps the decoder output to $C = 11$ class logits per pixel.\n",
        "\n",
        "> We call network `FC-DenseNet67` which is a smaller version of `FC-DenseNet103`, the architecture used in the paper."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bcvHmyXk4xr8"
      },
      "outputs": [],
      "source": [
        "\n",
        "class FCDenseNet(nn.Module):\n",
        "    \"\"\"\n",
        "    Fully Convolutional DenseNet for semantic segmentation.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels=3, n_classes=11, growth_rate=16):\n",
        "        super(FCDenseNet, self).__init__()\n",
        "        \n",
        "        # Initial Conv\n",
        "        self.conv1 = nn.Conv2d(in_channels, 48, kernel_size=3, padding=1, bias=False)\n",
        "        \n",
        "        cur_channels = 48\n",
        "        \n",
        "        # --- Encoder ---\n",
        "        \n",
        "        # Block 1 (4 layers)\n",
        "        self.db1 = DenseBlock(cur_channels, 4, growth_rate)\n",
        "        cur_channels += 4 * growth_rate # 48 + 64 = 112\n",
        "        self.skip1_channels = cur_channels\n",
        "        self.td1 = TransitionDown(cur_channels)\n",
        "        \n",
        "        # Block 2 (5 layers)\n",
        "        self.db2 = DenseBlock(cur_channels, 5, growth_rate)\n",
        "        cur_channels += 5 * growth_rate # 112 + 80 = 192\n",
        "        self.skip2_channels = cur_channels\n",
        "        self.td2 = TransitionDown(cur_channels)\n",
        "        \n",
        "        # Block 3 (7 layers)\n",
        "        self.db3 = DenseBlock(cur_channels, 7, growth_rate)\n",
        "        cur_channels += 7 * growth_rate # 192 + 112 = 304\n",
        "        self.skip3_channels = cur_channels\n",
        "        self.td3 = TransitionDown(cur_channels)\n",
        "        \n",
        "        # Block 4 (10 layers)\n",
        "        self.db4 = DenseBlock(cur_channels, 10, growth_rate)\n",
        "        cur_channels += 10 * growth_rate # 304 + 160 = 464\n",
        "        self.skip4_channels = cur_channels\n",
        "        self.td4 = TransitionDown(cur_channels)\n",
        "        \n",
        "        # --- Bottleneck ---\n",
        "        # Block Bottleneck (15 layers)\n",
        "        self.bottleneck = DenseBlock(cur_channels, 15, growth_rate)\n",
        "        cur_channels += 15 * growth_rate # 464 + 240 = 704\n",
        "        \n",
        "        # --- Decoder ---\n",
        "        \n",
        "        # Up 1\n",
        "        self.tu1 = TransitionUp(cur_channels, 240) \n",
        "        self.db_up1 = DenseBlock(240 + self.skip4_channels, 10, growth_rate)\n",
        "        cur_channels = (240 + self.skip4_channels) + 10 * growth_rate # 704 + 160 = 864\n",
        "        \n",
        "        # Up 2\n",
        "        self.tu2 = TransitionUp(cur_channels, 160)\n",
        "        self.db_up2 = DenseBlock(160 + self.skip3_channels, 7, growth_rate)\n",
        "        cur_channels = (160 + self.skip3_channels) + 7 * growth_rate # 464 + 112 = 576\n",
        "        \n",
        "        # Up 3\n",
        "        self.tu3 = TransitionUp(cur_channels, 112)\n",
        "        self.db_up3 = DenseBlock(112 + self.skip2_channels, 5, growth_rate)\n",
        "        cur_channels = (112 + self.skip2_channels) + 5 * growth_rate # 304 + 80 = 384\n",
        "        \n",
        "        # Up 4\n",
        "        self.tu4 = TransitionUp(cur_channels, 80)\n",
        "        self.db_up4 = DenseBlock(80 + self.skip1_channels, 4, growth_rate)\n",
        "        cur_channels = (80 + self.skip1_channels) + 4 * growth_rate # 192 + 64 = 256\n",
        "        \n",
        "        # Final Conv\n",
        "        self.final_conv = nn.Conv2d(cur_channels, n_classes, kernel_size=1)\n",
        "        \n",
        "        # Apply Kaiming He Initialization\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
        "                nn.init.kaiming_uniform_(m.weight, mode='fan_in', nonlinearity='relu')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_uniform_(m.weight)\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Initial\n",
        "        x = self.conv1(x)\n",
        "        \n",
        "        # Encoder\n",
        "        x1 = self.db1(x)\n",
        "        skip1 = x1\n",
        "        x = self.td1(x1)\n",
        "        \n",
        "        x2 = self.db2(x)\n",
        "        skip2 = x2\n",
        "        x = self.td2(x2)\n",
        "        \n",
        "        x3 = self.db3(x)\n",
        "        skip3 = x3\n",
        "        x = self.td3(x3)\n",
        "        \n",
        "        x4 = self.db4(x)\n",
        "        skip4 = x4\n",
        "        x = self.td4(x4)\n",
        "        \n",
        "        # Bottleneck\n",
        "        x = self.bottleneck(x)\n",
        "        \n",
        "        # Decoder\n",
        "        \n",
        "        # Up 1\n",
        "        x = self.tu1(x)\n",
        "        if x.size(2) != skip4.size(2) or x.size(3) != skip4.size(3):\n",
        "            x = F.interpolate(x, size=(skip4.size(2), skip4.size(3)), mode='bilinear', align_corners=True)\n",
        "        x = torch.cat([x, skip4], 1)\n",
        "        x = self.db_up1(x)\n",
        "        \n",
        "        # Up 2\n",
        "        x = self.tu2(x)\n",
        "        if x.size(2) != skip3.size(2) or x.size(3) != skip3.size(3):\n",
        "            x = F.interpolate(x, size=(skip3.size(2), skip3.size(3)), mode='bilinear', align_corners=True)\n",
        "        x = torch.cat([x, skip3], 1)\n",
        "        x = self.db_up2(x)\n",
        "        \n",
        "        # Up 3\n",
        "        x = self.tu3(x)\n",
        "        if x.size(2) != skip2.size(2) or x.size(3) != skip2.size(3):\n",
        "            x = F.interpolate(x, size=(skip2.size(2), skip2.size(3)), mode='bilinear', align_corners=True)\n",
        "        x = torch.cat([x, skip2], 1)\n",
        "        x = self.db_up3(x)\n",
        "        \n",
        "        # Up 4\n",
        "        x = self.tu4(x)\n",
        "        if x.size(2) != skip1.size(2) or x.size(3) != skip1.size(3):\n",
        "            x = F.interpolate(x, size=(skip1.size(2), skip1.size(3)), mode='bilinear', align_corners=True)\n",
        "        x = torch.cat([x, skip1], 1)\n",
        "        x = self.db_up4(x)\n",
        "        \n",
        "        # Final\n",
        "        x = self.final_conv(x)\n",
        "        \n",
        "        return x\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pVWosHBP7iqC"
      },
      "source": [
        "**Sanity Check!**\n",
        "\n",
        "Before training, we verify that the model:\n",
        "\n",
        "- accepts a tensor of shape `(B, 3, H, W)`,\n",
        "- returns logits of shape `(B, C, H, W)` matching the number of classes.\n",
        "\n",
        "This ensures that channel propagation, skip concatenation, and upsampling were implemented correctly.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mpfG0_x-40-o"
      },
      "outputs": [],
      "source": [
        "model = FCDenseNet(in_channels=3, n_classes=len(TRAIN_CLASSES), growth_rate=16)\n",
        "x = torch.randn(1, 3, 224, 224)\n",
        "y = model(x)\n",
        "print(\"Output shape:\", y.shape)  # expected: (1, 11, 224, 224)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W3IMxPNzhSV-"
      },
      "source": [
        "**Answer the following Questions:**\n",
        "\n",
        "<font color=\"red\">1. Why do segmentation networks need spatial priors that classification networks can ignore?</font>  \n",
        "<span style=\"color:red\">Segmentation requires dense prediction\u2014assigning a label to every single pixel. This means the network needs to know *where* objects are, not just *what* they are. Spatial priors (like relative positions of road vs sky, or sidewalk vs building) help the network resolve ambiguities and maintain geometric consistency. Classification networks only need to determine the presence of an object anywhere in the image, so they can discard spatial information (e.g., via global pooling) to gain translation invariance.</span>\n",
        "\n",
        "<font color=\"red\">2. What changes architecturally when we move from \"what is in the image?\" to \"where is it?\"</font>\n",
        "<span style=\"color:red\">We move from encoders that aggressively downsample to lose spatial resolution (capturing \"what\") to encoder-decoder architectures. We add upsampling layers (like transposed convolutions) to recover the spatial resolution lost during pooling. We also use skip connections to re-inject fine-grained spatial details from early layers into the decoder, which is crucial for precise localization (\"where\").</span>\n",
        "\n",
        "<font color=\"red\">3. What failure mode would you expect if skip connections were removed from Tiramisu?</font>  \n",
        "<span style=\"color:red\">Without skip connections, the decoder would have to reconstruct the high-resolution output solely from the low-resolution, highly abstract bottleneck representation. This would likely result in \"blobby\" or coarse segmentations with poor boundaries. Small objects might disappear entirely, and edges would be blurry because the fine spatial details were lost in the encoder.</span>\n",
        "\n",
        "<font color=\"red\">4. How do skip connections influence gradient flow and spatial detail recovery?</font>\n",
        "<span style=\"color:red\">Skip connections provide a direct path for gradients to flow from the loss function back to the early layers of the encoder, mitigating the vanishing gradient problem in deep networks. For spatial detail, they essentially \"copy-paste\" high-resolution feature maps from the encoder to the decoder, allowing the decoder to use these sharp details to refine the segmentation boundaries.</span>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQ-4u7d3-UQG"
      },
      "source": [
        "### Evaluation & Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Jx94cYx-Wet"
      },
      "source": [
        "We now train our network on CamVid and assess its performance. Before launching training, we first define **evaluation metrics** suited for semantic segmentation, followed by the standard training procedure we are used to."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2RngQ3_j_H3f"
      },
      "source": [
        "#### Evaluation Metrics\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "usvpob8z_Jyi"
      },
      "source": [
        "Semantic segmentation predictions assign a class to **every pixel**.  \n",
        "Therefore, our evaluation must measure how well the network labels individual pixels and how well it segments regions belonging to different semantic categories."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aCR4yJTu_NlZ"
      },
      "source": [
        "**1. Pixel-wise Accuracy**\n",
        "\n",
        "Pixel accuracy measures the fraction of correctly classified pixels:\n",
        "\n",
        "$$\n",
        "\\text{PixelAcc} =\n",
        "\\frac{\\sum_{(i,j)} \\mathbf{1}\\left[ \\hat{Y}_{ij} = Y_{ij} \\right]}\n",
        "     {\\sum_{(i,j)} 1},\n",
        "$$\n",
        "\n",
        "where $\\hat{Y}_{ij}$ is the predicted label and $Y_{ij}$ is the ground truth at pixel $(i,j)$.\n",
        "\n",
        "This metric is intuitive and easy to interpret, but can be misleading in imbalanced datasets:\n",
        "large regions like \u201croad\u201d or \u201csky\u201d dominate, masking poor performance on rare classes (e.g., pedestrians or signs).\n",
        "\n",
        "</br>\n",
        "</br>\n",
        "\n",
        "**2. Intersection over Union (IoU)**\n",
        "\n",
        "IoU evaluates segmentation quality by comparing overlap between prediction and ground truth.\n",
        "\n",
        "For a given class $c$, IoU is:\n",
        "\n",
        "$$\n",
        "\\text{IoU}_c =\n",
        "\\frac{\n",
        "|\\{\\hat{Y} = c\\} \\cap \\{Y = c\\}|\n",
        "}{\n",
        "|\\{\\hat{Y} = c\\} \\cup \\{Y = c\\}|\n",
        "}.\n",
        "$$\n",
        "\n",
        "IoU penalizes:\n",
        "\n",
        "- over-segmentation (predicting class $c$ where it does not exist), and  \n",
        "- under-segmentation (missing regions belonging to class $c$).\n",
        "\n",
        "To evaluate the entire model, we compute **mean IoU (mIoU)**:\n",
        "\n",
        "$$\n",
        "\\text{mIoU} = \\frac{1}{C}\\sum_{c=1}^C \\text{IoU}_c,\n",
        "$$\n",
        "\n",
        "where $C$ is the number of semantic classes.  \n",
        "mIoU treats **all classes equally**, even rare ones, making it a standard research metric for segmentation benchmarks including CamVid."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jqAgs-FeAEMW"
      },
      "outputs": [],
      "source": [
        "\n",
        "def pixel_accuracy(pred, target, ignore_index=255):\n",
        "    # pred: (B, C, H, W)\n",
        "    # target: (B, H, W)\n",
        "    pred_labels = torch.argmax(pred, dim=1)\n",
        "    \n",
        "    mask = (target != ignore_index)\n",
        "    correct = (pred_labels[mask] == target[mask]).sum().item()\n",
        "    total = mask.sum().item()\n",
        "    \n",
        "    if total == 0:\n",
        "        return 0.0\n",
        "    return correct / total\n",
        "\n",
        "def intersection_and_union(pred, target, num_classes, ignore_index=255):\n",
        "    pred_labels = torch.argmax(pred, dim=1)\n",
        "    \n",
        "    iou_per_class = []\n",
        "    \n",
        "    for c in range(num_classes):\n",
        "        pred_c = (pred_labels == c)\n",
        "        target_c = (target == c)\n",
        "        \n",
        "        intersection = (pred_c & target_c).sum().item()\n",
        "        union = (pred_c | target_c).sum().item()\n",
        "        \n",
        "        if union == 0:\n",
        "            # Avoid division by zero, and typically we ignore classes not present in the batch/image\n",
        "            # but for mIoU we sometimes treat it as NaN or 1. Let's return None to indicate 'no instance'\n",
        "            iou_per_class.append(float('nan')) \n",
        "        else:\n",
        "            iou_per_class.append(intersection / union)\n",
        "            \n",
        "    return iou_per_class\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1tu4R3xo-GzY"
      },
      "source": [
        "#### Training\n",
        "\n",
        "We train the network using a **pixel-wise cross-entropy loss**, treating segmentation as per-pixel classification.  \n",
        "Pixels belonging to the \u201cvoid\u201d class (255) are ignored:\n",
        "\n",
        "$$\n",
        "\\mathcal{L} = -\\frac{1}{N} \\sum_{(i,j)\\;|\\;Y_{ij}\\neq 255}\n",
        "\\log p\\left(\\, Y_{ij} \\mid X \\, \\right).\n",
        "$$\n",
        "\n",
        "Following the original FC-DenseNet paper:\n",
        "\n",
        "- We use **RMSProp** as the optimizer.\n",
        "- We include **weight decay (L2 regularization)** to encourage small parameter norms and improve stability.\n",
        "\n",
        "The RMSProp update maintains a moving average of squared gradients $v$ and performs:\n",
        "\n",
        "$$\n",
        "\\theta \\leftarrow \\theta - \\alpha \\cdot \\frac{\n",
        "\\nabla_\\theta \\mathcal{L}\n",
        "}{\n",
        "\\sqrt{v + \\epsilon}\n",
        "},\n",
        "$$\n",
        "\n",
        "which adaptively scales learning rates per parameter \u2014 particularly useful in deep architectures like DenseNets.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zoUkRwzzdnYO"
      },
      "source": [
        "**Training Procedure:**\n",
        "\n",
        "We train the network end-to-end over multiple epochs:\n",
        "\n",
        "1. Read a mini-batch of input images and ground-truth masks.\n",
        "2. Forward pass through `FCDenseNet`.\n",
        "3. Compute loss using cross-entropy (ignoring void pixels).\n",
        "4. Backpropagate gradients.\n",
        "5. Update weights using RMSProp.\n",
        "6. Accumulate accuracy and IoU statistics.\n",
        "7. Validate periodically to observe generalization.\n",
        "\n",
        "We repeat this process for 85 epochs, monitoring loss, pixel accuracy, and mIoU to evaluate convergence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "srwh_V3GAORE"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Model Setup\n",
        "model = FCDenseNet(in_channels=3, n_classes=len(TRAIN_CLASSES), growth_rate=16).to(device)\n",
        "\n",
        "# Class weighting for CamVid to handle imbalance\n",
        "# Approximate weights based on CamVid class frequencies (Source: common literature/Kaggle implementations)\n",
        "# Sky, Road, Building are frequent -> low weight\n",
        "# Pole, SignSymbol, Pedestrian are rare -> high weight\n",
        "class_weights = torch.tensor([\n",
        "    0.5,  # Sky\n",
        "    0.5,  # Building\n",
        "    3.0,  # Pole\n",
        "    0.3,  # Road (Very frequent)\n",
        "    0.5,  # Sidewalk\n",
        "    0.5,  # Tree\n",
        "    2.0,  # SignSymbol\n",
        "    1.0,  # Fence\n",
        "    1.0,  # Car\n",
        "    3.0,  # Pedestrian (Important!)\n",
        "    2.5   # Bicyclist\n",
        "]).float().to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(weight=class_weights, ignore_index=VOID_INDEX)\n",
        "optimizer = optim.RMSprop(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
        "\n",
        "def train_segmentation(model, train_loader, val_loader, criterion, optimizer, num_epochs=30):\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    val_ious = []\n",
        "    \n",
        "    # Scheduler to decay LR\n",
        "    # verbose=True removed as it is deprecated/removed in newer PyTorch versions\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        \n",
        "        loop = tqdm(train_loader, leave=True)\n",
        "        loop.set_description(f\"Epoch [{epoch+1}/{num_epochs}]\")\n",
        "        \n",
        "        for images, masks in loop:\n",
        "            images, masks = images.to(device), masks.to(device)\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, masks)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            running_loss += loss.item()\n",
        "            loop.set_postfix(loss=loss.item())\n",
        "            \n",
        "        train_losses.append(running_loss / len(train_loader))\n",
        "        \n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        total_iou = 0.0\n",
        "        valid_classes_count = 0\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            for images, masks in val_loader:\n",
        "                images, masks = images.to(device), masks.to(device)\n",
        "                outputs = model(images)\n",
        "                loss = criterion(outputs, masks)\n",
        "                val_loss += loss.item()\n",
        "                \n",
        "                # Metrics\n",
        "                ious = intersection_and_union(outputs, masks, num_classes=len(TRAIN_CLASSES), ignore_index=VOID_INDEX)\n",
        "                # Filter nans\n",
        "                valid_ious = [x for x in ious if not np.isnan(x)]\n",
        "                if valid_ious:\n",
        "                   total_iou += sum(valid_ious) / len(valid_ious)\n",
        "                   valid_classes_count += 1\n",
        "\n",
        "        avg_val_loss = val_loss / len(val_loader)\n",
        "        avg_mIoU = total_iou / valid_classes_count if valid_classes_count > 0 else 0\n",
        "        val_losses.append(avg_val_loss)\n",
        "        val_ious.append(avg_mIoU)\n",
        "        \n",
        "        scheduler.step(avg_val_loss)\n",
        "        \n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}] Train Loss: {train_losses[-1]:.4f}, Val Loss: {avg_val_loss:.4f}, mIoU: {avg_mIoU:.4f}\")\n",
        "\n",
        "# Train for reasonable amount of epochs\n",
        "train_segmentation(model, train_loader, val_loader, criterion, optimizer, num_epochs=30) \n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05DhLpPbaEfr"
      },
      "source": [
        "Beyond numerical metrics, it is important to **visually inspect** the model\u2019s predictions. In semantic segmentation, this usually means comparing:\n",
        "\n",
        "1. the input RGB image,\n",
        "2. the **ground-truth** segmentation mask,\n",
        "3. the **predicted** segmentation mask.\n",
        "\n",
        "By looking at these side by side, you can quickly see which classes the model recognizes well, where it struggles, typical failure modes.\n",
        "\n",
        "Sample a few Images from the test set, and visualize them in comparison to the ground truth and your own prediction.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_FD6och8aj5L"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Visualize Predictions\n",
        "model.eval()\n",
        "test_images, test_masks = next(iter(val_loader)) # Use val loader as test for now\n",
        "test_images, test_masks = test_images.to(device), test_masks.to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model(test_images)\n",
        "    preds = torch.argmax(outputs, dim=1)\n",
        "\n",
        "def visualize_prediction(img_tensor, mask_tensor, pred_tensor, idx=0):\n",
        "    img = img_tensor[idx].cpu().permute(1, 2, 0).numpy()\n",
        "    img = (img * 0.229 + 0.485).clip(0, 1) # Unnormalize roughly\n",
        "    \n",
        "    mask = mask_tensor[idx].cpu().numpy()\n",
        "    pred = pred_tensor[idx].cpu().numpy()\n",
        "    \n",
        "    color_mask = decode_class_mask(mask, TRAIN_ID_TO_COLOR, VOID_INDEX)\n",
        "    color_pred = decode_class_mask(pred, TRAIN_ID_TO_COLOR, VOID_INDEX)\n",
        "    \n",
        "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "    axes[0].imshow(img)\n",
        "    axes[0].set_title(\"Input Image\")\n",
        "    axes[0].axis('off')\n",
        "    \n",
        "    axes[1].imshow(color_mask)\n",
        "    axes[1].set_title(\"Ground Truth\")\n",
        "    axes[1].axis('off')\n",
        "    \n",
        "    axes[2].imshow(color_pred)\n",
        "    axes[2].set_title(\"Prediction\")\n",
        "    axes[2].axis('off')\n",
        "    \n",
        "    plt.show()\n",
        "\n",
        "# Show first in batch\n",
        "visualize_prediction(test_images, test_masks, preds, idx=0)\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9o_a1EohvHF"
      },
      "source": [
        "**Answer the following Questions:**\n",
        "\n",
        "<font color=\"red\">1. Do the errors your model makes seem semantic (wrong class) or spatial (wrong localization)?</font>  \n",
        "<span style=\"color:red\">Typically, in the early stages, errors are often semantic (confusing road with sidewalk). As training progresses, semantic errors decrease, and we mostly see spatial errors at the boundaries (e.g., the edge between a building and the sky is slightly off by a few pixels). Thin objects like poles are often the hardest and might be missed or broken (both semantic and spatial).</span>\n",
        "\n",
        "<font color=\"red\">2. Which component of the architecture most likely causes that type of error?</font>  \n",
        "<span style=\"color:red\">Spatial errors at boundaries are often due to the downsampling operations (pooling) in the encoder, which lose exact spatial information. Although skip connections help recover this, the recovery isn't perfect, especially if the decoder isn't deep enough or if the skip connections aren't used effectively. Semantic errors (confusing classes) are usually due to the capacity of the encoder or insufficient context (receptive field).</span>\n",
        "\n",
        "<font color=\"red\">3. If you could change one design choice to address it, what would you alter?</font>\n",
        "<span style=\"color:red\">To address spatial errors and boundary precision, I would consider using dilated convolutions (atrous convolutions) in the bottleneck or later encoder blocks instead of downsampling. This expands the receptive field without losing spatial resolution, allowing the network to maintain high-resolution feature maps throughout. Alternatively, using a more powerful backbone or adding an attention mechanism (like in UNet++) could help refine feature selection from skip connections.</span>\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "e6Ctiv2VvlD4"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}