{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Deep Learning: Assignment #4\n",
        "## Submission date: 28/01/2026, 23:59.\n",
        "### Topics:\n",
        "- Word Embeddings\n",
        "- Transformers\n",
        "- Vision Transformers\n",
        "- Few-Shot Learning\n",
        "- Self-Supervised Learning\n"
      ],
      "metadata": {
        "id": "gXRJ7GN8Leg-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Submitted by:**\n",
        "\n",
        "- **Student 1 — Name, ID**\n",
        "- **Student 2 — Name, ID**\n"
      ],
      "metadata": {
        "id": "p7pIe33k8642"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Assignment Instructions:**\n",
        "\n",
        "· Submissions are in **pairs only**. Write both names + IDs at the top of the notebook.\n",
        "\n",
        "· Keep your code **clean, concise, and readable**.\n",
        "\n",
        "· You may work in your IDE, but you **must** paste the final code back into the **matching notebook cells** and run it there.  \n",
        "\n",
        "\n",
        "· <font color='red'>Write your textual answers in red.</font>  \n",
        "(e.g., `<span style=\"color:red\">your answer here</span>`)\n",
        "\n",
        "· All figures, printed results, and outputs should remain visible in the notebook.  \n",
        "Run **all cells** before submitting and **do not clear outputs**.\n",
        "\n",
        "· Use relative paths — **no absolute file paths** pointing to local machines.\n",
        "\n",
        "· **Important:** Your submission must be entirely your own.  \n",
        "Any form of plagiarism (including uncredited use of ChatGPT or AI tools) will result in **grade 0** and disciplinary action.\n"
      ],
      "metadata": {
        "id": "-GKK2by08-LX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Global Setup\n",
        "\n",
        "import os\n",
        "import re\n",
        "import math\n",
        "import random\n",
        "from collections import defaultdict, Counter\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Dict, Tuple, Optional\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from tqdm.auto import tqdm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision\n",
        "import torchvision.transforms as T\n",
        "from torchvision.transforms.functional import InterpolationMode\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)\n"
      ],
      "metadata": {
        "id": "NmYYCmZ-9Uae"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 1: Learning Vision–Language Representations with Transformers (60 Points)\n",
        "\n",
        "Recent multimodal models have shown that images and natural language can be embedded into a **shared semantic space**, enabling tasks such as image–text retrieval and zero-shot inference without training task-specific classifiers.\n",
        "\n",
        "In this question, you will build a simplified multimodal model inspired by CLIP, combining a **Vision Transformer (ViT)** image encoder with a **Transformer-based text encoder**, trained using a **contrastive objective**.\n",
        "\n"
      ],
      "metadata": {
        "id": "fALa6kDDLcDk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load & Preprocess Data"
      ],
      "metadata": {
        "id": "zhoZRpUlWGZX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this homework, we use the **Flickr8k dataset**, a small-scale vision–language dataset commonly used in image captioning and multimodal learning research.\n",
        "\n",
        "The dataset contains:\n",
        "- approximately 8,000 natural images,\n",
        "- five human-written captions per image.\n",
        "\n",
        "Each training example consists of an image paired with one of its captions.  \n",
        "Throughout this assignment, images and captions will be used to learn a **shared embedding space** between vision and language.\n",
        "\n",
        "You may use the following commands to download and extract the dataset into your working directory:\n"
      ],
      "metadata": {
        "id": "i8JZDs0TWHI3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget \"https://github.com/awsaf49/flickr-dataset/releases/download/v1.0/flickr8k.zip\"\n",
        "!unzip -q flickr8k.zip -d ./flickr8k"
      ],
      "metadata": {
        "id": "pThr_RDoWMYT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before training any models, the raw images and captions must be preprocessed into a form suitable for Transformer-based models.\n",
        "\n",
        "Your preprocessing pipeline should include:\n",
        "- loading and transforming images into tensors of a fixed size,\n",
        "- parsing the captions file and associating each image with its captions,\n",
        "- basic text preprocessing (e.g., lowercasing and tokenization),\n",
        "- converting captions into sequences of token indices,\n",
        "- creating attention masks for padded tokens.\n",
        "\n",
        "You are free to choose reasonable design decisions (e.g., maximum caption length, tokenization strategy), as long as they are applied **consistently** throughout the assignment.\n"
      ],
      "metadata": {
        "id": "xqWPdB-HWOn1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Implement"
      ],
      "metadata": {
        "id": "327ZPgjLWXER"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Learning a Shared Vision–Language Representation"
      ],
      "metadata": {
        "id": "DsacHxVoyOkq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "The goal of this part is to learn a **shared embedding space** for images and natural language.\n",
        "\n",
        "Given an image and one of its captions, the model should map both modalities to vectors in the same vector space, such that:\n",
        "- semantically matching image–caption pairs are close,\n",
        "- non-matching pairs are far apart.\n",
        "\n",
        "This shared representation will later be used for retrieval and zero-shot inference, without training task-specific classifiers.\n"
      ],
      "metadata": {
        "id": "za7xzk54cshC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To achieve this goal, you will build two Transformer-based models:\n",
        "\n",
        "- **A Vision Transformer (ViT)** image encoder, which represents an image as a sequence of patch embeddings and produces a single image-level representation.\n",
        "- **A Transformer encoder for text**, which represents a caption as a sequence of token embeddings and produces a single caption-level representation.\n",
        "\n",
        "Both encoders should output vectors of the same dimension.  \n",
        "These vectors will be projected into a shared embedding space and normalized before computing similarity.\n",
        "\n",
        "The Vision Transformer should include:\n",
        "- patch embedding,\n",
        "- positional embeddings,\n",
        "- a learnable classification token,\n",
        "- a Transformer encoder stack.\n",
        "\n",
        "The text encoder should include:\n",
        "- token embeddings,\n",
        "- positional embeddings,\n",
        "- a Transformer encoder stack.\n",
        "\n",
        "You may choose reasonable architectural hyperparameters (e.g., depth, embedding dimension), as long as they are used consistently and yield good results.\n"
      ],
      "metadata": {
        "id": "UoX8sgMHczWE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Implement"
      ],
      "metadata": {
        "id": "Cu-Z3-h9c8OO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Training and Evaluation"
      ],
      "metadata": {
        "id": "2uiEhdBmyLe8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "The two encoders are trained jointly using a **contrastive learning objective** over mini-batches of image–caption pairs.\n",
        "\n",
        "During training, the model should increase similarity between matching image–caption pairs and decrease similarity between non-matching pairs within the same mini-batch.  \n",
        "Similarity between embeddings should be measured using **cosine similarity**.\n",
        "\n",
        "After training, evaluate the learned representations using **retrieval-based metrics**:\n",
        "- image → text retrieval,\n",
        "- text → image retrieval.\n",
        "\n",
        "Report Recall@1, Recall@3, and Recall@5 on the validation set.\n",
        "\n",
        "</br>\n",
        "\n",
        ">**Recall@K**\n",
        "\n",
        "Recall@K is a standard metric for evaluating retrieval-based models.\n",
        "\n",
        "For each query (an image or a caption), the model ranks all candidates from the opposite modality according to embedding similarity.\n",
        "\n",
        "Recall@K measures the fraction of queries for which **at least one correct match** appears among the top $K$ retrieved results.\n",
        "\n",
        "For example:\n",
        "- Recall@1 measures how often the top-ranked result is correct.\n",
        "- Recall@3 measures how often a correct result appears within the top 3.\n",
        "- Recall@5 measures how often a correct result appears within the top 5.\n",
        "\n",
        "Higher Recall@K values indicate better alignment between image and text representations.\n",
        "\n",
        "\n",
        "These metrics will be used throughout the assignment to assess alignment quality.\n"
      ],
      "metadata": {
        "id": "uTS5CCr_c-2N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Implement"
      ],
      "metadata": {
        "id": "EV1KrZELdsVO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Zero-Shot Caption Selection\n",
        "\n"
      ],
      "metadata": {
        "id": "MnpXMfaFtTWJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In deep learning, **zero-shot evaluation** refers to making predictions on a task without training a model specifically for that task.  \n",
        "Instead, the model relies entirely on representations learned during a different training objective.\n",
        "\n",
        "In this assignment, the vision–language model is trained only to align images and captions in a shared embedding space using a contrastive objective. It is not trained to perform caption selection or classification directly. As a result, any success on caption selection reflects the quality of the learned representations and their semantic alignment.\n",
        "\n",
        "In this evaluation, the trained Vision Transformer image encoder and Transformer-based text encoder are used without modification. Given a single image and a set of $N$ candidate captions, consisting of one correct caption and $N-1$ randomly selected captions, the model embeds the image and all captions and selects the caption whose embedding has the highest cosine similarity to the image embedding.\n",
        "\n",
        "No additional parameters are introduced, and no further training is performed.\n",
        "\n",
        "Evaluate caption selection accuracy for $N \\in \\{3, 5, 10, 20, 25\\}$.\n",
        "\n",
        "For each value of $N$, perform the evaluation over the **entire validation set**, using all images in the split. For each image, construct a candidate set consisting of the correct caption and $N-1\\$ randomly selected captions from other images, and record whether the correct caption is ranked highest by cosine similarity.\n",
        "\n",
        "Report the resulting caption selection accuracy as a function of $N$, and visualize the results in a plot with $N$ on the horizontal axis and accuracy on the vertical axis.\n",
        "\n",
        "In addition, include several qualitative examples illustrating both correct and incorrect selections by showing an image alongside the candidate captions and the model’s similarity scores.\n",
        "\n",
        "This evaluation provides an intuitive measure of how well semantic alignment has been learned.\n",
        "\n"
      ],
      "metadata": {
        "id": "jteW68R3yTNj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Implemnt"
      ],
      "metadata": {
        "id": "Ta8PWZhivk_B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Improving Visual Representations with Self-Supervised Learning\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "a8GDayW1ycOP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The experiments above rely on representations learned solely through image–caption alignment. However, Flickr8k provides limited paired supervision, which may limit the quality of the learned visual representations and, in turn, zero-shot performance.\n",
        "\n",
        "Self-supervised learning addresses this limitation by allowing models to learn meaningful visual structure from images alone, without relying on captions or labels.\n",
        "\n",
        "In this part, you will improve the Vision Transformer image encoder by **pretraining it using image-only self-supervised learning**, before performing vision–language alignment."
      ],
      "metadata": {
        "id": "uoBHm9vk0AdT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the self-supervised stage, the Vision Transformer is trained using **only images**, without access to captions. For each image, two different augmented views are generated, and the model is trained to produce similar representations for views of the same image while producing dissimilar representations for views of different images.\n",
        "\n",
        "After self-supervised pretraining, initialize the vision–language model with the pretrained Vision Transformer and repeat the contrastive image–caption training described earlier. You may choose whether to freeze the Vision Transformer or fine-tune it jointly with the text encoder, as long as the choice is applied consistently.\n",
        "\n",
        "Evaluate the resulting model using the same protocols as before: image–text retrieval with Recall@K and zero-shot caption selection for $N = 3, 5, 10, 20, 25$. Compare these results to training the Vision Transformer from scratch, and visualize the differences using appropriate plots. Briefly discuss the effect of self-supervised pretraining on representation quality and zero-shot generalization.\n"
      ],
      "metadata": {
        "id": "v1-g80z61xdp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Implement"
      ],
      "metadata": {
        "id": "dbyYP6VV21sw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Reflection"
      ],
      "metadata": {
        "id": "fwyTHZLz8PNg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Answer the following questions in your own words.  \n",
        "Your answers should demonstrate **conceptual understanding** rather than implementation details.\n",
        "\n",
        "1. The model is trained using a contrastive objective on image–caption pairs, yet it is evaluated on caption selection without being trained for this task explicitly. Explain why this is possible, and what properties of the learned embedding space make zero-shot inference feasible.\n",
        "\n",
        "2. Vision Transformers do not incorporate strong spatial inductive biases, unlike convolutional neural networks. Based on your experiments, discuss how this design choice affects learning in the low-data regime of Flickr8k, both before and after self-supervised pretraining.\n",
        "\n",
        "3. Self-supervised pretraining improves downstream performance even though no captions are used during this stage. Explain what types of information the Vision Transformer can learn during self-supervised pretraining that are useful for vision–language alignment later.\n",
        "\n",
        "4. Contrastive learning relies on negative examples drawn from within a mini-batch. Discuss how batch size influences the quality of learned representations, and how this consideration relates to both multimodal alignment and self-supervised learning.\n"
      ],
      "metadata": {
        "id": "hUC212V63Sni"
      }
    }
  ]
}