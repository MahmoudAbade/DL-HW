{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Deep Learning: Assignment #1\n",
        "## Submission date: 03/12/2025, 23:59.\n",
        "### Topics:\n",
        "- Logistic Regression\n",
        "- Feedforward Neural Networks\n",
        "- Backpropagation\n",
        "- Optimization\n",
        "- Batch Normalization"
      ],
      "metadata": {
        "id": "Cqq-BvoMLfV9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Submitted by:**\n",
        "\n",
        "- **Student 1 (Name, ID)**\n",
        "- **Student 2 (Name, ID)**  \n"
      ],
      "metadata": {
        "id": "tuazQ8v2MMH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Assignment Instruction:**\n",
        "\n",
        "· Submissions are in **pairs only**. Write both names + IDs at the top of the notebook.\n",
        "\n",
        "· Keep your code **clean, concise, and readable**.\n",
        "\n",
        "· You may work in your IDE, but you **must** paste the final code back into the **matching notebook cells** and run it there.  \n",
        "\n",
        "\n",
        "· <font color='red'>Write your textual answers in red.</font>  \n",
        "(e.g., `<span style=\"color:red\">your answer here</span>`)\n",
        "\n",
        "· All figures, printed results, and outputs should remain visible in the notebook.  \n",
        "Run **all cells** before submitting and **do not clear outputs**.\n",
        "\n",
        "· Use relative paths — **no absolute file paths** pointing to local machines.\n",
        "\n",
        "· **Important:** Your submission must be entirely your own.  \n",
        "Any form of plagiarism (including uncredited use of ChatGPT or AI tools) will result in **grade 0** and disciplinary action.\n"
      ],
      "metadata": {
        "id": "bwpMiMn8MCvk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Global Setup ---\n",
        "\n",
        "# Import Libraries\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import itertools\n",
        "import random\n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision import datasets, transforms\n",
        "import torch.optim as optim\n",
        "import torch.nn.init as init\n",
        "\n",
        "# Device\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", DEVICE)"
      ],
      "metadata": {
        "id": "H-7bwIarH56H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 1 — From Classic ML to Deep Networks\n",
        "\n",
        "In this question we embark on a short expedition from classic Machine Learning to Deep Learning.\n",
        "\n",
        "We will compare the performance of a **multiclass logistic regression** to a **multi-layer perceptron (MLP)** on the **MNIST** dataset.\n"
      ],
      "metadata": {
        "id": "RMFtAFBs4f7-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Start by running this cell which includes relative setup and useful helper functions implemented for you to use."
      ],
      "metadata": {
        "id": "jl8omp6tEHWh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Define Seed ---\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "# --- Helper Functions To Use ---\n",
        "def accuracy(logits: torch.Tensor, y: torch.Tensor):\n",
        "    \"\"\"Top-1 accuracy for logits [N,C] and labels [N].\"\"\"\n",
        "    return (logits.argmax(dim=1) == y).float().mean().item()\n",
        "\n",
        "def count_params(obj):\n",
        "    \"\"\"\n",
        "    Count trainable parameters.\n",
        "    - If obj is (W, b) tuple → counts elements.\n",
        "    - If obj is a nn.Module → sums requires_grad params.\n",
        "    \"\"\"\n",
        "    if isinstance(obj, tuple) and len(obj) == 2:\n",
        "        W, b = obj\n",
        "        return W.numel() + b.numel()\n",
        "    if isinstance(obj, nn.Module):\n",
        "        return sum(p.numel() for p in obj.parameters() if p.requires_grad)\n",
        "    raise TypeError(\"count_params expects (W,b) or nn.Module.\")\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate_acc(W: torch.Tensor, b: torch.Tensor, loader):\n",
        "    \"\"\"Dataset-level accuracy for a linear softmax model parameterized by (W,b).\"\"\"\n",
        "    total_acc, total_n = 0.0, 0\n",
        "    for xb, yb in loader:\n",
        "        xb = xb.to(DEVICE).view(xb.size(0), -1)\n",
        "        yb = yb.to(DEVICE)\n",
        "        logits = xb @ W + b\n",
        "        batch_acc = accuracy(logits, yb)\n",
        "        n = xb.size(0)\n",
        "        total_acc += batch_acc * n         # weight by batch size\n",
        "        total_n   += n\n",
        "    return total_acc / total_n\n",
        "\n",
        "\n",
        "# Use this function in the training loop for your nn.Module\n",
        "@torch.no_grad()\n",
        "def evaluate_module(model: nn.Module, loader):\n",
        "    model.eval()\n",
        "    total_acc, total_n = 0.0, 0\n",
        "    for xb, yb in loader:\n",
        "        xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
        "        logits = model(xb)\n",
        "        batch_acc = accuracy(logits, yb)\n",
        "        n = xb.size(0)\n",
        "        total_acc += batch_acc * n\n",
        "        total_n   += n\n",
        "    return total_acc / total_n"
      ],
      "metadata": {
        "id": "KSo1jo1w_vCS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load & Preprocess the Data:\n",
        "\n"
      ],
      "metadata": {
        "id": "znsUVksfebS5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We first load the MNIST dataset as provided by `torchvision`. All images are automatically converted to tensors in the range $[0,1]$.\n"
      ],
      "metadata": {
        "id": "-3njR3iB_e5D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the raw MNIST dataset\n",
        "transform = transforms.ToTensor()\n",
        "\n",
        "train_full = datasets.MNIST(root=\"./data\", train=True,  download=True, transform=transform)\n",
        "test_set   = datasets.MNIST(root=\"./data\", train=False, download=True, transform=transform)\n",
        "\n",
        "print(f\"\\n Train set: {len(train_full)} samples  |  Test set: {len(test_set)} samples\")"
      ],
      "metadata": {
        "id": "EyqF3cxV_baB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implement the following steps:\n",
        "\n",
        "1. **Split the data:**\n",
        "   - The MNIST test set is already provided.\n",
        "   - From the full training set, create an 80/20 split into **train** and **validation** subsets.\n",
        "\n",
        "2. **Construct `DataLoader` objects:**\n",
        "   - `train_loader` → use shuffling.\n",
        "   - `val_loader` → no shuffling.\n",
        "   - `test_loader` → no shuffling.2.\n",
        "\n",
        "3. **Print a quick sanity check:**\n",
        "   - batch shapes `(batch_size = 128, 1, 28, 28)`\n",
        "   - label range (0–9)\n",
        "\n",
        "*Example format:*\n",
        "```python\n",
        "Batch: torch.Size([128, 1, 28, 28]) torch.Size([128]) | pixel range = (0.0, 1.0)\n"
      ],
      "metadata": {
        "id": "mv240yiog3Jj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Implement preprocessing"
      ],
      "metadata": {
        "id": "ltfwQi51ex9t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  Multiclass Logistic Regression\n",
        "\n"
      ],
      "metadata": {
        "id": "4KjEI-7D_7mM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In binary logistic regression, we model  \n",
        "$$\n",
        "P(y=1 \\mid x) = \\sigma(w^\\top x + b)\n",
        "$$\n",
        "where  $\\sigma$ is the sigmoid activation function.\n",
        "\n",
        "For **multiclass** problems, like ours where MNIST is a dataset of images of 10 digits, hence 10 classes, we generalize this idea.\n",
        "\n",
        "Each class $c \\in \\{0, \\dots, 9\\}$ has its own weight vector  $w_c$ and bias  $b_c$.\n",
        "\n",
        "We stack them into a **weight matrix**  $W \\in \\mathbb{R}^{d \\times C}$\n",
        "and a **bias vector**  $b \\in \\mathbb{R}^{C}$.\n",
        "\n",
        "<br>\n",
        "\n",
        "Generally speaking, we compute a forward pass by computing:\n",
        "\n",
        "$$\n",
        "z_i = W^\\top x_i + b\n",
        "$$\n",
        "\n",
        "where $W \\in \\mathbb{R}^{d \\times C}$ is the weight matrix and $b \\in \\mathbb{R}^C$ is the bias vector.\n",
        "\n",
        "<br>\n",
        "\n",
        "We then obtain class probabilities via the softmax function:\n",
        "\n",
        "$$\n",
        "p_i(c) = \\frac{e^{z_i(c)}}{\\sum_{k=1}^{C} e^{z_i(k)}}\n",
        "$$\n",
        "\n",
        "The model is trained by minimizing the **cross-entropy loss** between the predicted probabilities and the true class labels:\n",
        "\n",
        "$$\n",
        "\\mathcal{L}(W,b) = -\\frac{1}{N} \\sum_{i=1}^N \\log p_i(y_i)\n",
        "$$\n",
        "\n",
        "and the parameters are updated using **gradient descent**:\n",
        "\n",
        "$$\n",
        "W \\leftarrow W - \\eta \\, \\nabla_W \\mathcal{L}, \\qquad\n",
        "b \\leftarrow b - \\eta \\, \\nabla_b \\mathcal{L}\n",
        "$$\n",
        "\n",
        "\n",
        "<br><br>\n",
        "In this question, you will build a multiclass logistic regression model from scratch, train it using the cross-entropy loss, and optimize it with gradient descent.\n",
        "\n",
        "<br>\n",
        "\n",
        "You will then evaluate the model’s performance on the MNIST dataset, and train it on 20 epochs.\n"
      ],
      "metadata": {
        "id": "7SkDxB6PSwHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def logistic_regression(train_loader, val_loader, epochs=20, lr=0.1, tol=1e-6):\n",
        "    \"\"\"\n",
        "    Train a multiclass logistic regression model using gradient descent.\n",
        "    - X: [N, d] input features (flattened images)\n",
        "    - y: [N] class labels in {0,...,9}\n",
        "    - lr: learning rate\n",
        "    - max_steps: max number of iterations\n",
        "    - tol: stop early when gradients converge\n",
        "\n",
        "    Returns: (W, b)\n",
        "    \"\"\"\n",
        "\n",
        "    # TODO: Implement logistic_regression\n",
        "\n"
      ],
      "metadata": {
        "id": "adjKo8KWH8aK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now run the next cell to evaluate your multiclass logistic regression.\n",
        "\n",
        "We use the `evaluate_acc` helper function to evaluate.\n"
      ],
      "metadata": {
        "id": "rh1Hm_do9k5U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Run\n",
        "W, b = logistic_regression(train_loader, val_loader, epochs=20, lr=0.1)\n",
        "\n",
        "model = (W, b)\n",
        "\n",
        "# Evaluate\n",
        "val_acc  = evaluate_acc(W, b, val_loader)\n",
        "test_acc = evaluate_acc(W, b, test_loader)\n",
        "\n",
        "print(f\"\\nNumber of Parameters: {count_params(model):,}\")\n",
        "print(f\"Val. acc.: {val_acc:.4f}\")\n",
        "print(f\"Test acc.: {test_acc:.4f}\")"
      ],
      "metadata": {
        "id": "s2rGj2FT6KrD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer the following questions:\n",
        "\n",
        "1. Is a *logistic regression* a sufficient model for this task?  \n",
        "   What **constraints** does this model impose on the decision boundaries and on what it can “represent”?  \n",
        "   What **modification** might help address these constraints?\n",
        "\n",
        "\n",
        "2. Give two examples of incorrectly classified digits. Include the image of the misclassified digit,\n",
        "the predicted class and the actual class in your write up\n",
        "\n",
        "</br>\n",
        "\n",
        "<font color='red'>Write your answers here for questions above, with an explaination as requested.</font>"
      ],
      "metadata": {
        "id": "c9OfQ7qpCFGj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Feedforward Neural Network\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "cmYeojbPFXe2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we learned in class, a **Feedforward Neural Network (FFNN)** maps an input — in our case, a flattened $28 \\times 28$ image, $x \\in \\mathbb{R}^{784}$ — through a sequence of **linear layers** interleaved with **nonlinear activation functions**.\n",
        "\n",
        "Formally, the computation proceeds as follows:\n",
        "\n",
        "$$\n",
        "y^{(1)} = \\sigma (W^{(1)}x+b^{(1)})\n",
        "$$\n",
        "$$\n",
        "y^{(\\ell)} = \\sigma (W^{(1)}y^{(\\ell -1)}+b^{(1)})\n",
        "$$\n",
        "\n",
        "\n",
        "Where:\n",
        "- $y^{(i)}$ is the output of layer $i$.\n",
        "- $W^{(i)}$ and $b^{(i)}$ are the weight matrix and bias vector for layer $i$, respectively.\n",
        "- $\\sigma ^{(i)}$ is the activation function used in layer $i$.\n",
        "\n",
        "In this section you will implement a FFNN from scratch, and see how it fairs with the MNIST dataset."
      ],
      "metadata": {
        "id": "zhvDn8ZkVfNL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the cell below, implement your `MLP` network, a two-layer Multi-Layer Perceptron with no regularization to classify MNIST with the following architecture:\n",
        "\n",
        "- Input layer : 784 nodes (MNIST images size)\n",
        "- First hidden layer : 400 nodes\n",
        "- Second hidden layer : 400 nodes\n",
        "- Output layer : 10 nodes (number of classes for MNIST)\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "- Use `RELU()` as your activation function for each layer."
      ],
      "metadata": {
        "id": "Daf3EJqkVnKs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        # TODO: implement\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)"
      ],
      "metadata": {
        "id": "bcd4kayMFv07"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After defining `MLP` class, train it for **20 epochs** using **cross-entropy loss** and **Adam** as your optimizer.  \n",
        "\n",
        "At the end of every epoch, evaluate the model both on the training set and on the validation set. You should print the loss, the training accuracy, and the validation accuracy so that you can monitor how the model improves over time and detect potential overfitting.\n",
        "\n",
        "Once training is complete, produce a plot that shows how the training accuracy and the validation accuracy evolve across the 20 epochs. The plot should clearly reflect the learning dynamics of your model and allow you to compare its performance to the logistic regression model from the previous section.\n",
        "\n",
        "> You already have the helpers `evaluate_module`, `accuracy`, and `count_params` in the setup cell— you may use them directly in your implementation.\n"
      ],
      "metadata": {
        "id": "2_NbWBsDS3qV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Implement"
      ],
      "metadata": {
        "id": "8LS032KjFz0U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"red\">Answer the following questions:</font>\n",
        "\n",
        "<ol>\n",
        "  <li>\n",
        "    Using your results, explain why this model outperforms the logistic-regression baseline.\n",
        "    What key property of the architecture could be tied to the success?\n",
        "    Cite specific evidence from your plots or errors.\n",
        "    <br>\n",
        "  </li>\n",
        "<br>\n",
        "  <li>\n",
        "    Did you need all 20 epochs? If not, choose a stopping point from the validation curve, and justify it.\n",
        "    Propose a concrete rule you would use next time to limit computation waste (spell out the metric and the trigger).\n",
        "    <br>\n",
        "  </li>\n",
        "\n",
        "\n",
        "</ol>\n"
      ],
      "metadata": {
        "id": "5aD7f2qgMb7r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exploring Initialization"
      ],
      "metadata": {
        "id": "5PZpjan7KyHp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we’ll explore how **weight initialization** affects training dynamics and accuracy. In this section, you will expirement with different initilization methods, **changing only the initialization scheme** while keeping **everything else fixed**, including the model architecture, optimizer, data splits, number of epochs, batch size, etc.\n",
        "\n",
        "<br>\n",
        "\n",
        "In the previous code, you used PyTorch’s default: **Kaiming/He initialization**, which scales weights so that\n",
        "$$\n",
        "\\mathrm{Var}(W_{ij}) \\approx \\frac{2}{\\text{fan}_{in}}\n",
        "$$\n",
        "to help stabilize activations and gradients with ReLU layers.\n",
        "\n",
        "In this section, you'll compare this baseline to four alternatives:\n",
        "1. Zero initialization\n",
        "2. Uniform $[0,1]$\n",
        "3. Standard Normal: $\\mathcal{N}(0,1)$\n",
        "4. Xavier initialization\n",
        "\n",
        "For each scheme, reinitialize <strong>all</strong> linear layers (weights and biases), train for 20 epochs using adam optimizer, and record validation accuracy per epoch and the final test accuracy. Then plot all validation-accuracy curves on one figure, report the final test accuracies for each method.\n",
        "\n",
        "> Tip: Recreate a **fresh model** for each initialization; don’t re-use trained weights between runs."
      ],
      "metadata": {
        "id": "OFWVD_CnGKbo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Implement"
      ],
      "metadata": {
        "id": "vRiTeqRZK7m_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"red\">Answer the following question:</font>\n",
        "\n",
        "Why does zero initialization cause training to fail, even though the optimizer still updates the weights?\n"
      ],
      "metadata": {
        "id": "GMYPUHM056Gv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exploring Optimization Methods\n"
      ],
      "metadata": {
        "id": "s2LNPoBNj7I1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we’ll explore how **different Optimization** methods affect training dynamics and accuracy. In this section, you will expirement with different optimizers, **changing only the optimization method** while keeping **everything else fixed**, including the model architecture, selecting the best initialization scheme you got earlier, data splits, number of epochs, batch size, etc.\n",
        "\n",
        "<br>\n",
        "\n",
        "Up to this point, you used the **Adam** optimizer. Here, you will experiment with alternative optimizers and compare their performance under the same settings.\n",
        "\n",
        "In this section, you'll compare this baseline to three other alternatives:\n",
        "1. Stochastic Gradient Descent (SGD)\n",
        "2. RMSProp\n",
        "3. Adagrad\n",
        "\n",
        "\n",
        "As you did with initilization exploring, for each optimizer, create a **fresh model** and apply the **same fixed initialization** to all `nn.Linear` layers (weights & biases). Train the model for **20 epochs**. Plot all **validation-accuracy vs. epoch** curves on one figure and compare test accuracies.\n",
        "\n",
        "> Tip: Recreate a **fresh model** for each initialization; don’t re-use trained weights between runs."
      ],
      "metadata": {
        "id": "RHY7EnF_VdoQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Implement"
      ],
      "metadata": {
        "id": "Hoq03PxrkFIz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Reflection"
      ],
      "metadata": {
        "id": "BvJavva86VKl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Consider the interaction between initialization and optimization:\n",
        "\n",
        "- Combine the **best-performing initialization** with the **worst-performing optimizer**.\n",
        "- Combine the **worst-performing initialization** with the **best-performing optimizer**.\n",
        "\n",
        "Run both experiments and compare the learning dynamics.\n",
        "\n"
      ],
      "metadata": {
        "id": "NVcPSalg6e_C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Implement"
      ],
      "metadata": {
        "id": "0g_ISgaY6py2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 2:  Multidimentional XOR and backpropagation\n",
        "\n",
        "Given $ d $ bits $b=(b_1, b_2, ... , b_d) \\in \\{0, 1\\}^d$, we define $$XOR(b) = Σ_{i=1}^db_i\\ mod\\ 2$$\n",
        "\n",
        "In this question, you will train a feedforward neural network to model the multidimensional XOR problem.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "chJr15C6b3J4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following function returns the labeled XOR \"dataset\" for any given $d$ (`dim`)"
      ],
      "metadata": {
        "id": "np4zMfa7j_gQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def XORData(dim):\n",
        "  X = np.array(list(itertools.product([0, 1], repeat=dim)))\n",
        "  Y = X.sum(axis=1)%2\n",
        "  return X, Y"
      ],
      "metadata": {
        "id": "1U1fydfZkB3c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implement the following:\n",
        "\n",
        "1. A linear layer class:`Linear(torch.nn.Module)`\\\n",
        "• You should inherit from `torch.nn.Module`\\\n",
        "• You can not use `torch.nn.LinearLayer` or any other model implemented by PyTorch, only tensors. Wrap every tensor object with `torch.nn.Parameter` (for the optimizer).\\\n",
        "• Use `torch.randn` to initialize the parameter tensors (weights matrix and biases vector)\\\n",
        "• Implement the `forward` method according to the linear model $y = Wx + b$\\\n",
        "• The number of parameters should be defined by `in_features` and `out_features` \\\n",
        "• Make sure to enable gradient computation!"
      ],
      "metadata": {
        "id": "n2KhDk8dk1zR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Linear(nn.Module):\n",
        "  def __init__(self, in_features, out_features):\n",
        "    # Implement here\n",
        "\n",
        "  def forward(self, x):\n",
        "    # Implement here"
      ],
      "metadata": {
        "id": "NDfRbDU0lUiQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. A feedforward neural network:`FFNet(torch.nn.module)`\\\n",
        "• Use the `Linear` class you implemented to construct a network with `in_features` input size, `out_features` output size, and one hidden layer with `hidden_size` neurons.\\\n",
        "• Use the sigmoid activation function on top of the hidden layer neurons."
      ],
      "metadata": {
        "id": "3Vt_EpKKlW7F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FFNet(nn.Module):\n",
        "  def __init__(self, in_features, out_features, hidden_size):\n",
        "    # Implement here\n",
        "\n",
        "  def forward(self, x):\n",
        "    # Implement here"
      ],
      "metadata": {
        "id": "h-DVaTOmlYWY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exploring the hidden layer\n",
        "In class we discussed the **Universal Approximation Theorem**. The following question will help us to understand how hard it is to find the optimal weight mentioned in the theorem.\n",
        "\n",
        "You are given a basic training function:"
      ],
      "metadata": {
        "id": "uzKXPb-WlaP8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss_func = nn.MSELoss()\n",
        "\n",
        "def train(net, X, Y, epochs=300):\n",
        "  steps = X.shape[0]\n",
        "  for i in range(epochs):\n",
        "      for j in range(steps):\n",
        "          data_point = np.random.randint((X.shape[0]))\n",
        "          x_var = torch.Tensor(X[data_point])\n",
        "          y_var = torch.Tensor([Y[data_point]])\n",
        "          optimizer.zero_grad()\n",
        "          y_hat = net(x_var)\n",
        "          loss = loss_func(y_hat, y_var)\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "      if(i % 100 == 0):\n",
        "          print(\"Epoch:{}, Loss:{}\".format(i, loss.detach()))"
      ],
      "metadata": {
        "id": "eFkYRpzmli-c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the code above, for each of the dimensions: $d = 2, 3, 4, 5$:\n",
        "1. Create the corresponding dataset using `XORData`\n",
        "2. Create models with different hidden layer sizes($1, 2, 3, ..., d,d+const, const*d,...$ etc.) and an optimizer for it using: `optimizer = optim.SGD(model.parameters(), lr=0.02, momentum=0.9)\n",
        "`\n",
        "3. Try to train the different networks on the dataset.\n",
        "4. Plot the losses for different models on the same grid. Reflect on your findings.\n",
        "\n",
        "Make sure to create a new `FFNet` and optimizer objects each time you call `train`.\n"
      ],
      "metadata": {
        "id": "kXA7V3v_8dcC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Implement here"
      ],
      "metadata": {
        "id": "dwJLVCh28gen"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"red\">**Answer the following questions:** </font>\n",
        "\n",
        "1. For which dimensions were you able to reach a model that converges (loss value is geting close to 0)?\n",
        "\n",
        "2. What is the number of hidden neurons in that network and how did you find it?\n",
        "\n",
        "3. What results did you expect and how does it differ from the results you achieved?\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7suNsqpx8jEE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Backpropagation\n",
        "\n",
        "The following part will focus on the computaion of gradients with respect to the model parameters, aimed at understanding the functioning of `loss.backward()`.\n",
        "\n",
        "Implement the function:`calc_gradients(net, x, loss)` with the following instructions:\n",
        "\n",
        "- Input: instance of `FFNet` with a hidden layer of size 2, input of size 2 and output of size 1, input vector `x` and `loss` value.\n",
        "\n",
        "- Output: One dimensional tensor with the gradients of the loss w.r.t each of the parameters, ordered as the corresponding parameters order `net.parameters()` returns (if we would flatten it's output).\n",
        "\n",
        "1. Compute (by hand) the derivatives w.r.t the second linear layer parameters.\n",
        "2. Use the chain rule to compute (by hand) the derivatives w.r.t the first linear layer parameters.\n",
        "3. Implement the final form computations per parameter.\n",
        "4. Order the gradients as described above.\n"
      ],
      "metadata": {
        "id": "1k3wc4dW8kWG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_gradients(net, x, y_hat, loss):\n",
        "  # Implement here"
      ],
      "metadata": {
        "id": "IeOYd_YB8oBQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the following cell to check the correctness of your computation:"
      ],
      "metadata": {
        "id": "YxKx62wD8r6d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def equal_gradients(net, x, y_var, y_hat, loss):\n",
        "  grads = []\n",
        "  for param in net.parameters():\n",
        "    grads.append(param.grad.view(-1))\n",
        "  grads = torch.cat(grads)\n",
        "  return True if torch.sum(grads - calc_gradients(net, x, y_var, y_hat, loss)).round() == 0 else False # Added round because results were very close but not identical\n",
        "\n",
        "def train_and_compare(net, X, Y, epochs=100):\n",
        "  steps = X.shape[0]\n",
        "  for i in range(epochs):\n",
        "      for j in range(steps):\n",
        "          data_point = np.random.randint(X.shape[0])\n",
        "          x_var = torch.Tensor(X[data_point])\n",
        "          y_var = torch.Tensor([Y[data_point]])\n",
        "          optimizer.zero_grad()\n",
        "          y_hat = net(x_var)\n",
        "          loss = loss_func(y_hat, y_var)\n",
        "          loss.backward()\n",
        "          if not equal_gradients(net, x_var, y_var, y_hat, loss.item()):\n",
        "            print(\"Wrong gradients computation!\")\n",
        "            return\n",
        "          optimizer.step()\n",
        "  print(\"Correct gradients computation!\")\n",
        "\n",
        "model = FFNet(2, 1, 2)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.02, momentum=0.9)\n",
        "X, Y = XORData(2)\n",
        "\n",
        "\n",
        "train_and_compare(model, X, Y)"
      ],
      "metadata": {
        "id": "nlzeciblOFkj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}