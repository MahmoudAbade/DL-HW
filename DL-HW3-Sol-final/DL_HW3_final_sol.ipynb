{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Deep Learning: Assignment #3\n",
        "## Submission date: 07/01/2026, 23:59.\n",
        "### Topics:\n",
        "- RNNs\n",
        "- GRUs\n",
        "- LSTMs\n",
        "- Transformers\n"
      ],
      "metadata": {
        "id": "qMNKwshvcblK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Submitted by:**\n",
        "\n",
        "- **Student 1 \u2014 Name, ID**\n",
        "- **Student 2 \u2014 Name, ID**\n"
      ],
      "metadata": {
        "id": "iMUwOkZSc54s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Assignment Instructions:**\n",
        "\n",
        "\u00b7 Submissions are in **pairs only**. Write both names + IDs at the top of the notebook.\n",
        "\n",
        "\u00b7 Keep your code **clean, concise, and readable**.\n",
        "\n",
        "\n",
        "\n",
        "\u00b7 <font color='red'>Write your textual answers in red.</font>  \n",
        "(e.g., `<span style=\"color:red\">your answer here</span>`)\n",
        "\n",
        "\u00b7 All figures, printed results, and outputs should remain visible in the notebook.  \n",
        "Run **all cells** before submitting and **do not clear outputs**.\n",
        "\n",
        "\u00b7 Use relative paths \u2014 **no absolute file paths** pointing to local machines.\n",
        "\n",
        "\u00b7 **Important:** Your submission must be entirely your own.  \n"
      ],
      "metadata": {
        "id": "qkusen4bc8sm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 1 \u2014 Chatbot Tutorial (35 Points)\n",
        "\n",
        "Building a generative chatbot using a seq2seq model on the Cornell Movie-Dialogs Corpus.\n"
      ],
      "metadata": {
        "id": "nChedj8RE02i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load & Preprocess Data\n"
      ],
      "metadata": {
        "id": "NUpTPkZPUrlJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To start, we load the data. Download the data ZIP file\n"
      ],
      "metadata": {
        "id": "uxNiywKRLcqv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Mqe24ejcUJq"
      },
      "outputs": [],
      "source": [
        "!mkdir data\n",
        "!cd data\n",
        "!wget https://zissou.infosci.cornell.edu/convokit/datasets/movie-corpus/movie-corpus.zip\n",
        "!unzip -q movie-corpus.zip\n",
        "!cd .."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After loading the data, let\u2019s import some necessities."
      ],
      "metadata": {
        "id": "JPK6wWGhOlWK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "from __future__ import unicode_literals\n",
        "\n",
        "import torch\n",
        "from torch.jit import script, trace\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "import csv\n",
        "import random\n",
        "import re\n",
        "import os\n",
        "import unicodedata\n",
        "import codecs\n",
        "from io import open\n",
        "import itertools\n",
        "import math\n",
        "import json\n",
        "\n",
        "\n",
        "USE_CUDA = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if USE_CUDA else \"cpu\")"
      ],
      "metadata": {
        "id": "hPbCdr4KMMLu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next step is to reformat our data file and load the data into\n",
        "structures that we can work with.\n",
        "\n",
        "The [Cornell Movie-Dialogs\n",
        "Corpus](https://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html)_\n",
        "is a rich dataset of movie character dialog:\n",
        "\n",
        "-  220,579 conversational exchanges between 10,292 pairs of movie\n",
        "   characters\n",
        "-  9,035 characters from 617 movies\n",
        "-  304,713 total utterances\n",
        "\n",
        "This dataset is large and diverse, and there is a great variation of\n",
        "language formality, time periods, sentiment, etc. Our hope is that this\n",
        "diversity makes our model robust to many forms of inputs and queries.\n",
        "\n",
        "First, we\u2019ll take a look at some lines of our datafile to see the\n",
        "original format.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5VNg1NgUMSvp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls data"
      ],
      "metadata": {
        "id": "TEghaBHkMU5u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = \"movie-corpus\"\n",
        "corpus_name = \"movie-corpus\"\n",
        "\n",
        "def printLines(file, n=10):\n",
        "    with open(file, 'rb') as datafile:\n",
        "        lines = datafile.readlines()\n",
        "    for line in lines[:n]:\n",
        "        print(line)\n",
        "\n",
        "printLines(os.path.join(corpus, \"utterances.jsonl\"))"
      ],
      "metadata": {
        "id": "UBDxLSZQMaDk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Create formatted data file"
      ],
      "metadata": {
        "id": "2NEREmyNUodJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The following functions facilitate the parsing of the raw\n",
        "*utterances.jsonl* data file.\n",
        "\n",
        "\n",
        "In the next cell, you'll find the functions:\n",
        "\n",
        "-  ``extractSentencePairs`` extracts pairs of sentences from\n",
        "   conversations.\n",
        "\n",
        "\n",
        "> Run the code in the next cell."
      ],
      "metadata": {
        "id": "ZPf46yAiN5U1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Splits each line of the file to create lines and conversations\n",
        "def loadLinesAndConversations(fileName):\n",
        "    lines = {}\n",
        "    conversations = {}\n",
        "    with open(fileName, 'r', encoding='iso-8859-1') as f:\n",
        "        for line in f:\n",
        "            lineJson = json.loads(line)\n",
        "            # Extract fields for line object\n",
        "            lineObj = {}\n",
        "            lineObj[\"lineID\"] = lineJson[\"id\"]\n",
        "            lineObj[\"characterID\"] = lineJson[\"speaker\"]\n",
        "            lineObj[\"text\"] = lineJson[\"text\"]\n",
        "            lines[lineObj['lineID']] = lineObj\n",
        "\n",
        "            # Extract fields for conversation object\n",
        "            if lineJson[\"conversation_id\"] not in conversations:\n",
        "                convObj = {}\n",
        "                convObj[\"conversationID\"] = lineJson[\"conversation_id\"]\n",
        "                convObj[\"movieID\"] = lineJson[\"meta\"][\"movie_id\"]\n",
        "                convObj[\"lines\"] = [lineObj]\n",
        "            else:\n",
        "                convObj = conversations[lineJson[\"conversation_id\"]]\n",
        "                convObj[\"lines\"].insert(0, lineObj)\n",
        "            conversations[convObj[\"conversationID\"]] = convObj\n",
        "\n",
        "    return lines, conversations\n",
        "\n",
        "\n",
        "# Extracts pairs of sentences from conversations\n",
        "def extractSentencePairs(conversations):\n",
        "    qa_pairs = []\n",
        "    for conversation in conversations.values():\n",
        "        # Iterate over all the lines of the conversation\n",
        "        for i in range(len(conversation[\"lines\"]) - 1):  # We ignore the last line (no answer for it)\n",
        "            inputLine = conversation[\"lines\"][i][\"text\"].strip()\n",
        "            targetLine = conversation[\"lines\"][i+1][\"text\"].strip()\n",
        "            # Filter wrong samples (if one of the lists is empty)\n",
        "            if inputLine and targetLine:\n",
        "                qa_pairs.append([inputLine, targetLine])\n",
        "    return qa_pairs"
      ],
      "metadata": {
        "id": "Fjk7-RjfNsik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we\u2019ll call these functions and create the file. We\u2019ll call it\n",
        "*formatted_movie_lines.txt*.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0pj4e4Q0N2pc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define path to new file\n",
        "datafile = os.path.join(corpus, \"formatted_movie_lines.txt\")\n",
        "\n",
        "delimiter = '\\t'\n",
        "# Unescape the delimiter\n",
        "delimiter = str(codecs.decode(delimiter, \"unicode_escape\"))\n",
        "\n",
        "# Initialize lines dict and conversations dict\n",
        "lines = {}\n",
        "conversations = {}\n",
        "# Load lines and conversations\n",
        "print(\"\\nProcessing corpus into lines and conversations...\")\n",
        "lines, conversations = loadLinesAndConversations(os.path.join(corpus, \"utterances.jsonl\"))\n",
        "\n",
        "# Write new csv file\n",
        "print(\"\\nWriting newly formatted file...\")\n",
        "with open(datafile, 'w', encoding='utf-8') as outputfile:\n",
        "    writer = csv.writer(outputfile, delimiter=delimiter, lineterminator='\\n')\n",
        "    for pair in extractSentencePairs(conversations):\n",
        "        writer.writerow(pair)\n",
        "\n",
        "# Print a sample of lines\n",
        "print(\"\\nSample lines from file:\")\n",
        "printLines(datafile)"
      ],
      "metadata": {
        "id": "1RODL5HeNvO9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Trim Data"
      ],
      "metadata": {
        "id": "DUprvMAtUjE4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now move from raw text to a representation that a neural network can process.\n",
        "Our next task is to build a **vocabulary** and load the **query/response pairs**\n",
        "into memory.\n",
        "\n",
        "Unlike images, text does not come with an inherent mapping to a numerical space.\n",
        "A sequence model expects **integer token indices**, so we must define a mapping\n",
        "from each unique word in the dataset to a discrete index.\n",
        "\n",
        "To do this, we define a `Voc` (vocabulary) class that maintains:\n",
        "\n",
        "- `word2index`: a mapping from each word to an integer index  \n",
        "- `index2word`: the inverse mapping from indices back to words  \n",
        "- `word2count`: a frequency table used for trimming rare words  \n",
        "- `num_words`: the current vocabulary size  \n",
        "\n",
        "In addition, we reserve a small set of **special tokens**:\n",
        "\n",
        "- `PAD` for padding shorter sequences in a batch  \n",
        "- `SOS` to mark the start of a sequence for the decoder  \n",
        "- `EOS` to mark the end of a sequence  \n",
        "- `UNK` to represent words that are not in the vocabulary  \n",
        "\n",
        "Later, we will remove infrequent words from the vocabulary using a minimum\n",
        "frequency threshold (`MIN_COUNT`). This reduces noise and decreases the effective\n",
        "problem size, which often improves training stability and convergence.\n"
      ],
      "metadata": {
        "id": "n8ZVo4fvUg4n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Default word tokens\n",
        "PAD_token = 0  # Used for padding short sentences\n",
        "SOS_token = 1  # Start-of-sentence token\n",
        "EOS_token = 2  # End-of-sentence token\n",
        "UNK_token = 3  # Unknown word token\n",
        "\n",
        "class Voc:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.trimmed = False\n",
        "        self.word2index = {}\n",
        "        self.word2count = {}\n",
        "        self.index2word = {\n",
        "            PAD_token: \"PAD\",\n",
        "            SOS_token: \"SOS\",\n",
        "            EOS_token: \"EOS\",\n",
        "            UNK_token: \"UNK\",\n",
        "        }\n",
        "        self.num_words = 4  # Count default tokens\n",
        "\n",
        "    def addSentence(self, sentence):\n",
        "        for word in sentence.split(' '):\n",
        "            self.addWord(word)\n",
        "\n",
        "    def addWord(self, word):\n",
        "        if word not in self.word2index:\n",
        "            self.word2index[word] = self.num_words\n",
        "            self.word2count[word] = 1\n",
        "            self.index2word[self.num_words] = word\n",
        "            self.num_words += 1\n",
        "        else:\n",
        "            self.word2count[word] += 1\n",
        "\n",
        "    # Remove words below a certain count threshold\n",
        "    def trim(self, min_count):\n",
        "        if self.trimmed:\n",
        "            return\n",
        "        self.trimmed = True\n",
        "\n",
        "        keep_words = []\n",
        "        for k, v in self.word2count.items():\n",
        "            if v >= min_count:\n",
        "                keep_words.append(k)\n",
        "\n",
        "        print('keep_words {} / {} = {:.4f}'.format(\n",
        "            len(keep_words), len(self.word2index), len(keep_words) / len(self.word2index)\n",
        "        ))\n",
        "\n",
        "        # Reinitialize dictionaries\n",
        "        self.word2index = {}\n",
        "        self.word2count = {}\n",
        "        self.index2word = {\n",
        "            PAD_token: \"PAD\",\n",
        "            SOS_token: \"SOS\",\n",
        "            EOS_token: \"EOS\",\n",
        "            UNK_token: \"UNK\",\n",
        "        }\n",
        "        self.num_words = 4  # Count default tokens\n",
        "\n",
        "        for word in keep_words:\n",
        "            self.addWord(word)\n"
      ],
      "metadata": {
        "id": "8dW73MDYUu9g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We begin by converting Unicode strings to ASCII using `unicodeToAscii`.\n",
        "Next, all text is lowercased and non-letter characters are removed while\n",
        "preserving basic punctuation (`normalizeString`).\n",
        "Finally, to promote stable training and reduce unnecessary computation,\n",
        "we filter out sentence pairs whose length exceeds the `MAX_LENGTH`\n",
        "threshold (`filterPairs`).\n"
      ],
      "metadata": {
        "id": "wHrvoycoU1Cq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LENGTH = 10  # Maximum sentence length to consider\n",
        "\n",
        "def unicodeToAscii(s):\n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn'\n",
        "    )\n",
        "\n",
        "def normalizeString(s):\n",
        "    s = unicodeToAscii(s.lower().strip())\n",
        "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
        "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
        "    s = re.sub(r\"\\s+\", r\" \", s).strip()\n",
        "    return s\n",
        "\n",
        "def readVocs(datafile, corpus_name):\n",
        "    print(\"Reading lines...\")\n",
        "    lines = open(datafile, encoding='utf-8').read().strip().split('\\n')\n",
        "    pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
        "    voc = Voc(corpus_name)\n",
        "    return voc, pairs\n",
        "\n",
        "def filterPair(p):\n",
        "    return len(p[0].split(' ')) < MAX_LENGTH and len(p[1].split(' ')) < MAX_LENGTH\n",
        "\n",
        "def filterPairs(pairs):\n",
        "    return [pair for pair in pairs if filterPair(pair)]\n",
        "\n",
        "def loadPrepareData(corpus, corpus_name, datafile, save_dir):\n",
        "    print(\"Start preparing training data ...\")\n",
        "    voc, pairs = readVocs(datafile, corpus_name)\n",
        "    print(\"Read {!s} sentence pairs\".format(len(pairs)))\n",
        "    pairs = filterPairs(pairs)\n",
        "    print(\"Trimmed to {!s} sentence pairs\".format(len(pairs)))\n",
        "    print(\"Counting words...\")\n",
        "    for pair in pairs:\n",
        "        voc.addSentence(pair[0])\n",
        "        voc.addSentence(pair[1])\n",
        "    print(\"Counted words:\", voc.num_words)\n",
        "    return voc, pairs\n",
        "\n",
        "save_dir = os.path.join(\"data\", \"save\")\n",
        "voc, pairs = loadPrepareData(corpus, corpus_name, datafile, save_dir)\n",
        "\n",
        "print(\"\\nSample pairs:\")\n",
        "for pair in pairs[:10]:\n",
        "    print(pair)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gU_8C4XkWjsB",
        "outputId": "dd8a31bc-eab1-4bb2-a61c-af787676edc2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start preparing training data ...\n",
            "Reading lines...\n",
            "Read 221282 sentence pairs\n",
            "Trimmed to 64313 sentence pairs\n",
            "Counting words...\n",
            "Counted words: 18083\n",
            "\n",
            "Sample pairs:\n",
            "['they do to !', 'they do not !']\n",
            "['she okay ?', 'i hope so .']\n",
            "['wow', 'let s go .']\n",
            "['what good stuff ?', 'the real you .']\n",
            "['the real you .', 'like my fear of wearing pastels ?']\n",
            "['do you listen to this crap ?', 'what crap ?']\n",
            "['well no . . .', 'then that s all you had to say .']\n",
            "['then that s all you had to say .', 'but']\n",
            "['but', 'you always been this selfish ?']\n",
            "['have fun tonight ?', 'tons']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Another technique that often improves training efficiency is **trimming\n",
        "rarely used words** from the vocabulary. Intuitively, words that appear\n",
        "only a handful of times contribute little to the learning signal, while\n",
        "significantly increasing the size of the vocabulary.\n",
        "\n",
        "We perform trimming as a two-step process:\n",
        "\n",
        "1. Remove words that appear fewer than `MIN_COUNT` times from the\n",
        "   vocabulary.\n",
        "2. Remove sentence pairs that contain any of the trimmed words, ensuring\n",
        "   that all remaining training examples are fully represented in the\n",
        "   vocabulary.\n",
        "\n",
        "This procedure reduces noise, lowers the dimensionality of the problem,\n",
        "and often leads to faster and more stable convergence during training.\n"
      ],
      "metadata": {
        "id": "xICI7hK6XX48"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MIN_COUNT = 3    # Minimum word count threshold for trimming\n",
        "\n",
        "def trimRareWords(voc, pairs, MIN_COUNT):\n",
        "    # Trim words used under the MIN_COUNT from the voc\n",
        "    voc.trim(MIN_COUNT)\n",
        "    # Filter out pairs with trimmed words\n",
        "    keep_pairs = []\n",
        "    for pair in pairs:\n",
        "        input_sentence = pair[0]\n",
        "        output_sentence = pair[1]\n",
        "        keep_input = True\n",
        "        keep_output = True\n",
        "        # Check input sentence\n",
        "        for word in input_sentence.split(' '):\n",
        "            if word not in voc.word2index:\n",
        "                keep_input = False\n",
        "                break\n",
        "        # Check output sentence\n",
        "        for word in output_sentence.split(' '):\n",
        "            if word not in voc.word2index:\n",
        "                keep_output = False\n",
        "                break\n",
        "\n",
        "        # Only keep pairs that do not contain trimmed word(s) in their input or output sentence\n",
        "        if keep_input and keep_output:\n",
        "            keep_pairs.append(pair)\n",
        "\n",
        "    print(\"Trimmed from {} pairs to {}, {:.4f} of total\".format(\n",
        "        len(pairs), len(keep_pairs), len(keep_pairs) / len(pairs)\n",
        "    ))\n",
        "    return keep_pairs\n",
        "\n",
        "\n",
        "# Trim voc and pairs\n",
        "pairs = trimRareWords(voc, pairs, MIN_COUNT)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y41EzlFrXcGS",
        "outputId": "725cd735-d2bb-4f70-8c0e-e9d11c828e46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "keep_words 7833 / 18079 = 0.4333\n",
            "Trimmed from 64313 pairs to 53131, 0.8261 of total\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preparing Data for the Model"
      ],
      "metadata": {
        "id": "1nXul3gFYtOr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "So far, we have transformed raw conversational text into a cleaned and\n",
        "trimmed set of *(input, response)* sentence pairs, along with a vocabulary\n",
        "that maps words to integer indices.\n",
        "\n",
        "However, neural sequence models do not operate directly on text. Instead,\n",
        "they expect **numerical tensors** as input. In this section, we convert\n",
        "our sentence pairs into padded tensors that can be efficiently processed\n",
        "by the encoder\u2013decoder model.\n",
        "\n",
        "To accelerate training and make effective use of GPU parallelism, we will\n",
        "train the model using **mini-batches** rather than individual sentence\n",
        "pairs. This introduces an additional challenge: sentences within a batch\n",
        "may have different lengths.\n",
        "\n",
        "To handle variable-length sequences, we adopt the following conventions:\n",
        "\n",
        "- Sentences are converted to sequences of word indices and terminated\n",
        "  with an `EOS_token`.\n",
        "- All sequences in a batch are padded to the length of the longest\n",
        "  sequence using the `PAD_token`.\n",
        "- Batched input tensors are shaped as  \n",
        "  **(max_sequence_length, batch_size)**,  \n",
        "  so that each time step can be processed across all sequences in parallel.\n",
        "\n",
        "This layout is particularly convenient for recurrent models, which\n",
        "process input one time step at a time.\n"
      ],
      "metadata": {
        "id": "Ax3M_oAtYreC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In addition to the padded input and target tensors, we also construct:\n",
        "\n",
        "- A **lengths tensor**, which stores the true (unpadded) length of each\n",
        "  input sequence. This will later be used to efficiently process variable-\n",
        "  length sequences.\n",
        "- A **binary mask tensor** for the target sequences, where padded positions\n",
        "  are marked with 0 and valid tokens with 1. This allows us to ignore padded\n",
        "  values when computing the training loss.\n",
        "\n",
        "The following helper functions implement this batching pipeline.\n"
      ],
      "metadata": {
        "id": "yauQ1U9EZHB4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def indexesFromSentence(voc, sentence):\n",
        "    return [voc.word2index.get(word, UNK_token) for word in sentence.split(' ')] + [EOS_token]\n",
        "\n",
        "\n",
        "def zeroPadding(l, fillvalue=PAD_token):\n",
        "    return list(itertools.zip_longest(*l, fillvalue=fillvalue))\n",
        "\n",
        "\n",
        "def binaryMatrix(l, value=PAD_token):\n",
        "    m = []\n",
        "    for i, seq in enumerate(l):\n",
        "        m.append([])\n",
        "        for token in seq:\n",
        "            if token == PAD_token:\n",
        "                m[i].append(0)\n",
        "            else:\n",
        "                m[i].append(1)\n",
        "    return m\n",
        "\n",
        "\n",
        "# Returns padded input sequence tensor and lengths\n",
        "def inputVar(l, voc):\n",
        "    indexes_batch = [indexesFromSentence(voc, sentence) for sentence in l]\n",
        "    lengths = torch.tensor([len(indexes) for indexes in indexes_batch])\n",
        "    padList = zeroPadding(indexes_batch)\n",
        "    padVar = torch.LongTensor(padList)\n",
        "    return padVar, lengths\n",
        "\n",
        "\n",
        "# Returns padded target sequence tensor, padding mask, and max target length\n",
        "def outputVar(l, voc):\n",
        "    indexes_batch = [indexesFromSentence(voc, sentence) for sentence in l]\n",
        "    max_target_len = max(len(indexes) for indexes in indexes_batch)\n",
        "    padList = zeroPadding(indexes_batch)\n",
        "    mask = binaryMatrix(padList)\n",
        "    mask = torch.BoolTensor(mask)\n",
        "    padVar = torch.LongTensor(padList)\n",
        "    return padVar, mask, max_target_len\n",
        "\n",
        "\n",
        "# Returns all items for a given batch of sentence pairs\n",
        "def batch2TrainData(voc, pair_batch):\n",
        "    # Sort pairs by input sentence length (descending)\n",
        "    pair_batch.sort(key=lambda x: len(x[0].split(\" \")), reverse=True)\n",
        "\n",
        "    input_batch, output_batch = [], []\n",
        "    for pair in pair_batch:\n",
        "        input_batch.append(pair[0])\n",
        "        output_batch.append(pair[1])\n",
        "\n",
        "    inp, lengths = inputVar(input_batch, voc)\n",
        "    output, mask, max_target_len = outputVar(output_batch, voc)\n",
        "    return inp, lengths, output, mask, max_target_len\n"
      ],
      "metadata": {
        "id": "F5uPrvpRZFxI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Architecture\n",
        "We use a Seq2Seq model with an Encoder (Bi-GRU), Decoder (GRU), and Global Attention.\n"
      ],
      "metadata": {
        "id": "hX0acWXjir68"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "#### Sequence-to-Sequence Architecture\n",
        "\n",
        "The core of our chatbot is a **sequence-to-sequence (seq2seq)** model.\n",
        "The objective of a seq2seq model is to map a variable-length input\n",
        "sequence to a variable-length output sequence using a fixed-size neural\n",
        "network.\n",
        "\n",
        "[Sutskever et al. (2014)](https://arxiv.org/abs/1409.3215) demonstrated\n",
        "that this task can be accomplished by composing two recurrent neural\n",
        "networks:\n",
        "\n",
        "- An **encoder**, which processes the input sequence and compresses it\n",
        "  into an internal representation.\n",
        "- A **decoder**, which generates the output sequence one token at a time,\n",
        "  conditioned on the encoder\u2019s representation.\n",
        "\n",
        "In the context of conversational modeling, the encoder reads the input\n",
        "sentence (the query), and the decoder generates the response.\n",
        "\n",
        "This encoder\u2013decoder formulation underlies many modern sequence modeling\n",
        "approaches in machine translation, dialogue systems, and text\n",
        "generation.\n"
      ],
      "metadata": {
        "id": "VGW83R6GiqTu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Encoder\n",
        "\n",
        "In this part, you will implement the **encoder** component of the\n",
        "sequence-to-sequence model.\n",
        "\n",
        "The encoder processes the input sentence one token (word) at a time.\n",
        "At each time step, it produces an output vector and updates an internal\n",
        "**hidden state** that summarizes the sequence observed so far.\n",
        "\n",
        "You should implement the encoder using a **multi-layer Gated Recurrent\n",
        "Unit (GRU)**, introduced by [Cho et al. (2014)](https://arxiv.org/pdf/1406.1078v3.pdf).\n",
        "GRUs extend standard recurrent neural networks by incorporating gating\n",
        "mechanisms that regulate information flow, enabling more effective\n",
        "modeling of long-term dependencies.\n",
        "\n",
        "\n",
        "\n",
        "When processing padded mini-batches, you must correctly pack and unpack\n",
        "sequences using:\n",
        "\n",
        "- `nn.utils.rnn.pack_padded_sequence`\n",
        "- `nn.utils.rnn.pad_packed_sequence`\n",
        "\n",
        "This ensures that the GRU does not perform unnecessary computation over\n",
        "padding tokens.\n"
      ],
      "metadata": {
        "id": "TNvwQHykjUkr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Encoder Inputs**\n",
        "\n",
        "- `input_seq`: Padded batch of input sentences  \n",
        "  Shape: *(max_length, batch_size)*\n",
        "\n",
        "- `input_lengths`: True lengths of each sentence in the batch  \n",
        "  Shape: *(batch_size)*\n",
        "\n",
        "**Encoder Outputs**\n",
        "\n",
        "- `outputs`: Output features from the last hidden layer of the GRU  \n",
        "  (sum of bidirectional outputs)  \n",
        "  Shape: *(max_length, batch_size, hidden_size)*\n",
        "\n",
        "- `hidden`: Final hidden state of the GRU  \n",
        "  Shape: *(n_layers \u00d7 num_directions, batch_size, hidden_size)*\n"
      ],
      "metadata": {
        "id": "YlhQVEO0lweR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, embedding, n_layers=1, dropout=0):\n",
        "        # TODO: Implement\n",
        "\n",
        "    def forward(self, input_seq, input_lengths, hidden=None):\n",
        "        # TODO: Implement\n"
      ],
      "metadata": {
        "id": "EtkW86wKlI9V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Decoder with Attention\n",
        "\n",
        "The decoder generates the response sequence in a token-by-token manner.\n",
        "At each time step, it predicts the next word based on:\n",
        "\n",
        "- its current hidden state,\n",
        "- the previously generated word, and\n",
        "- a **context vector** computed from the encoder outputs.\n",
        "\n",
        "A limitation of vanilla seq2seq models is their reliance on a single\n",
        "fixed-length context vector to represent the entire input sequence.\n",
        "This bottleneck becomes particularly problematic for long input\n",
        "sentences.\n",
        "\n",
        "\n",
        "[Luong et al. (2015)](https://arxiv.org/abs/1508.04025) later proposed\n",
        "**Global Attention**, in which the decoder attends to *all* encoder\n",
        "hidden states at every time step. Attention weights are computed using\n",
        "the decoder\u2019s current hidden state and the encoder outputs via a\n",
        "parameterized **score function**.\n",
        "\n"
      ],
      "metadata": {
        "id": "JSTK4LoclWY_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "At a given decoding step $t$, the attention mechanism computes an\n",
        "alignment score between the current decoder hidden state $h_t$ and each encoder output $\\bar{h_s}$.\n",
        "\n",
        "Luong attention defines three possible **score functions**:\n",
        "\n",
        "- **Dot**:  \n",
        "  $\\text{score}(h_t, \\bar{h}_s) = h_t^\\top \\bar{h}_s$\n",
        "\n",
        "- **General**:  \n",
        "  $\\text{score}(h_t, \\bar{h}_s) = h_t^\\top W \\bar{h}_s$\n",
        "\n",
        "- **Concat**:  \n",
        "  $\\text{score}(h_t, \\bar{h}_s) = v^\\top \\tanh(W [h_t ; \\bar{h}_s])$\n",
        "\n"
      ],
      "metadata": {
        "id": "V1wE-Y-QlqTG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Attn(nn.Module):\n",
        "    def __init__(self, method, hidden_size):\n",
        "        super().__init__()\n",
        "        # TODO: validate method\n",
        "        # TODO: define parameters for the chosen attention method\n",
        "\n",
        "    def forward(self, hidden, encoder_outputs):\n",
        "        # TODO: compute attention energies\n",
        "        # TODO: normalize with softmax\n",
        "        # TODO: return attention weights of shape (batch_size, 1, max_length)"
      ],
      "metadata": {
        "id": "KHmecq2klos8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Decoder with Luong Attention\n",
        "\n",
        "We now define the decoder. Unlike the encoder (which processes the entire input\n",
        "sequence in one forward pass), the decoder is executed **one time step at a time**:\n",
        "at each step it receives a single token and produces a probability distribution\n",
        "over the vocabulary for the next token.\n",
        "\n",
        "Concretely, at decoding step $t$:\n",
        "\n",
        "- The input to the decoder is a tensor `input_step` of shape **(1, batch_size)**,\n",
        "  containing one token per sequence.\n",
        "- After embedding, this becomes **(1, batch_size, hidden_size)**.\n",
        "- The decoder updates its hidden state using a unidirectional GRU.\n",
        "\n",
        "\n",
        "\n",
        "We use **Luong et al. (2015)** global attention:\n",
        "[Luong et al., 2015](https://arxiv.org/abs/1508.04025).\n"
      ],
      "metadata": {
        "id": "kY0UkUFun6KG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "At each decoding step, the decoder should:\n",
        "\n",
        "1. Embed the current input token.\n",
        "2. Run one GRU step to update the decoder hidden state.\n",
        "3. Compute attention weights using the current decoder state and encoder outputs.\n",
        "4. Use the attention weights to compute a context vector.\n",
        "5. Combine the decoder state and context vector.\n",
        "6. Predict a probability distribution over the vocabulary.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "**Inputs**\n",
        "\n",
        "- `input_step`: one time step (one token) for each sequence in the batch  \n",
        "  Shape: **(1, batch_size)**\n",
        "\n",
        "- `last_hidden`: previous decoder hidden state  \n",
        "  Shape: **(n_layers, batch_size, hidden_size)**\n",
        "\n",
        "- `encoder_outputs`: encoder outputs for all time steps  \n",
        "  Shape: **(max_length, batch_size, hidden_size)**\n",
        "\n",
        "**Outputs**\n",
        "\n",
        "- `output`: probability distribution over the vocabulary for the next token  \n",
        "  Shape: **(batch_size, voc.num_words)**\n",
        "\n",
        "- `hidden`: updated decoder hidden state  \n",
        "  Shape: **(n_layers, batch_size, hidden_size)**\n"
      ],
      "metadata": {
        "id": "KTLo7QWpoQI3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LuongAttnDecoderRNN(nn.Module):\n",
        "    def __init__(self, attn_model, embedding, hidden_size, output_size, n_layers=1, dropout=0.1):\n",
        "        # TODO: Implement\n",
        "\n",
        "    def forward(self, input_step, last_hidden, encoder_outputs):\n",
        "        # TODO: Implement"
      ],
      "metadata": {
        "id": "VnQq5GgYoMvF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define Training Procedure\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_MPgDGdroq1T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now define the components required to train our seq2seq chatbot.\n",
        "Training is performed over mini-batches of padded sequences, which\n",
        "requires special care when computing the loss and updating model\n",
        "parameters."
      ],
      "metadata": {
        "id": "R5abIAKLJwpJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Masked Loss\n",
        "\n",
        "Because batches contain padded sequences, not all positions in the target\n",
        "tensor correspond to valid words. We therefore compute the loss only over\n",
        "non-padding positions.\n",
        "\n",
        "The function below computes a **masked negative log-likelihood loss**,\n",
        "given the decoder\u2019s output distribution, the target tokens, and a binary\n",
        "mask that indicates which positions are valid (i.e., not `PAD_token`).\n"
      ],
      "metadata": {
        "id": "HXMD9pMr3SIH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def maskNLLLoss(inp, target, mask):\n",
        "    \"\"\"\n",
        "    inp:    (batch_size, vocab_size) softmax probabilities\n",
        "    target: (batch_size,) target token indices\n",
        "    mask:   (batch_size,) boolean mask (True for valid tokens, False for PAD)\n",
        "    \"\"\"\n",
        "    nTotal = mask.sum()\n",
        "    crossEntropy = -torch.log(torch.gather(inp, 1, target.view(-1, 1)).squeeze(1))\n",
        "    loss = crossEntropy.masked_select(mask).mean()\n",
        "    loss = loss.to(device)\n",
        "    return loss, nTotal.item()"
      ],
      "metadata": {
        "id": "Wmsmc_8D3Ufi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Single Training Iteration\n",
        "\n",
        "We now describe a **single training iteration**, corresponding to one\n",
        "mini-batch update.\n",
        "\n",
        "During each iteration, we apply two commonly used techniques to improve\n",
        "training stability and convergence:\n",
        "\n",
        "- **Teacher forcing**: with some probability, the decoder receives the\n",
        "  ground-truth token as its next input instead of its own prediction.\n",
        "  This can significantly speed up training, but excessive teacher\n",
        "  forcing may lead to poor performance at inference time.\n",
        "\n",
        "- **Gradient clipping**: gradients are clipped to a maximum norm to\n",
        "  prevent the exploding gradient problem, which is particularly common\n",
        "  in recurrent neural networks.\n",
        "\n",
        "The sequence of operations for a single iteration is as follows:\n"
      ],
      "metadata": {
        "id": "ltGbRYR53_Gc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Single Training Iteration (Implement)\n",
        "\n",
        "In this part you will implement a **single mini-batch update** for the\n",
        "seq2seq chatbot.\n",
        "\n",
        "You must handle two key ideas we covered in class:\n",
        "\n",
        "- **Gradient clipping**: clip gradient norms to avoid exploding gradients in RNN training.\n",
        "\n",
        "Your implementation should follow this high-level sequence:\n",
        "\n"
      ],
      "metadata": {
        "id": "Tpn5JL0qGm4b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Sequence of Operations**\n",
        "\n",
        "1. Forward pass the input batch through the encoder.\n",
        "2. Initialize the decoder input with `SOS_token` and the decoder hidden\n",
        "   state with the encoder\u2019s final hidden state.\n",
        "3. Decode one time step at a time:\n",
        "   - apply teacher forcing with probability `teacher_forcing_ratio`\n",
        "   - otherwise, feed the decoder\u2019s own prediction as the next input\n",
        "4. Compute and accumulate masked loss.\n",
        "5. Backpropagate gradients.\n",
        "6. Clip gradients.\n"
      ],
      "metadata": {
        "id": "cz6ux67A_qQ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "YbN0IZL4HzGf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(input_variable, lengths, target_variable, mask, max_target_len,\n",
        "          encoder, decoder, embedding,\n",
        "          encoder_optimizer, decoder_optimizer,\n",
        "          batch_size, clip, max_length=MAX_LENGTH):\n",
        "    \"\"\"\n",
        "    Performs a single mini-batch update step.\n",
        "\n",
        "    Inputs:\n",
        "        input_variable:  (max_input_len, batch_size)\n",
        "        lengths:         (batch_size,)  (must be on CPU for packing)\n",
        "        target_variable: (max_target_len, batch_size)\n",
        "        mask:            (max_target_len, batch_size) boolean\n",
        "        max_target_len:  int\n",
        "\n",
        "    Returns:\n",
        "        avg_loss: average masked loss per non-PAD token (float)\n",
        "    \"\"\"\n",
        "\n",
        "    # Move tensors to the correct device\n",
        "    input_variable = input_variable.to(device)\n",
        "    target_variable = target_variable.to(device)\n",
        "    mask = mask.to(device)\n",
        "    lengths = lengths.to(\"cpu\")  # required by pack_padded_sequence\n",
        "\n",
        "\n",
        "    decoder_input = torch.LongTensor([[SOS_token for _ in range(batch_size)]]).to(device)\n",
        "\n",
        "\n",
        "    use_teacher_forcing = (random.random() < teacher_forcing_ratio)\n",
        "\n",
        "    # We will accumulate loss over time steps\n",
        "    loss = 0\n",
        "    print_losses = []\n",
        "    n_totals = 0\n",
        "\n",
        "\n",
        "    # Return average loss per non-pad token\n",
        "    return sum(print_losses) / n_totals\n"
      ],
      "metadata": {
        "id": "slCS1Ee3_cBW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Training Loop\n",
        "\n",
        "We now wrap the single training iteration into a full training loop.\n",
        "At each iteration, a random mini-batch is sampled and one parameter\n",
        "update is performed.\n",
        "\n",
        "We periodically print training statistics and save model checkpoints,\n",
        "which include the encoder and decoder parameters, optimizer states, and\n",
        "vocabulary information.\n"
      ],
      "metadata": {
        "id": "of4H957VF7QE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def trainIters(model_name, voc, pairs, encoder, decoder, encoder_optimizer, decoder_optimizer,\n",
        "               embedding, encoder_n_layers, decoder_n_layers, save_dir, n_iteration,\n",
        "               batch_size, print_every, save_every, clip, corpus_name, loadFilename):\n",
        "\n",
        "    training_batches = [\n",
        "        batch2TrainData(voc, [random.choice(pairs) for _ in range(batch_size)])\n",
        "        for _ in range(n_iteration)\n",
        "    ]\n",
        "\n",
        "    print('Initializing ...')\n",
        "    start_iteration = 1\n",
        "    print_loss = 0\n",
        "\n",
        "    if loadFilename:\n",
        "        start_iteration = checkpoint['iteration'] + 1\n",
        "\n",
        "    print(\"Training...\")\n",
        "    for iteration in range(start_iteration, n_iteration + 1):\n",
        "        input_variable, lengths, target_variable, mask, max_target_len = training_batches[iteration - 1]\n",
        "\n",
        "        loss = train(input_variable, lengths, target_variable, mask, max_target_len,\n",
        "                     encoder, decoder, embedding,\n",
        "                     encoder_optimizer, decoder_optimizer,\n",
        "                     batch_size, clip)\n",
        "\n",
        "        print_loss += loss\n",
        "\n",
        "        if iteration % print_every == 0:\n",
        "            print_loss_avg = print_loss / print_every\n",
        "            print(f\"Iteration: {iteration}; Percent complete: {iteration / n_iteration * 100:.1f}%; Average loss: {print_loss_avg:.4f}\")\n",
        "            print_loss = 0\n",
        "\n",
        "        if iteration % save_every == 0:\n",
        "            directory = os.path.join(\n",
        "                save_dir, model_name, corpus_name,\n",
        "                f'{encoder_n_layers}-{decoder_n_layers}_{hidden_size}'\n",
        "            )\n",
        "            os.makedirs(directory, exist_ok=True)\n",
        "\n",
        "            torch.save({\n",
        "                'iteration': iteration,\n",
        "                'en': encoder.state_dict(),\n",
        "                'de': decoder.state_dict(),\n",
        "                'en_opt': encoder_optimizer.state_dict(),\n",
        "                'de_opt': decoder_optimizer.state_dict(),\n",
        "                'loss': loss,\n",
        "                'voc_dict': voc.__dict__,\n",
        "                'embedding': embedding.state_dict()\n",
        "            }, os.path.join(directory, f'{iteration}_checkpoint.tar'))\n"
      ],
      "metadata": {
        "id": "pu0PNcyoGEvo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Evaluation\n"
      ],
      "metadata": {
        "id": "OMmmeKgWMzbH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After training the chatbot model, we would like to interact with it and\n",
        "generate responses to user-provided input sentences. To do so, we must\n",
        "define how the decoder generates an output sequence from the encoded\n",
        "input.\n",
        "\n",
        "During training, the decoder may receive ground-truth tokens as input\n",
        "(teacher forcing). During evaluation, however, the model must rely\n",
        "entirely on its own predictions. This requires defining an explicit\n",
        "**decoding strategy**."
      ],
      "metadata": {
        "id": "F3lkjGOYJzXO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Greedy Decoding\n",
        "\n",
        "We use **greedy decoding** to generate responses at evaluation time.\n",
        "\n",
        "Formally, given the decoder output distribution at time step $t$,\n",
        "greedy decoding chooses:\n",
        "\n",
        "$$\n",
        "\\hat{w}_t = \\arg\\max_w \\; p(w \\mid w_{<t}, x)\n",
        "$$\n",
        "\n",
        "limitation, it is simple, efficient, and serves as a strong baseline for sequence generation.\n"
      ],
      "metadata": {
        "id": "RLXvRDF3M28Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To implement greedy decoding, we define a `GreedySearchDecoder` module.\n",
        "This module wraps the encoder and decoder and performs decoding one time step at a time.\n",
        "\n",
        "Given an input sentence, decoding proceeds as follows:\n",
        "\n",
        "**Computation Graph**\n",
        "\n",
        "1. Forward the input sequence through the encoder.\n",
        "2. Initialize the decoder hidden state using the encoder\u2019s final hidden state.\n",
        "3. Initialize the decoder input with the `SOS_token`.\n",
        "4. Iteratively decode one token at a time:\n",
        "   - Forward pass through the decoder.\n",
        "   - Select the most probable token.\n",
        "   - Feed the selected token as input to the next step.\n",
        "5. Collect and return the generated tokens and their scores.\n"
      ],
      "metadata": {
        "id": "R-i_qAUFN4bN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GreedySearchDecoder(nn.Module):\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super(GreedySearchDecoder, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "\n",
        "    def forward(self, input_seq, input_length, max_length):\n",
        "        # Forward input through encoder model\n",
        "        encoder_outputs, encoder_hidden = self.encoder(input_seq, input_length)\n",
        "        # Prepare encoder's final hidden layer to be first hidden input to the decoder\n",
        "        decoder_hidden = encoder_hidden[:decoder.n_layers]\n",
        "        # Initialize decoder input with SOS_token\n",
        "        decoder_input = torch.ones(1, 1, device=device, dtype=torch.long) * SOS_token\n",
        "        # Initialize tensors to append decoded words to\n",
        "        all_tokens = torch.zeros([0], device=device, dtype=torch.long)\n",
        "        all_scores = torch.zeros([0], device=device)\n",
        "        # Iteratively decode one word token at a time\n",
        "        for _ in range(max_length):\n",
        "            # Forward pass through decoder\n",
        "            decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
        "            # Obtain most likely word token and its softmax score\n",
        "            decoder_scores, decoder_input = torch.max(decoder_output, dim=1)\n",
        "            # Record token and score\n",
        "            all_tokens = torch.cat((all_tokens, decoder_input), dim=0)\n",
        "            all_scores = torch.cat((all_scores, decoder_scores), dim=0)\n",
        "            # Prepare current token to be next decoder input (add a dimension)\n",
        "            decoder_input = torch.unsqueeze(decoder_input, 0)\n",
        "        # Return collections of word tokens and scores\n",
        "        return all_tokens, all_scores"
      ],
      "metadata": {
        "id": "90nlU-80OB_Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Evaluating Text Input\n",
        "\n",
        "With the decoding procedure defined, we can now evaluate individual input sentences.\n",
        "\n",
        "The `evaluate` function handles the low-level mechanics of evaluation:\n",
        "\n"
      ],
      "metadata": {
        "id": "SW5kohCkOE3C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(encoder, decoder, searcher, voc, sentence, max_length=MAX_LENGTH):\n",
        "    \"\"\"\n",
        "    Evaluate a single input sentence using greedy decoding.\n",
        "    Out-of-vocabulary words are mapped to UNK_token.\n",
        "    \"\"\"\n",
        "    # words -> indexes (UNK-safe)\n",
        "    indexes = [\n",
        "        voc.word2index.get(word, UNK_token)\n",
        "        for word in sentence.split(' ')\n",
        "    ] + [EOS_token]\n",
        "\n",
        "    indexes_batch = [indexes]\n",
        "\n",
        "    # Create lengths tensor\n",
        "    lengths = torch.tensor([len(indexes)])\n",
        "\n",
        "    # Prepare input batch (max_length, batch_size=1)\n",
        "    input_batch = torch.LongTensor(indexes_batch).transpose(0, 1)\n",
        "    input_batch = input_batch.to(device)\n",
        "    lengths = lengths.to(\"cpu\")\n",
        "\n",
        "    # Decode sentence\n",
        "    tokens, scores = searcher(input_batch, lengths, max_length)\n",
        "\n",
        "    # indexes -> words\n",
        "    decoded_words = [voc.index2word[token.item()] for token in tokens]\n",
        "    return decoded_words\n",
        "\n",
        "\n",
        "def evaluateInput(encoder, decoder, searcher, voc):\n",
        "    \"\"\"\n",
        "    Interactive chatbot interface.\n",
        "    Type 'q' or 'quit' to exit.\n",
        "    \"\"\"\n",
        "    while True:\n",
        "        # Get input sentence\n",
        "        input_sentence = input('> ')\n",
        "        if input_sentence in ('q', 'quit'):\n",
        "            break\n",
        "\n",
        "        # Normalize input\n",
        "        input_sentence = normalizeString(input_sentence)\n",
        "\n",
        "        # Evaluate sentence\n",
        "        output_words = evaluate(encoder, decoder, searcher, voc, input_sentence)\n",
        "\n",
        "        # Remove EOS and PAD tokens from output\n",
        "        output_words = [w for w in output_words if w not in ('EOS', 'PAD')]\n",
        "\n",
        "        print('Bot:', ' '.join(output_words))\n"
      ],
      "metadata": {
        "id": "XlnrdhjeOnBV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run Model"
      ],
      "metadata": {
        "id": "Zzadv_mbXgpV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, it is time to run our model!\n",
        "\n",
        "Regardless of whether we want to train or test the chatbot model, we\n",
        "must initialize the individual encoder and decoder models. In the\n",
        "optimize performance."
      ],
      "metadata": {
        "id": "TLCOsQwVXf49"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Configure models\n",
        "model_name = 'cb_model'\n",
        "attn_model = 'dot'\n",
        "#attn_model = 'general'\n",
        "#attn_model = 'concat'\n",
        "hidden_size = 500\n",
        "encoder_n_layers = 2\n",
        "decoder_n_layers = 2\n",
        "dropout = 0.1\n",
        "batch_size = 64\n",
        "\n",
        "# Set checkpoint to load from; set to None if starting from scratch\n",
        "loadFilename = None\n",
        "checkpoint_iter = 4000\n",
        "#loadFilename = os.path.join(save_dir, model_name, corpus_name,\n",
        "#                            '{}-{}_{}'.format(encoder_n_layers, decoder_n_layers, hidden_size),\n",
        "#                            '{}_checkpoint.tar'.format(checkpoint_iter))\n",
        "\n",
        "\n",
        "# Load model if a loadFilename is provided\n",
        "if loadFilename:\n",
        "    # If loading on same machine the model was trained on\n",
        "    checkpoint = torch.load(loadFilename)\n",
        "    # If loading a model trained on GPU to CPU\n",
        "    #checkpoint = torch.load(loadFilename, map_location=torch.device('cpu'))\n",
        "    encoder_sd = checkpoint['en']\n",
        "    decoder_sd = checkpoint['de']\n",
        "    encoder_optimizer_sd = checkpoint['en_opt']\n",
        "    decoder_optimizer_sd = checkpoint['de_opt']\n",
        "    embedding_sd = checkpoint['embedding']\n",
        "    voc.__dict__ = checkpoint['voc_dict']\n",
        "\n",
        "\n",
        "print('Building encoder and decoder ...')\n",
        "# Initialize word embeddings\n",
        "embedding = nn.Embedding(voc.num_words, hidden_size)\n",
        "if loadFilename:\n",
        "    embedding.load_state_dict(embedding_sd)\n",
        "# Initialize encoder & decoder models\n",
        "encoder = EncoderRNN(hidden_size, embedding, encoder_n_layers, dropout)\n",
        "decoder = LuongAttnDecoderRNN(attn_model, embedding, hidden_size, voc.num_words, decoder_n_layers, dropout)\n",
        "if loadFilename:\n",
        "    encoder.load_state_dict(encoder_sd)\n",
        "    decoder.load_state_dict(decoder_sd)\n",
        "# Use appropriate device\n",
        "encoder = encoder.to(device)\n",
        "decoder = decoder.to(device)\n",
        "print('Models built and ready to go!')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rVFEfeXfXo2l",
        "outputId": "7a0b2c9b-8107-4d0e-8432-55d91a808fe1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Building encoder and decoder ...\n",
            "Models built and ready to go!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Run Training\n",
        "\n",
        "Run the following block in order to train the model.\n",
        "\n",
        "iterations."
      ],
      "metadata": {
        "id": "uk2T7qIYXtPA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Configure training/optimization\n",
        "clip = 50.0\n",
        "teacher_forcing_ratio = 1.0\n",
        "learning_rate = 0.0001\n",
        "decoder_learning_ratio = 5.0\n",
        "n_iteration = 4000\n",
        "print_every = 1\n",
        "save_every = 500\n",
        "\n",
        "# Ensure dropout layers are in train mode\n",
        "encoder.train()\n",
        "decoder.train()\n",
        "\n",
        "# Initialize optimizers\n",
        "print('Building optimizers ...')\n",
        "encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
        "decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate * decoder_learning_ratio)\n",
        "if loadFilename:\n",
        "    encoder_optimizer.load_state_dict(encoder_optimizer_sd)\n",
        "    decoder_optimizer.load_state_dict(decoder_optimizer_sd)\n",
        "\n",
        "# If you have cuda, configure cuda to call\n",
        "for state in encoder_optimizer.state.values():\n",
        "    for k, v in state.items():\n",
        "        if isinstance(v, torch.Tensor):\n",
        "            state[k] = v.cuda()\n",
        "\n",
        "for state in decoder_optimizer.state.values():\n",
        "    for k, v in state.items():\n",
        "        if isinstance(v, torch.Tensor):\n",
        "            state[k] = v.cuda()\n",
        "\n",
        "# Run training iterations\n",
        "print(\"Starting Training!\")\n",
        "trainIters(model_name, voc, pairs, encoder, decoder, encoder_optimizer, decoder_optimizer,\n",
        "           embedding, encoder_n_layers, decoder_n_layers, save_dir, n_iteration, batch_size,\n",
        "           print_every, save_every, clip, corpus_name, loadFilename)"
      ],
      "metadata": {
        "id": "UllNVty_XyH7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Run Evaluation\n",
        "\n",
        "To chat with your model, run the following block."
      ],
      "metadata": {
        "id": "iwLXQmmxX2II"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set dropout layers to eval mode\n",
        "encoder.eval()\n",
        "decoder.eval()\n",
        "\n",
        "# Initialize search module\n",
        "searcher = GreedySearchDecoder(encoder, decoder)\n",
        "\n",
        "# Begin chatting by uncommenting the line:\n",
        "evaluateInput(encoder, decoder, searcher, voc)"
      ],
      "metadata": {
        "id": "kAWQIcv2X4MO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Beam Search Decoding\n"
      ],
      "metadata": {
        "id": "7fkZhXcRvHBm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "So far, we used **greedy decoding**, which selects the most likely next token at each step.\n",
        "Greedy decoding is fast, but it may miss better full sequences because it commits to local choices.\n",
        "\n",
        "In this task you will implement **beam search**, as discussed in class.\n",
        "\n",
        "**Requirements**\n",
        "- Implement `BeamSearchDecoder` similar in interface to `GreedySearchDecoder`.\n",
        "- Use beam width `k` (beam size) as a configurable argument.\n",
        "- Use **log-probabilities** (sum of log-probs) to score sequences.\n",
        "- Return the best decoded token sequence (and optionally its scores)."
      ],
      "metadata": {
        "id": "nqckaLMhJ3QE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BeamSearchDecoder(nn.Module):\n",
        "    def __init__(self, encoder, decoder, beam_size=5):\n",
        "        super(BeamSearchDecoder, self).__init__()\n",
        "        # Implement here\n",
        "\n",
        "    def forward(self, input_seq, input_length, max_length):\n",
        "        # Implement here\n",
        "        raise NotImplementedError"
      ],
      "metadata": {
        "id": "BmxxhuysvMiY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the following cell and try the **same 10 prompts** with greedy vs. beam search.\n",
        "Briefly comment: do you observe differences? When might beam search help, and when might it hurt?\n"
      ],
      "metadata": {
        "id": "2zZLTGUjvM6U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "greedy_searcher = GreedySearchDecoder(encoder, decoder)\n",
        "beam_searcher = BeamSearchDecoder(encoder, decoder)\n",
        "\n",
        "evaluateInput(encoder, decoder, greedy_searcher, voc)\n",
        "evaluateInput(encoder, decoder, beam_searcher, voc)"
      ],
      "metadata": {
        "id": "q3IWk_5ZvS8d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Understanding & Reflection Questions"
      ],
      "metadata": {
        "id": "DcsE-SjmJsGZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Architecture\n",
        "We use a Seq2Seq model with an Encoder (Bi-GRU), Decoder (GRU), and Global Attention.\n"
      ],
      "metadata": {
        "id": "lP0h0J-ws7RY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Neural Machine Translation (35 Points)\n",
        "\n",
        "Translating Portuguese to English using an Encoder-Decoder RNN with Attention.\n"
      ],
      "metadata": {
        "id": "2pW6UubbC2KZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import math\n",
        "import random\n",
        "import time\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "from tqdm.auto import tqdm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, Sampler\n",
        "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "# Seeds\n",
        "SEED = 1234\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Device: {device}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow_datasets as tfds\n",
        "\n",
        "# Load Portuguese-English dataset\n",
        "dataset, info = tfds.load('ted_hrlr_translate/pt_to_en', with_info=True, as_supervised=True)\n",
        "train_examples, val_examples, test_examples = dataset['train'], dataset['validation'], dataset['test']\n",
        "\n",
        "# Helper to convert TF dataset to list of strings\n",
        "def tf_to_list(tf_dataset):\n",
        "    pt_list, en_list = [], []\n",
        "    for pt, en in tfds.as_numpy(tf_dataset):\n",
        "        pt_list.append(pt.decode('utf-8'))\n",
        "        en_list.append(en.decode('utf-8'))\n",
        "    return pt_list, en_list\n",
        "\n",
        "train_pt, train_en = tf_to_list(train_examples)\n",
        "val_pt,   val_en   = tf_to_list(val_examples)\n",
        "test_pt,  test_en  = tf_to_list(test_examples)\n",
        "\n",
        "print(f\"Train size: {len(train_pt)}\")\n",
        "print(f\"Val size:   {len(val_pt)}\")\n",
        "print(f\"Test size:  {len(test_pt)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Tokenization & Vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Reuse special tokens defined earlier\n",
        "PAD_TOKEN = \"<pad>\"\n",
        "UNK_TOKEN = \"<unk>\"\n",
        "BOS_TOKEN = \"<bos>\"\n",
        "EOS_TOKEN = \"<eos>\"\n",
        "\n",
        "SPECIAL_TOKENS = [PAD_TOKEN, UNK_TOKEN, BOS_TOKEN, EOS_TOKEN]\n",
        "\n",
        "def tokenize(sentence: str):\n",
        "    return sentence.strip().split()\n",
        "\n",
        "\n",
        "class Vocabulary:\n",
        "    def __init__(self, sentences, min_freq=2):\n",
        "        \"\"\"\n",
        "        sentences: list of raw text sentences\n",
        "        min_freq : minimum frequency for a word to be included\n",
        "        \"\"\"\n",
        "        counter = Counter()\n",
        "        for sent in sentences:\n",
        "            counter.update(tokenize(sent))\n",
        "\n",
        "        # Initialize vocab with special tokens\n",
        "        self.itos = list(SPECIAL_TOKENS)\n",
        "        self.stoi = {tok: i for i, tok in enumerate(self.itos)}\n",
        "\n",
        "        # Add frequent words\n",
        "        for word, freq in counter.items():\n",
        "            if freq >= min_freq and word not in self.stoi:\n",
        "                self.stoi[word] = len(self.itos)\n",
        "                self.itos.append(word)\n",
        "\n",
        "    def encode(self, sentence, add_bos=False, add_eos=False):\n",
        "        tokens = tokenize(sentence)\n",
        "        ids = [self.stoi.get(tok, self.stoi[UNK_TOKEN]) for tok in tokens]\n",
        "\n",
        "        if add_bos:\n",
        "            ids = [self.stoi[BOS_TOKEN]] + ids\n",
        "        if add_eos:\n",
        "            ids = ids + [self.stoi[EOS_TOKEN]]\n",
        "\n",
        "        return ids\n",
        "\n",
        "    def decode(self, ids):\n",
        "        words = []\n",
        "        for idx in ids:\n",
        "            word = self.itos[idx]\n",
        "            if word in {BOS_TOKEN, PAD_TOKEN}:\n",
        "                continue\n",
        "            if word == EOS_TOKEN:\n",
        "                break\n",
        "            words.append(word)\n",
        "        return \" \".join(words)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.itos)\n",
        "\n",
        "\n",
        "# Build vocabularies\n",
        "pt_vocab = Vocabulary(train_pt, min_freq=2)\n",
        "en_vocab = Vocabulary(train_en, min_freq=2)\n",
        "\n",
        "print(\"Portuguese vocab size:\", len(pt_vocab))\n",
        "print(\"English vocab size:\", len(en_vocab))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Batching"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TranslationDataset(Dataset):\n",
        "    def __init__(self, src_sentences, tgt_sentences, src_vocab, tgt_vocab):\n",
        "        assert len(src_sentences) == len(tgt_sentences)\n",
        "        self.src = src_sentences\n",
        "        self.tgt = tgt_sentences\n",
        "        self.src_vocab = src_vocab\n",
        "        self.tgt_vocab = tgt_vocab\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.src)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        src_ids = self.src_vocab.encode(self.src[idx])\n",
        "        tgt_ids = self.tgt_vocab.encode(self.tgt[idx], add_bos=True, add_eos=True)\n",
        "        return src_ids, tgt_ids\n",
        "\n",
        "def collate_fn(batch):\n",
        "    src_batch, tgt_batch = zip(*batch)\n",
        "\n",
        "    src_lens = torch.tensor([len(x) for x in src_batch], dtype=torch.long)\n",
        "    tgt_lens = torch.tensor([len(x) for x in tgt_batch], dtype=torch.long)\n",
        "\n",
        "    src_pad = pad_sequence(\n",
        "        [torch.tensor(x, dtype=torch.long) for x in src_batch],\n",
        "        batch_first=True,\n",
        "        padding_value=pt_vocab.stoi[PAD_TOKEN]\n",
        "    )\n",
        "    tgt_pad = pad_sequence(\n",
        "        [torch.tensor(x, dtype=torch.long) for x in tgt_batch],\n",
        "        batch_first=True,\n",
        "        padding_value=en_vocab.stoi[PAD_TOKEN]\n",
        "    )\n",
        "\n",
        "    return src_pad, src_lens, tgt_pad, tgt_lens\n",
        "\n",
        "class BucketBatchSampler(Sampler):\n",
        "    def __init__(self, lengths, batch_size, bucket_size=2048, shuffle=True, drop_last=False):\n",
        "        self.lengths = np.asarray(lengths)\n",
        "        self.batch_size = batch_size\n",
        "        self.bucket_size = bucket_size\n",
        "        self.shuffle = shuffle\n",
        "        self.drop_last = drop_last\n",
        "        self.indices = np.arange(len(self.lengths))\n",
        "\n",
        "    def __iter__(self):\n",
        "        idxs = self.indices.copy()\n",
        "        if self.shuffle:\n",
        "            np.random.shuffle(idxs)\n",
        "\n",
        "        for i in range(0, len(idxs), self.bucket_size):\n",
        "            bucket = idxs[i:i + self.bucket_size]\n",
        "            bucket = bucket[np.argsort(self.lengths[bucket])]\n",
        "\n",
        "            for j in range(0, len(bucket), self.batch_size):\n",
        "                batch = bucket[j:j + self.batch_size]\n",
        "                if self.drop_last and len(batch) < self.batch_size:\n",
        "                    continue\n",
        "                yield batch.tolist()\n",
        "\n",
        "    def __len__(self):\n",
        "        n = len(self.indices) // self.batch_size\n",
        "        if not self.drop_last and len(self.indices) % self.batch_size != 0:\n",
        "            n += 1\n",
        "        return n\n",
        "\n",
        "train_dataset = TranslationDataset(train_pt, train_en, pt_vocab, en_vocab)\n",
        "val_dataset   = TranslationDataset(val_pt,   val_en,   pt_vocab, en_vocab)\n",
        "\n",
        "train_src_lengths = [len(pt_vocab.encode(s)) for s in train_pt]\n",
        "val_src_lengths   = [len(pt_vocab.encode(s)) for s in val_pt]\n",
        "\n",
        "BATCH_SIZE = 16  # You may experiment with other batch sizes\n",
        "\n",
        "train_sampler = BucketBatchSampler(train_src_lengths, batch_size=BATCH_SIZE, bucket_size=2048, shuffle=True)\n",
        "val_sampler   = BucketBatchSampler(val_src_lengths,   batch_size=BATCH_SIZE, bucket_size=2048, shuffle=False)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_sampler=train_sampler, collate_fn=collate_fn)\n",
        "val_loader   = DataLoader(val_dataset,   batch_sampler=val_sampler,   collate_fn=collate_fn)\n",
        "\n",
        "print(\"Train batches:\", len(train_loader))\n",
        "print(\"Val batches:\", len(val_loader))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model Architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class EncoderBiLSTM(nn.Module):\n",
        "    def __init__(self, src_vocab_size, emb_dim, hidden_dim, pad_idx, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(src_vocab_size, emb_dim, padding_idx=pad_idx)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.lstm = nn.LSTM(emb_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
        "        self.pad_idx = pad_idx\n",
        "\n",
        "    def forward(self, src_pad, src_lens):\n",
        "        # src_pad: (B, T_src)\n",
        "        # src_lens: (B,)\n",
        "\n",
        "        embedded = self.dropout(self.embedding(src_pad))\n",
        "\n",
        "        # Pack padded sequence\n",
        "        # Note: enforce_sorted=False is generally safer if not pre-sorted\n",
        "        packed = pack_padded_sequence(embedded, src_lens.cpu(), batch_first=True, enforce_sorted=False)\n",
        "\n",
        "        outputs, (hidden, cell) = self.lstm(packed)\n",
        "\n",
        "        # Unpack\n",
        "        outputs, _ = pad_packed_sequence(outputs, batch_first=True)\n",
        "\n",
        "        return outputs, (hidden, cell)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DotProductAttention(nn.Module):\n",
        "    def __init__(self, hidden_dim):\n",
        "        super().__init__()\n",
        "        # Project encoder states (2H) to decoder hidden dim (H)\n",
        "        self.W_p = nn.Linear(hidden_dim * 2, hidden_dim)\n",
        "\n",
        "    def forward(self, dec_h, enc_hiddens, src_lens):\n",
        "        # dec_h: (B, H)\n",
        "        # enc_hiddens: (B, T_src, 2H)\n",
        "        # src_lens: (B,)\n",
        "\n",
        "        # Project encoder hiddens\n",
        "        enc_hiddens_proj = self.W_p(enc_hiddens) # (B, T_src, H)\n",
        "\n",
        "        # Compute scores: (B, T_src)\n",
        "        # (B, H) unsqueeze -> (B, 1, H)\n",
        "        # (B, T_src, H) transpose -> (B, H, T_src)\n",
        "        scores = torch.bmm(enc_hiddens_proj, dec_h.unsqueeze(2)).squeeze(2)\n",
        "\n",
        "        # Masking\n",
        "        max_len = enc_hiddens.size(1)\n",
        "        src_lens = src_lens.to(scores.device)\n",
        "        # mask[i, j] is True if j < len[i] (valid), False if padding\n",
        "        mask = torch.arange(max_len, device=scores.device).expand(len(src_lens), max_len) < src_lens.unsqueeze(1)\n",
        "\n",
        "        scores = scores.masked_fill(~mask, -1e9)\n",
        "\n",
        "        attn_weights = F.softmax(scores, dim=1) # (B, T_src)\n",
        "\n",
        "        # Context vector: (B, 2H)\n",
        "        # (B, 1, T_src) x (B, T_src, 2H) -> (B, 1, 2H)\n",
        "        context = torch.bmm(attn_weights.unsqueeze(1), enc_hiddens).squeeze(1)\n",
        "\n",
        "        return context, attn_weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Decoder Components"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DecoderInit(nn.Module):\n",
        "    \"\"\"\n",
        "    Projects final BiLSTM encoder states -> initial UniLSTM decoder states.\n",
        "\n",
        "    Input:\n",
        "      h_n, c_n: (2, B, H)  (forward + backward)\n",
        "    Output:\n",
        "      h0_dec, c0_dec: (B, H)\n",
        "    \"\"\"\n",
        "    def __init__(self, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.hidden_proj = nn.Linear(hidden_dim * 2, hidden_dim)\n",
        "        self.cell_proj = nn.Linear(hidden_dim * 2, hidden_dim)\n",
        "\n",
        "    def forward(self, h_n, c_n):\n",
        "        # Concatenate forward and backward states along dim 1\n",
        "        # h_n shape is (num_directions, batch, hidden_size) -> (2, B, H)\n",
        "        # Cat (B, H) and (B, H) -> (B, 2H)\n",
        "        h_cat = torch.cat((h_n[0], h_n[1]), dim=1)\n",
        "        c_cat = torch.cat((c_n[0], c_n[1]), dim=1)\n",
        "\n",
        "        h0_dec = torch.tanh(self.hidden_proj(h_cat))\n",
        "        c0_dec = torch.tanh(self.cell_proj(c_cat))\n",
        "\n",
        "        return h0_dec, c0_dec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, tgt_vocab_size, emb_dim, hidden_dim, pad_idx, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(tgt_vocab_size, emb_dim, padding_idx=pad_idx)\n",
        "        self.lstm = nn.LSTMCell(emb_dim, hidden_dim)\n",
        "        self.attention = DotProductAttention(hidden_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # Combine hidden (H) + context (2H) -> H\n",
        "        self.combine_proj = nn.Linear(hidden_dim + hidden_dim * 2, hidden_dim)\n",
        "\n",
        "    def forward(self, tgt_pad, dec_init_state, enc_hiddens, src_lens):\n",
        "        \"\"\"\n",
        "        tgt_pad:       (B, T_tgt) includes <bos> ... <eos>\n",
        "        dec_init_state: (h0_dec, c0_dec) each (B, H)\n",
        "        enc_hiddens:   (B, T_src, 2H)\n",
        "        src_lens:      (B,)\n",
        "\n",
        "        returns:\n",
        "          attn_vecs: (B, T_tgt-1, H)\n",
        "        \"\"\"\n",
        "        batch_size, seq_len = tgt_pad.size()\n",
        "        h, c = dec_init_state\n",
        "\n",
        "        embedded = self.embedding(tgt_pad) # (B, T, E)\n",
        "        embedded = self.dropout(embedded)\n",
        "\n",
        "        attn_vecs = []\n",
        "\n",
        "        # Decode T-1 steps (predicting 2nd token onwards)\n",
        "        # Using teacher forcing: input at t is tgt_pad[:, t]\n",
        "        for t in range(seq_len - 1):\n",
        "            x_t = embedded[:, t, :] # (B, E)\n",
        "\n",
        "            h, c = self.lstm(x_t, (h, c))\n",
        "\n",
        "            context, _ = self.attention(h, enc_hiddens, src_lens)\n",
        "\n",
        "            combined = torch.cat((h, context), dim=1)\n",
        "            attn_vec = torch.tanh(self.combine_proj(combined))\n",
        "\n",
        "            attn_vecs.append(attn_vec)\n",
        "\n",
        "        attn_vecs = torch.stack(attn_vecs, dim=1) # (B, T-1, H)\n",
        "        return attn_vecs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class OutputProjection(nn.Module):\n",
        "    def __init__(self, hidden_dim, tgt_vocab_size):\n",
        "        super().__init__()\n",
        "        self.proj = nn.Linear(hidden_dim, tgt_vocab_size)\n",
        "\n",
        "    def forward(self, attn_vecs):\n",
        "        \"\"\"\n",
        "        attn_vecs: (B, T, H)\n",
        "        returns:\n",
        "          logits: (B, T, |V_tgt|)\n",
        "        \"\"\"\n",
        "        return self.proj(attn_vecs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Full Seq2Seq Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Seq2SeqNMT(nn.Module):\n",
        "    def __init__(self, encoder, decoder, dec_init, out_proj):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.dec_init = dec_init\n",
        "        self.out_proj = out_proj\n",
        "\n",
        "    def forward(self, src_pad, src_lens, tgt_pad):\n",
        "        enc_hiddens, (h_n, c_n) = self.encoder(src_pad, src_lens)\n",
        "        dec_init_state = self.dec_init(h_n, c_n)\n",
        "        attn_vecs = self.decoder(tgt_pad, dec_init_state, enc_hiddens, src_lens)\n",
        "        logits = self.out_proj(attn_vecs)\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "EMB_DIM = 128\n",
        "HIDDEN_DIM = 128\n",
        "DROPOUT = 0.1\n",
        "N_EPOCHS = 25\n",
        "LEARNING_RATE = 0.0005\n",
        "CLIP = 1.0\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Initialize components with NUM_LAYERS\n",
        "encoder = EncoderBiLSTM(len(pt_vocab), EMB_DIM, HIDDEN_DIM, pt_vocab.stoi[PAD_TOKEN], DROPOUT)\n",
        "decoder = Decoder(len(en_vocab), EMB_DIM, HIDDEN_DIM, en_vocab.stoi[PAD_TOKEN], DROPOUT)\n",
        "dec_init = DecoderInit(HIDDEN_DIM)\n",
        "out_proj = OutputProjection(HIDDEN_DIM, len(en_vocab))\n",
        "\n",
        "model = Seq2SeqNMT(encoder, decoder, dec_init, out_proj).to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=en_vocab.stoi[PAD_TOKEN])\n",
        "\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "print(f\"Training on {device}...\")\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "\n",
        "    # Train Loop\n",
        "    for src_pad, src_lens, tgt_pad, tgt_lens in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{N_EPOCHS}\", leave=False):\n",
        "        src_pad, tgt_pad = src_pad.to(device), tgt_pad.to(device)\n",
        "        # but my EncoderBiLSTM calls .cpu() on it, so it's safe to be on any device.\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward\n",
        "        logits = model(src_pad, src_lens, tgt_pad)\n",
        "\n",
        "        # Targets are shifted by 1 (exclude BOS)\n",
        "        targets = tgt_pad[:, 1:]\n",
        "\n",
        "        # Flatten\n",
        "        logits = logits.reshape(-1, logits.shape[-1])\n",
        "        targets = targets.reshape(-1)\n",
        "\n",
        "        loss = criterion(logits, targets)\n",
        "        loss.backward()\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), CLIP)\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    avg_train_loss = epoch_loss / len(train_loader)\n",
        "    train_losses.append(avg_train_loss)\n",
        "\n",
        "    # Validation Loop\n",
        "    model.eval()\n",
        "    epoch_val_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for src_pad, src_lens, tgt_pad, tgt_lens in val_loader:\n",
        "            src_pad, tgt_pad = src_pad.to(device), tgt_pad.to(device)\n",
        "            logits = model(src_pad, src_lens, tgt_pad)\n",
        "            targets = tgt_pad[:, 1:]\n",
        "\n",
        "            logits = logits.reshape(-1, logits.shape[-1])\n",
        "            targets = targets.reshape(-1)\n",
        "\n",
        "            loss = criterion(logits, targets)\n",
        "            epoch_val_loss += loss.item()\n",
        "\n",
        "    avg_val_loss = epoch_val_loss / len(val_loader)\n",
        "    val_losses.append(avg_val_loss)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}: Train Loss = {avg_train_loss:.4f} | Val Loss = {avg_val_loss:.4f}\")\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(train_losses, label='Train Loss')\n",
        "plt.plot(val_losses, label='Val Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Evaluation (BLEU)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def beam_search_decode(model, src_sentence, beam_size=5, max_len=50):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        # Encode\n",
        "        src_ids = pt_vocab.encode(src_sentence)\n",
        "        # Add batch dim\n",
        "        src_tensor = torch.tensor([src_ids], dtype=torch.long, device=device)\n",
        "        src_len = torch.tensor([len(src_ids)], dtype=torch.long)\n",
        "\n",
        "        enc_hiddens, (h_n, c_n) = model.encoder(src_tensor, src_len)\n",
        "        h, c = model.dec_init(h_n, c_n)\n",
        "\n",
        "        # Candidates: (score, [token_ids], (h, c))\n",
        "        # Start with BOS\n",
        "        candidates = [(0.0, [en_vocab.stoi[BOS_TOKEN]], (h, c))]\n",
        "        completed = []\n",
        "\n",
        "        for _ in range(max_len):\n",
        "            new_candidates = []\n",
        "            for score, seq, (curr_h, curr_c) in candidates:\n",
        "                if seq[-1] == en_vocab.stoi[EOS_TOKEN]:\n",
        "                    completed.append((score, seq))\n",
        "                    continue\n",
        "\n",
        "                inp_token = torch.tensor([seq[-1]], dtype=torch.long, device=device)\n",
        "\n",
        "                # Decoder Step (manual expansion of Decoder logic)\n",
        "                # 1. Embed\n",
        "                embed = model.decoder.embedding(inp_token) # (1, E)\n",
        "                embed = model.decoder.dropout(embed) # Apply dropout? Usually not in eval, but model.eval() handles it.\n",
        "\n",
        "                # 2. LSTM\n",
        "                new_h, new_c = model.decoder.lstm(embed, (curr_h, curr_c))\n",
        "\n",
        "                # 3. Attention\n",
        "                context, _ = model.decoder.attention(new_h, enc_hiddens, src_len.to(device))\n",
        "\n",
        "                # 4. Combine\n",
        "                combined = torch.cat((new_h, context), dim=1)\n",
        "                attn_vec = torch.tanh(model.decoder.combine_proj(combined))\n",
        "\n",
        "                # 5. Output Proj\n",
        "                logits = model.out_proj(attn_vec)\n",
        "                log_probs = F.log_softmax(logits, dim=1)\n",
        "\n",
        "                topv, topi = log_probs.topk(beam_size)\n",
        "\n",
        "                for v, i in zip(topv[0], topi[0]):\n",
        "                    new_candidates.append((score + v.item(), seq + [i.item()], (new_h, new_c)))\n",
        "\n",
        "            if not new_candidates:\n",
        "                break\n",
        "\n",
        "            new_candidates.sort(key=lambda x: x[0], reverse=True)\n",
        "            candidates = new_candidates[:beam_size]\n",
        "\n",
        "        if not completed:\n",
        "            completed = [(c[0], c[1]) for c in candidates]\n",
        "\n",
        "        completed.sort(key=lambda x: x[0], reverse=True)\n",
        "        best_seq = completed[0][1]\n",
        "\n",
        "        return en_vocab.decode(best_seq)\n",
        "\n",
        "def compute_bleu(references, hypotheses):\n",
        "    # Calculate BLEU-4 score\n",
        "    precisions = []\n",
        "    for n in range(1, 5):\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for ref, hyp in zip(references, hypotheses):\n",
        "            ref_tokens = ref.split()\n",
        "            hyp_tokens = hyp.split()\n",
        "\n",
        "            ref_counts = Counter([tuple(ref_tokens[i:i+n]) for i in range(len(ref_tokens)-n+1)])\n",
        "            hyp_counts = Counter([tuple(hyp_tokens[i:i+n]) for i in range(len(hyp_tokens)-n+1)])\n",
        "\n",
        "            for gram, count in hyp_counts.items():\n",
        "                total += count\n",
        "                correct += min(count, ref_counts.get(gram, 0))\n",
        "\n",
        "        if total > 0:\n",
        "            precisions.append(correct / total)\n",
        "        else:\n",
        "            precisions.append(0)\n",
        "\n",
        "    if min(precisions) == 0:\n",
        "        return 0.0\n",
        "\n",
        "    geo_mean = math.exp(sum(math.log(p) for p in precisions) / 4)\n",
        "\n",
        "    ref_len = sum(len(r.split()) for r in references)\n",
        "    hyp_len = sum(len(h.split()) for h in hypotheses)\n",
        "\n",
        "    if hyp_len > ref_len:\n",
        "        bp = 1.0\n",
        "    else:\n",
        "        bp = math.exp(1 - ref_len / hyp_len) if hyp_len > 0 else 0\n",
        "\n",
        "    return bp * geo_mean\n",
        "\n",
        "# Evaluate on Test Set\n",
        "print(\"Evaluating on Test Set...\")\n",
        "hypotheses = []\n",
        "references = []\n",
        "\n",
        "# We loaded test_pt and test_en (2000 examples)\n",
        "for pt, en in tqdm(zip(test_pt, test_en), total=len(test_pt), desc=\"Decoding\"):\n",
        "    hyp = beam_search_decode(model, pt, beam_size=5)\n",
        "    hypotheses.append(hyp)\n",
        "    references.append(en)\n",
        "\n",
        "bleu_score = compute_bleu(references, hypotheses)\n",
        "print(f\"Corpus-Level BLEU Score: {bleu_score*100:.2f}\")\n",
        "\n",
        "# Show some examples\n",
        "print(\"\\nExample Translations:\")\n",
        "for i in range(5):\n",
        "    print(f\"Src: {test_pt[i]}\")\n",
        "    print(f\"Ref: {test_en[i]}\")\n",
        "    print(f\"Hyp: {hypotheses[i]}\")\n",
        "    print(\"-\" * 30)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q2: Understanding & Reflection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**1. Parallel Corpus**: Source-target sentence pairs used for supervised training.\n\n",
        "**2. Special Tokens**:\n* `<pad>`: Batch padding.\n* `<unk>`: Unknown words.\n* `<bos>`/`<eos>`: Sequence boundaries.\n\n",
        "**3. Encoder-Decoder Init**: Bi-directional Encoder outputs 2 hidden states. We concat and project them to initialize the Unidirectional Decoder.\n\n",
        "**4. Attention**: Calculates a weighted sum of encoder outputs (context) for each decoding step, focusing on relevant source info.\n\n",
        "**5. Teacher Forcing**: Uses ground-truth inputs during training. Inference uses model predictions. Discrepancy causes exposure bias.\n\n",
        "**6. BLEU**: Corpus-level n-gram precision metric. Beam search improves scores by exploring better sequences.\n\n",
        "**7. Length Effects**: Long sentences degrade performance due to fixed-vector bottleneck; attention helps.\n\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "nChedj8RE02i"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}